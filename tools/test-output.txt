kubectl config get-contexts
CURRENT   NAME             CLUSTER          AUTHINFO         NAMESPACE
*         docker-desktop   docker-desktop   docker-desktop   
--------------------
kubectl get nodes
NAME             STATUS   ROLES           AGE   VERSION
docker-desktop   Ready    control-plane   23h   v1.32.2
--------------------
cd probe-test-app
--------------------
kubectl apply -f probe-test-app-pod.yaml
pod/probe-test-app created
pod/probe-test-app condition met
--------------------
kubectl get pods -o wide
NAME             READY   STATUS    RESTARTS   AGE   IP          NODE             NOMINATED NODE   READINESS GATES
probe-test-app   1/1     Running   0          33s   10.1.0.12   docker-desktop   <none>           <none>
--------------------
kubectl apply -f probe-test-app-service.yaml
service/probe-test-app created
--------------------
kubectl get services -o wide
NAME             TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE   SELECTOR
kubernetes       ClusterIP      10.96.0.1       <none>        443/TCP          23h   <none>
probe-test-app   LoadBalancer   10.98.161.126   <pending>     8000:32492/TCP   1s    app.kubernetes.io/name=probe-test-app
--------------------
kubectl get endpoints
NAME             ENDPOINTS           AGE
kubernetes       192.168.65.3:6443   23h
probe-test-app   10.1.0.12:8080      2s
--------------------
kubectl apply -f probe-test-app-pod-2.yaml
pod/probe-test-app-2 created
pod/probe-test-app-2 condition met
--------------------
kubectl get endpoints
NAME             ENDPOINTS                       AGE
kubernetes       192.168.65.3:6443               23h
probe-test-app   10.1.0.12:8080,10.1.0.13:8080   5s
--------------------
kubectl delete pods --all
pod "probe-test-app" deleted
pod "probe-test-app-2" deleted
--------------------
kubectl apply -f probe-test-app-replicaset.yaml
replicaset.apps/probe-test-app created
pod/probe-test-app-b7t45 condition met
pod/probe-test-app-f9n8z condition met
pod/probe-test-app-rlk2g condition met
--------------------
kubectl scale replicaset probe-test-app --replicas=2
replicaset.apps/probe-test-app scaled
--------------------
kubectl get pods
NAME                   READY   STATUS        RESTARTS   AGE
probe-test-app-b7t45   1/1     Running       0          5s
probe-test-app-f9n8z   1/1     Terminating   0          5s
probe-test-app-rlk2g   1/1     Running       0          5s
--------------------
kubectl delete replicaset probe-test-app
replicaset.apps "probe-test-app" deleted
--------------------
kubectl apply -f probe-test-app-deployment.yaml
deployment.apps/probe-test-app created
Waiting for deployment "probe-test-app" rollout to finish: 0 of 3 updated replicas are available...
Waiting for deployment "probe-test-app" rollout to finish: 1 of 3 updated replicas are available...
Waiting for deployment "probe-test-app" rollout to finish: 2 of 3 updated replicas are available...
deployment "probe-test-app" successfully rolled out
--------------------
kubectl get pods
NAME                              READY   STATUS    RESTARTS   AGE
probe-test-app-685956d4cf-9q5z7   1/1     Running   0          3s
probe-test-app-685956d4cf-mvqpz   1/1     Running   0          3s
probe-test-app-685956d4cf-tmgvm   1/1     Running   0          3s
--------------------
kubectl get replicasets
NAME                        DESIRED   CURRENT   READY   AGE
probe-test-app-685956d4cf   3         3         3       4s
--------------------
kubectl set image deployment/probe-test-app probe-test-app=jasonumiker/probe-test-app:v2
deployment.apps/probe-test-app image updated
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "probe-test-app" rollout to finish: 1 old replicas are pending termination...
deployment "probe-test-app" successfully rolled out
--------------------
kubectl events
LAST SEEN               TYPE      REASON                              OBJECT                                 MESSAGE
23h                     Normal    Starting                            Node/docker-desktop                    Starting kubelet.
23h                     Warning   CgroupV1                            Node/docker-desktop                    cgroup v1 support is in maintenance mode, please migrate to cgroup v2
23h                     Warning   InvalidDiskCapacity                 Node/docker-desktop                    invalid capacity 0 on image filesystem
23h (x8 over 23h)       Normal    NodeHasSufficientMemory             Node/docker-desktop                    Node docker-desktop status is now: NodeHasSufficientMemory
23h (x8 over 23h)       Normal    NodeHasNoDiskPressure               Node/docker-desktop                    Node docker-desktop status is now: NodeHasNoDiskPressure
23h (x7 over 23h)       Normal    NodeHasSufficientPID                Node/docker-desktop                    Node docker-desktop status is now: NodeHasSufficientPID
23h                     Normal    NodeAllocatableEnforced             Node/docker-desktop                    Updated Node Allocatable limit across pods
23h                     Normal    RegisteredNode                      Node/docker-desktop                    Node docker-desktop event: Registered Node docker-desktop in Controller
23h                     Normal    Starting                            Node/docker-desktop                    
3m22s                   Warning   PossibleMemoryBackedVolumesOnDisk   Node/docker-desktop                    The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
3m22s                   Normal    Starting                            Node/docker-desktop                    Starting kubelet.
3m22s                   Warning   CgroupV1                            Node/docker-desktop                    cgroup v1 support is in maintenance mode, please migrate to cgroup v2
3m22s                   Warning   InvalidDiskCapacity                 Node/docker-desktop                    invalid capacity 0 on image filesystem
3m22s (x8 over 3m22s)   Normal    NodeHasSufficientMemory             Node/docker-desktop                    Node docker-desktop status is now: NodeHasSufficientMemory
3m22s (x8 over 3m22s)   Normal    NodeHasNoDiskPressure               Node/docker-desktop                    Node docker-desktop status is now: NodeHasNoDiskPressure
3m22s (x7 over 3m22s)   Normal    NodeHasSufficientPID                Node/docker-desktop                    Node docker-desktop status is now: NodeHasSufficientPID
3m22s                   Normal    NodeAllocatableEnforced             Node/docker-desktop                    Updated Node Allocatable limit across pods
3m19s                   Warning   Rebooted                            Node/docker-desktop                    Node docker-desktop has been rebooted, boot id: 72e1fd00-9075-416a-93ce-bb4a92ad29a8
3m17s                   Normal    Starting                            Node/docker-desktop                    
3m16s                   Normal    RegisteredNode                      Node/docker-desktop                    Node docker-desktop event: Registered Node docker-desktop in Controller
104s                    Normal    Starting                            Node/docker-desktop                    
69s                     Normal    Scheduled                           Pod/probe-test-app                     Successfully assigned default/probe-test-app to docker-desktop
68s                     Normal    Pulling                             Pod/probe-test-app                     Pulling image "mirror.gcr.io/jasonumiker/probe-test-app:v1"
39s                     Normal    Pulled                              Pod/probe-test-app                     Successfully pulled image "mirror.gcr.io/jasonumiker/probe-test-app:v1" in 30.214s (30.214s including waiting). Image size: 1024950162 bytes.
38s                     Normal    Started                             Pod/probe-test-app                     Started container probe-test-app
38s                     Normal    Created                             Pod/probe-test-app                     Created container: probe-test-app
32s                     Normal    Started                             Pod/probe-test-app-2                   Started container probe-test-app
32s                     Normal    Created                             Pod/probe-test-app-2                   Created container: probe-test-app
32s                     Normal    Pulled                              Pod/probe-test-app-2                   Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
32s                     Normal    Scheduled                           Pod/probe-test-app-2                   Successfully assigned default/probe-test-app-2 to docker-desktop
29s                     Normal    Killing                             Pod/probe-test-app                     Stopping container probe-test-app
28s                     Normal    Killing                             Pod/probe-test-app-2                   Stopping container probe-test-app
27s                     Warning   Unhealthy                           Pod/probe-test-app                     Readiness probe failed: Get "http://10.1.0.12:8080/readyz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
26s                     Normal    SuccessfulCreate                    ReplicaSet/probe-test-app              Created pod: probe-test-app-b7t45
26s                     Normal    SuccessfulCreate                    ReplicaSet/probe-test-app              Created pod: probe-test-app-rlk2g
26s                     Normal    SuccessfulCreate                    ReplicaSet/probe-test-app              Created pod: probe-test-app-f9n8z
25s                     Normal    Scheduled                           Pod/probe-test-app-rlk2g               Successfully assigned default/probe-test-app-rlk2g to docker-desktop
25s                     Normal    Scheduled                           Pod/probe-test-app-b7t45               Successfully assigned default/probe-test-app-b7t45 to docker-desktop
25s                     Normal    Scheduled                           Pod/probe-test-app-f9n8z               Successfully assigned default/probe-test-app-f9n8z to docker-desktop
25s                     Warning   Unhealthy                           Pod/probe-test-app-f9n8z               Readiness probe failed: Get "http://10.1.0.15:8080/readyz": dial tcp 10.1.0.15:8080: connect: connection refused
25s                     Normal    Started                             Pod/probe-test-app-f9n8z               Started container probe-test-app
25s                     Normal    Created                             Pod/probe-test-app-b7t45               Created container: probe-test-app
25s                     Normal    Started                             Pod/probe-test-app-b7t45               Started container probe-test-app
25s                     Normal    Pulled                              Pod/probe-test-app-f9n8z               Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
25s                     Normal    Created                             Pod/probe-test-app-f9n8z               Created container: probe-test-app
25s                     Normal    Pulled                              Pod/probe-test-app-b7t45               Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
25s                     Normal    Pulled                              Pod/probe-test-app-rlk2g               Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
25s                     Normal    Created                             Pod/probe-test-app-rlk2g               Created container: probe-test-app
25s                     Normal    Started                             Pod/probe-test-app-rlk2g               Started container probe-test-app
25s                     Warning   Unhealthy                           Pod/probe-test-app-rlk2g               Readiness probe failed: Get "http://10.1.0.14:8080/readyz": dial tcp 10.1.0.14:8080: connect: connection refused
22s                     Normal    SuccessfulDelete                    ReplicaSet/probe-test-app              Deleted pod: probe-test-app-f9n8z
21s                     Normal    Killing                             Pod/probe-test-app-f9n8z               Stopping container probe-test-app
20s                     Normal    Killing                             Pod/probe-test-app-rlk2g               Stopping container probe-test-app
20s                     Normal    Killing                             Pod/probe-test-app-b7t45               Stopping container probe-test-app
19s                     Normal    ScalingReplicaSet                   Deployment/probe-test-app              Scaled up replica set probe-test-app-685956d4cf from 0 to 3
19s                     Normal    SuccessfulCreate                    ReplicaSet/probe-test-app-685956d4cf   Created pod: probe-test-app-685956d4cf-tmgvm
19s                     Normal    SuccessfulCreate                    ReplicaSet/probe-test-app-685956d4cf   Created pod: probe-test-app-685956d4cf-mvqpz
19s                     Normal    SuccessfulCreate                    ReplicaSet/probe-test-app-685956d4cf   Created pod: probe-test-app-685956d4cf-9q5z7
19s                     Normal    Scheduled                           Pod/probe-test-app-685956d4cf-9q5z7    Successfully assigned default/probe-test-app-685956d4cf-9q5z7 to docker-desktop
19s                     Normal    Scheduled                           Pod/probe-test-app-685956d4cf-tmgvm    Successfully assigned default/probe-test-app-685956d4cf-tmgvm to docker-desktop
19s                     Normal    Scheduled                           Pod/probe-test-app-685956d4cf-mvqpz    Successfully assigned default/probe-test-app-685956d4cf-mvqpz to docker-desktop
18s                     Normal    Created                             Pod/probe-test-app-685956d4cf-mvqpz    Created container: probe-test-app
18s                     Normal    Pulled                              Pod/probe-test-app-685956d4cf-tmgvm    Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
18s                     Normal    Pulled                              Pod/probe-test-app-685956d4cf-mvqpz    Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
18s                     Normal    Started                             Pod/probe-test-app-685956d4cf-9q5z7    Started container probe-test-app
18s                     Normal    Started                             Pod/probe-test-app-685956d4cf-tmgvm    Started container probe-test-app
18s                     Normal    Created                             Pod/probe-test-app-685956d4cf-9q5z7    Created container: probe-test-app
18s                     Normal    Started                             Pod/probe-test-app-685956d4cf-mvqpz    Started container probe-test-app
18s                     Normal    Pulled                              Pod/probe-test-app-685956d4cf-9q5z7    Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
18s                     Normal    Created                             Pod/probe-test-app-685956d4cf-tmgvm    Created container: probe-test-app
14s                     Normal    ScalingReplicaSet                   Deployment/probe-test-app              Scaled up replica set probe-test-app-68d99fdc94 from 0 to 1
14s                     Normal    SuccessfulCreate                    ReplicaSet/probe-test-app-68d99fdc94   Created pod: probe-test-app-68d99fdc94-hsl28
13s                     Normal    Scheduled                           Pod/probe-test-app-68d99fdc94-hsl28    Successfully assigned default/probe-test-app-68d99fdc94-hsl28 to docker-desktop
13s                     Normal    Pulling                             Pod/probe-test-app-68d99fdc94-hsl28    Pulling image "jasonumiker/probe-test-app:v2"
6s                      Normal    Created                             Pod/probe-test-app-68d99fdc94-hsl28    Created container: probe-test-app
6s                      Normal    Started                             Pod/probe-test-app-68d99fdc94-hsl28    Started container probe-test-app
6s                      Normal    Pulled                              Pod/probe-test-app-68d99fdc94-hsl28    Successfully pulled image "jasonumiker/probe-test-app:v2" in 6.932s (6.932s including waiting). Image size: 1024950162 bytes.
5s                      Normal    SuccessfulDelete                    ReplicaSet/probe-test-app-685956d4cf   Deleted pod: probe-test-app-685956d4cf-9q5z7
5s                      Normal    ScalingReplicaSet                   Deployment/probe-test-app              Scaled down replica set probe-test-app-685956d4cf from 3 to 2
5s                      Normal    ScalingReplicaSet                   Deployment/probe-test-app              Scaled up replica set probe-test-app-68d99fdc94 from 1 to 2
5s                      Normal    Killing                             Pod/probe-test-app-685956d4cf-9q5z7    Stopping container probe-test-app
5s                      Normal    SuccessfulCreate                    ReplicaSet/probe-test-app-68d99fdc94   Created pod: probe-test-app-68d99fdc94-rdzbn
5s                      Normal    Scheduled                           Pod/probe-test-app-68d99fdc94-rdzbn    Successfully assigned default/probe-test-app-68d99fdc94-rdzbn to docker-desktop
4s                      Normal    Started                             Pod/probe-test-app-68d99fdc94-rdzbn    Started container probe-test-app
4s                      Normal    Killing                             Pod/probe-test-app-685956d4cf-tmgvm    Stopping container probe-test-app
4s                      Normal    ScalingReplicaSet                   Deployment/probe-test-app              Scaled up replica set probe-test-app-68d99fdc94 from 2 to 3
4s                      Normal    ScalingReplicaSet                   Deployment/probe-test-app              Scaled down replica set probe-test-app-685956d4cf from 2 to 1
4s                      Normal    SuccessfulCreate                    ReplicaSet/probe-test-app-68d99fdc94   Created pod: probe-test-app-68d99fdc94-6qp4t
4s                      Warning   Unhealthy                           Pod/probe-test-app-68d99fdc94-rdzbn    Readiness probe failed: Get "http://10.1.0.21:8080/readyz": dial tcp 10.1.0.21:8080: connect: connection refused
4s                      Normal    Pulled                              Pod/probe-test-app-68d99fdc94-rdzbn    Container image "jasonumiker/probe-test-app:v2" already present on machine
4s                      Normal    Created                             Pod/probe-test-app-68d99fdc94-rdzbn    Created container: probe-test-app
4s                      Normal    SuccessfulDelete                    ReplicaSet/probe-test-app-685956d4cf   Deleted pod: probe-test-app-685956d4cf-tmgvm
4s                      Normal    Scheduled                           Pod/probe-test-app-68d99fdc94-6qp4t    Successfully assigned default/probe-test-app-68d99fdc94-6qp4t to docker-desktop
3s                      Normal    Created                             Pod/probe-test-app-68d99fdc94-6qp4t    Created container: probe-test-app
3s                      Warning   Unhealthy                           Pod/probe-test-app-68d99fdc94-6qp4t    Readiness probe failed: Get "http://10.1.0.22:8080/readyz": dial tcp 10.1.0.22:8080: connect: connection refused
3s                      Normal    Started                             Pod/probe-test-app-68d99fdc94-6qp4t    Started container probe-test-app
3s                      Normal    Pulled                              Pod/probe-test-app-68d99fdc94-6qp4t    Container image "jasonumiker/probe-test-app:v2" already present on machine
2s                      Normal    SuccessfulDelete                    ReplicaSet/probe-test-app-685956d4cf   Deleted pod: probe-test-app-685956d4cf-mvqpz
2s                      Normal    Killing                             Pod/probe-test-app-685956d4cf-mvqpz    Stopping container probe-test-app
2s                      Normal    ScalingReplicaSet                   Deployment/probe-test-app              Scaled down replica set probe-test-app-685956d4cf from 1 to 0
--------------------
kubectl rollout undo deployment/probe-test-app
deployment.apps/probe-test-app rolled back
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "probe-test-app" rollout to finish: 1 old replicas are pending termination...
deployment "probe-test-app" successfully rolled out
--------------------
kubectl get pods
NAME                              READY   STATUS    RESTARTS   AGE
probe-test-app-685956d4cf-cm59g   1/1     Running   0          7s
probe-test-app-685956d4cf-hvcmn   1/1     Running   0          5s
probe-test-app-685956d4cf-xrcbz   1/1     Running   0          3s
--------------------
kubectl describe replicaset probe-test-app
Name:           probe-test-app-685956d4cf
Namespace:      default
Selector:       app.kubernetes.io/name=probe-test-app,pod-template-hash=685956d4cf
Labels:         app.kubernetes.io/name=probe-test-app
                pod-template-hash=685956d4cf
Annotations:    deployment.kubernetes.io/desired-replicas: 3
                deployment.kubernetes.io/max-replicas: 4
                deployment.kubernetes.io/revision: 3
                deployment.kubernetes.io/revision-history: 1
Controlled By:  Deployment/probe-test-app
Replicas:       3 current / 3 desired
Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app.kubernetes.io/name=probe-test-app
           pod-template-hash=685956d4cf
  Containers:
   probe-test-app:
    Image:      mirror.gcr.io/jasonumiker/probe-test-app:v1
    Port:       8080/TCP
    Host Port:  0/TCP
    Requests:
      cpu:         50m
      memory:      52Mi
    Liveness:      http-get http://:8080/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:     http-get http://:8080/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  30s   replicaset-controller  Created pod: probe-test-app-685956d4cf-9q5z7
  Normal  SuccessfulCreate  30s   replicaset-controller  Created pod: probe-test-app-685956d4cf-mvqpz
  Normal  SuccessfulCreate  30s   replicaset-controller  Created pod: probe-test-app-685956d4cf-tmgvm
  Normal  SuccessfulDelete  16s   replicaset-controller  Deleted pod: probe-test-app-685956d4cf-9q5z7
  Normal  SuccessfulDelete  15s   replicaset-controller  Deleted pod: probe-test-app-685956d4cf-tmgvm
  Normal  SuccessfulDelete  13s   replicaset-controller  Deleted pod: probe-test-app-685956d4cf-mvqpz
  Normal  SuccessfulCreate  10s   replicaset-controller  Created pod: probe-test-app-685956d4cf-cm59g
  Normal  SuccessfulCreate  8s    replicaset-controller  Created pod: probe-test-app-685956d4cf-hvcmn
  Normal  SuccessfulCreate  6s    replicaset-controller  Created pod: probe-test-app-685956d4cf-xrcbz

Name:           probe-test-app-68d99fdc94
Namespace:      default
Selector:       app.kubernetes.io/name=probe-test-app,pod-template-hash=68d99fdc94
Labels:         app.kubernetes.io/name=probe-test-app
                pod-template-hash=68d99fdc94
Annotations:    deployment.kubernetes.io/desired-replicas: 3
                deployment.kubernetes.io/max-replicas: 4
                deployment.kubernetes.io/revision: 2
Controlled By:  Deployment/probe-test-app
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app.kubernetes.io/name=probe-test-app
           pod-template-hash=68d99fdc94
  Containers:
   probe-test-app:
    Image:      jasonumiker/probe-test-app:v2
    Port:       8080/TCP
    Host Port:  0/TCP
    Requests:
      cpu:         50m
      memory:      52Mi
    Liveness:      http-get http://:8080/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:     http-get http://:8080/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  26s   replicaset-controller  Created pod: probe-test-app-68d99fdc94-hsl28
  Normal  SuccessfulCreate  17s   replicaset-controller  Created pod: probe-test-app-68d99fdc94-rdzbn
  Normal  SuccessfulCreate  16s   replicaset-controller  Created pod: probe-test-app-68d99fdc94-6qp4t
  Normal  SuccessfulDelete  9s    replicaset-controller  Deleted pod: probe-test-app-68d99fdc94-6qp4t
  Normal  SuccessfulDelete  7s    replicaset-controller  Deleted pod: probe-test-app-68d99fdc94-hsl28
  Normal  SuccessfulDelete  5s    replicaset-controller  Deleted pod: probe-test-app-68d99fdc94-rdzbn

--------------------
cd ../sidecar-and-init-containers
--------------------
kubectl apply -f sidecar.yaml
pod/pod-with-sidecar created
pod/pod-with-sidecar condition met
--------------------
kubectl apply -f init.yaml
pod/myapp-pod created
--------------------
kubectl get pod myapp-pod
NAME        READY   STATUS     RESTARTS   AGE
myapp-pod   0/1     Init:0/2   0          1s
--------------------
kubectl apply -f services-init-requires.yaml
service/myservice created
service/mydb created
--------------------
NAME        READY   STATUS    RESTARTS   AGE
myapp-pod   1/1     Running   0          13s
--------------------
pod "myapp-pod" deleted
pod "pod-with-sidecar" deleted
service "myservice" deleted
service "mydb" deleted
--------------------
cd ../pvs-and-statefulsets
--------------------
kubectl apply -f hostpath-provisioner.yaml
deployment.apps/hostpath-provisioner created
storageclass.storage.k8s.io/hostpath-provisioner created
serviceaccount/hostpath-provisioner created
clusterrole.rbac.authorization.k8s.io/hostpath-provisioner created
clusterrolebinding.rbac.authorization.k8s.io/hostpath-provisioner created
--------------------
kubectl get storageclass
NAME                   PROVISIONER            RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
hostpath (default)     docker.io/hostpath     Delete          Immediate              false                  23h
hostpath-provisioner   microk8s.io/hostpath   Delete          WaitForFirstConsumer   false                  1s
--------------------
kubectl apply -f pvc.yaml
persistentvolumeclaim/test-pvc created
--------------------
kubectl get pvc
NAME       STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS           VOLUMEATTRIBUTESCLASS   AGE
test-pvc   Pending                                      hostpath-provisioner   <unset>                 1s
--------------------
kubectl apply -f pod.yaml
pod/nginx created
pod/nginx condition met
--------------------
kubectl get pvc
NAME       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS           VOLUMEATTRIBUTESCLASS   AGE
test-pvc   Bound    pvc-fd27e180-9d95-4985-9ce2-0315ca6143ed   1Gi        RWO            hostpath-provisioner   <unset>                 19s
--------------------
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS           VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-fd27e180-9d95-4985-9ce2-0315ca6143ed   1Gi        RWO            Delete           Bound    default/test-pvc   hostpath-provisioner   <unset>                          4s
--------------------
kubectl apply -f service.yaml
service/nginx created
--------------------
curl http://localhost:8001
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
curl: (7) Failed to connect to localhost port 8001 after 0 ms: Couldn't connect to server
--------------------
kubectl exec -it nginx  -- bash -c "echo 'Data on PV' > /usr/share/nginx/html/index.html"
--------------------
curl http://localhost:8001
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
curl: (7) Failed to connect to localhost port 8001 after 0 ms: Couldn't connect to server
--------------------
kubectl delete pod nginx
pod "nginx" deleted
--------------------
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS           VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-fd27e180-9d95-4985-9ce2-0315ca6143ed   1Gi        RWO            Delete           Bound    default/test-pvc   hostpath-provisioner   <unset>                          21s
--------------------
kubectl apply -f pod.yaml
pod/nginx created
pod/nginx condition met
--------------------
curl http://localhost:8001
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
curl: (7) Failed to connect to localhost port 8001 after 0 ms: Couldn't connect to server
--------------------
kubectl delete service nginx
service "nginx" deleted
--------------------
kubectl delete pod nginx
pod "nginx" deleted
--------------------
kubectl delete pvc test-pvc
persistentvolumeclaim "test-pvc" deleted
--------------------
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM              STORAGECLASS           VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-fd27e180-9d95-4985-9ce2-0315ca6143ed   1Gi        RWO            Delete           Released   default/test-pvc   hostpath-provisioner   <unset>                          29s
--------------------
--------------------
kubectl apply -k .
serviceaccount/rabbitmq created
role.rbac.authorization.k8s.io/rabbitmq created
rolebinding.rbac.authorization.k8s.io/rabbitmq created
configmap/rabbitmq-config created
secret/erlang-cookie created
secret/rabbitmq-admin created
service/rabbitmq-client created
service/rabbitmq-headless created
statefulset.apps/rabbitmq created
Waiting for 1 pods to be ready...
partitioned roll out complete: 1 new pods have been updated...
--------------------
kubectl describe statefulset rabbitmq
Name:               rabbitmq
Namespace:          default
CreationTimestamp:  Tue, 15 Apr 2025 22:26:50 +1000
Selector:           app=rabbitmq
Labels:             <none>
Annotations:        <none>
Replicas:           1 desired | 1 total
Update Strategy:    RollingUpdate
  Partition:        0
Pods Status:        1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=rabbitmq
  Service Account:  rabbitmq
  Init Containers:
   rabbitmq-config:
    Image:      mirror.gcr.io/busybox:1.37.0
    Port:       <none>
    Host Port:  <none>
    Command:
      sh
      -c
      cp /tmp/rabbitmq/rabbitmq.conf /etc/rabbitmq/rabbitmq.conf && echo '' >> /etc/rabbitmq/rabbitmq.conf; cp /tmp/rabbitmq/enabled_plugins /etc/rabbitmq/enabled_plugins
    Environment:  <none>
    Mounts:
      /etc/rabbitmq from rabbitmq-config-rw (rw)
      /tmp/rabbitmq from rabbitmq-config (rw)
  Containers:
   rabbitmq:
    Image:       mirror.gcr.io/rabbitmq:3.8.34
    Ports:       5672/TCP, 15672/TCP, 15692/TCP, 4369/TCP
    Host Ports:  0/TCP, 0/TCP, 0/TCP, 0/TCP
    Liveness:    exec [rabbitmq-diagnostics status] delay=60s timeout=15s period=60s #success=1 #failure=3
    Readiness:   exec [rabbitmq-diagnostics ping] delay=20s timeout=10s period=60s #success=1 #failure=3
    Environment:
      RABBITMQ_DEFAULT_PASS:   <set to the key 'pass' in secret 'rabbitmq-admin'>   Optional: false
      RABBITMQ_DEFAULT_USER:   <set to the key 'user' in secret 'rabbitmq-admin'>   Optional: false
      RABBITMQ_ERLANG_COOKIE:  <set to the key 'cookie' in secret 'erlang-cookie'>  Optional: false
    Mounts:
      /etc/rabbitmq from rabbitmq-config-rw (rw)
      /var/lib/rabbitmq/mnesia from rabbitmq-data (rw)
  Volumes:
   rabbitmq-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      rabbitmq-config
    Optional:  false
   rabbitmq-config-rw:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
   rabbitmq-data:
    Type:          PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:     rabbitmq-data
    ReadOnly:      false
  Node-Selectors:  <none>
  Tolerations:     <none>
Volume Claims:
  Name:          rabbitmq-data
  StorageClass:  hostpath-provisioner
  Labels:        <none>
  Annotations:   <none>
  Capacity:      3Gi
  Access Modes:  [ReadWriteOnce]
Events:
  Type    Reason            Age   From                    Message
  ----    ------            ----  ----                    -------
  Normal  SuccessfulCreate  78s   statefulset-controller  create Claim rabbitmq-data-rabbitmq-0 Pod rabbitmq-0 in StatefulSet rabbitmq success
  Normal  SuccessfulCreate  78s   statefulset-controller  create Pod rabbitmq-0 in StatefulSet rabbitmq successful
--------------------
kubectl get pods
NAME                              READY   STATUS    RESTARTS   AGE
probe-test-app-685956d4cf-cm59g   1/1     Running   0          3m58s
probe-test-app-685956d4cf-hvcmn   1/1     Running   0          3m56s
probe-test-app-685956d4cf-xrcbz   1/1     Running   0          3m54s
rabbitmq-0                        1/1     Running   0          80s
--------------------
kubectl get pvc
NAME                       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS           VOLUMEATTRIBUTESCLASS   AGE
rabbitmq-data-rabbitmq-0   Bound    pvc-478cebd3-fb98-49c5-a1ea-1514769ceadd   3Gi        RWO            hostpath-provisioner   <unset>                 81s
--------------------
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                              STORAGECLASS           VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-478cebd3-fb98-49c5-a1ea-1514769ceadd   3Gi        RWO            Delete           Bound    default/rabbitmq-data-rabbitmq-0   hostpath-provisioner   <unset>                          76s
--------------------
kubectl delete pod rabbitmq-0
pod "rabbitmq-0" deleted
--------------------
kubectl get pods
NAME                              READY   STATUS            RESTARTS   AGE
probe-test-app-685956d4cf-cm59g   1/1     Running           0          4m5s
probe-test-app-685956d4cf-hvcmn   1/1     Running           0          4m3s
probe-test-app-685956d4cf-xrcbz   1/1     Running           0          4m1s
rabbitmq-0                        0/1     PodInitializing   0          2s
--------------------
cd ../../monitoring
--------------------
./install-prometheus.sh

Updating docker-desktop pods to expose metrics endpoints
This will involve several kube-system pod restarts

Fetching debian image to run nsenter on the docker-desktop host...
12.10: Pulling from debian
23b7d26ef1d2: Already exists
Digest: sha256:00cd074b40c4d99ff0c24540bdde0533ca3791edcdac0de36d6b9fb3260d89e2
Status: Downloaded newer image for mirror.gcr.io/debian:12.10
mirror.gcr.io/debian:12.10
Host Node IP: 192.168.65.3
Updating kube-proxy configmap...
configmap "kube-proxy" deleted
configmap/kube-proxy created
Restarting the kube-proxy pod
pod "kube-proxy-cn2qq" deleted
pod/kube-proxy-kxzqm condition met
kube-proxy pod restarted.
Updating bind-address on kube-controller-manager...
Waiting for kube-controller-manager to restart, this can take some time...
pod/kube-controller-manager-docker-desktop condition met
pod/kube-controller-manager-docker-desktop condition met
kube-controller-manager pod restarted.
Updating bind-address on kube-scheduler
Waiting for kube-scheduler to restart, this can take some time...
pod/kube-scheduler-docker-desktop condition met
pod/kube-scheduler-docker-desktop condition met
kube-scheduler pod restarted.
Adding node ip to listen-metrics-urls on etcd
Waiting for etcd to restart, this can take some time...
Error from server (Timeout): the server was unable to return a response in the time allotted, but may still be processing the request (get pods)
pod/etcd-docker-desktop condition met

Done! You can now deploy the monitoring components.

"prometheus-community" already exists with the same configuration, skipping
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "kiali" chart repository
...Successfully got an update from the "kedacore" chart repository
...Successfully got an update from the "ingress-nginx" chart repository
...Successfully got an update from the "gatekeeper" chart repository
...Successfully got an update from the "argo-helm" chart repository
...Successfully got an update from the "prometheus-community" chart repository
...Successfully got an update from the "istio" chart repository
Update Complete. ⎈Happy Helming!⎈
namespace/monitoring created
NAME: prometheus
LAST DEPLOYED: Tue Apr 15 22:30:48 2025
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
NOTES:
kube-prometheus-stack has been installed. Check its status by running:
  kubectl --namespace monitoring get pods -l "release=prometheus"

Get Grafana 'admin' user password by running:

  kubectl --namespace monitoring get secrets prometheus-grafana -o jsonpath="{.data.admin-password}" | base64 -d ; echo

Access Grafana local instance:

  export POD_NAME=$(kubectl --namespace monitoring get pod -l "app.kubernetes.io/name=grafana,app.kubernetes.io/instance=prometheus" -oname)
  kubectl --namespace monitoring port-forward $POD_NAME 3000

Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create & configure Alertmanager and Prometheus instances using the Operator.
NAME: adapter
LAST DEPLOYED: Tue Apr 15 22:31:53 2025
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
adapter-prometheus-adapter has been deployed.
In a few minutes you should be able to list metrics using the following command(s):

  kubectl get --raw /apis/metrics.k8s.io/v1beta1
  kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1
Waiting for deployment "adapter-prometheus-adapter" rollout to finish: 0 of 1 updated replicas are available...
deployment "adapter-prometheus-adapter" successfully rolled out
Waiting for 1 pods to be ready...
statefulset rolling update complete 1 pods at revision prometheus-prometheus-kube-prometheus-prometheus-75b9fff9cd...
--------------------
kubectl top nodes
NAME             CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)   
docker-desktop   77m          0%       3670Mi          11%         
--------------------
kubectl top pods
NAME                              CPU(cores)   MEMORY(bytes)   
probe-test-app-685956d4cf-cm59g   0m           41Mi            
probe-test-app-685956d4cf-hvcmn   0m           41Mi            
probe-test-app-685956d4cf-xrcbz   0m           41Mi            
rabbitmq-0                        21m          140Mi           
--------------------
kubectl top pods -n monitoring
NAME                                                     CPU(cores)   MEMORY(bytes)   
adapter-prometheus-adapter-57b7646776-7d8vp              0m           29Mi            
alertmanager-prometheus-kube-prometheus-alertmanager-0   0m           30Mi            
prometheus-grafana-6854b47bf4-cx985                      2m           285Mi           
prometheus-kube-prometheus-operator-7f8d744cd7-st2b4     0m           31Mi            
prometheus-kube-state-metrics-f699c577d-8pz9c            0m           20Mi            
prometheus-prometheus-kube-prometheus-prometheus-0       8m           117Mi           
prometheus-prometheus-node-exporter-v6xj6                0m           8Mi             
--------------------
cd ../probe-test-app
--------------------
kubectl apply -f probe-test-app-hpa.yaml
horizontalpodautoscaler.autoscaling/probe-test-app created
--------------------
kubectl apply -f generate-load-app-replicaset.yaml
replicaset.apps/generate-load-app created
--------------------
kubectl get pods
NAME                              READY   STATUS    RESTARTS   AGE
generate-load-app-2gqqc           1/1     Running   0          45s
generate-load-app-9l7zz           1/1     Running   0          45s
generate-load-app-djx4q           1/1     Running   0          45s
generate-load-app-p5wzq           1/1     Running   0          45s
generate-load-app-sx48m           1/1     Running   0          45s
probe-test-app-685956d4cf-cm59g   1/1     Running   0          11m
probe-test-app-685956d4cf-hvcmn   1/1     Running   0          11m
probe-test-app-685956d4cf-xrcbz   1/1     Running   0          11m
rabbitmq-0                        1/1     Running   0          7m19s
--------------------
kubectl delete replicaset generate-load-app
replicaset.apps "generate-load-app" deleted
--------------------
kubectl describe hpa probe-test-app
Name:                                                  probe-test-app
Namespace:                                             default
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Tue, 15 Apr 2025 22:34:48 +1000
Reference:                                             Deployment/probe-test-app
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  56% (28m) / 50%
Min replicas:                                          1
Max replicas:                                          5
Deployment pods:                                       3 current / 4 desired
Conditions:
  Type            Status  Reason              Message
  ----            ------  ------              -------
  AbleToScale     True    SucceededRescale    the HPA controller was able to update the target scale to 4
  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)
  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range
Events:
  Type    Reason             Age   From                       Message
  ----    ------             ----  ----                       -------
  Normal  SuccessfulRescale  33s   horizontal-pod-autoscaler  New size: 4; reason: cpu resource utilization (percentage of request) above target
--------------------
cd ../limit-examples
--------------------
kubectl apply -f cpu-stressor.yaml
deployment.apps/cpu-stressor created
Waiting for deployment "cpu-stressor" rollout to finish: 0 of 1 updated replicas are available...
deployment "cpu-stressor" successfully rolled out
--------------------
kubectl delete deployment cpu-stressor
deployment.apps "cpu-stressor" deleted
--------------------
kubectl apply -f memory-stressor.yaml
pod/memory-stressor created
--------------------
kubectl delete pod memory-stressor
pod "memory-stressor" deleted
--------------------
cd ../keda-example
--------------------
./install-keda.sh
"kedacore" already exists with the same configuration, skipping
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "kiali" chart repository
...Successfully got an update from the "kedacore" chart repository
...Successfully got an update from the "gatekeeper" chart repository
...Successfully got an update from the "argo-helm" chart repository
...Successfully got an update from the "ingress-nginx" chart repository
...Successfully got an update from the "prometheus-community" chart repository
...Successfully got an update from the "istio" chart repository
Update Complete. ⎈Happy Helming!⎈
NAME: keda
LAST DEPLOYED: Tue Apr 15 22:36:49 2025
NAMESPACE: keda
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
:::^.     .::::^:     :::::::::::::::    .:::::::::.                   .^.                  
7???~   .^7????~.     7??????????????.   :?????????77!^.              .7?7.                 
7???~  ^7???7~.       ~!!!!!!!!!!!!!!.   :????!!!!7????7~.           .7???7.                
7???~^7????~.                            :????:    :~7???7.         :7?????7.               
7???7????!.           ::::::::::::.      :????:      .7???!        :7??77???7.              
7????????7:           7???????????~      :????:       :????:      :???7?5????7.             
7????!~????^          !77777777777^      :????:       :????:     ^???7?#P7????7.            
7???~  ^????~                            :????:      :7???!     ^???7J#@J7?????7.           
7???~   :7???!.                          :????:   .:~7???!.    ~???7Y&@#7777????7.          
7???~    .7???7:      !!!!!!!!!!!!!!!    :????7!!77????7^     ~??775@@@GJJYJ?????7.         
7???~     .!????^     7?????????????7.   :?????????7!~:      !????G@@@@@@@@5??????7:        
::::.       :::::     :::::::::::::::    .::::::::..        .::::JGGGB@@@&7:::::::::        
                                                                      ?@@#~                  
                                                                      P@B^                   
                                                                    :&G:                    
                                                                    !5.                     
                                                                    .Kubernetes Event-driven Autoscaling (KEDA) - Application autoscaling made simple.

Get started by deploying Scaled Objects to your cluster:
    - Information about Scaled Objects : https://keda.sh/docs/latest/concepts/
    - Samples: https://github.com/kedacore/samples

Get information about the deployed ScaledObjects:
  kubectl get scaledobject [--namespace <namespace>]

Get details about a deployed ScaledObject:
  kubectl describe scaledobject <scaled-object-name> [--namespace <namespace>]

Get information about the deployed ScaledObjects:
  kubectl get triggerauthentication [--namespace <namespace>]

Get details about a deployed ScaledObject:
  kubectl describe triggerauthentication <trigger-authentication-name> [--namespace <namespace>]

Get an overview of the Horizontal Pod Autoscalers (HPA) that KEDA is using behind the scenes:
  kubectl get hpa [--all-namespaces] [--namespace <namespace>]

Learn more about KEDA:
- Documentation: https://keda.sh/
- Support: https://keda.sh/support/
- File an issue: https://github.com/kedacore/keda/issues/new/choose
--------------------
kubectl apply -f consumer.yaml
secret/rabbitmq-consumer-secret created
deployment.apps/rabbitmq-consumer created
--------------------
kubectl apply -f keda-scaled-object.yaml
scaledobject.keda.sh/rabbitmq-consumer created
triggerauthentication.keda.sh/rabbitmq-consumer-trigger created
--------------------
kubectl apply -f publisher.yaml
job.batch/rabbitmq-publish created
--------------------
kubectl get pods
NAME                                 READY   STATUS              RESTARTS   AGE
probe-test-app-685956d4cf-cm59g      1/1     Running             0          12m
probe-test-app-685956d4cf-hnhlg      1/1     Running             0          79s
probe-test-app-685956d4cf-hvcmn      1/1     Running             0          12m
probe-test-app-685956d4cf-q6zq5      1/1     Running             0          22s
probe-test-app-685956d4cf-xrcbz      1/1     Running             0          12m
rabbitmq-0                           1/1     Running             0          8m50s
rabbitmq-consumer-54454cf965-66mjc   0/1     ContainerCreating   0          4s
rabbitmq-publish-kbq2x               0/1     ContainerCreating   0          1s
--------------------
kubectl events
LAST SEEN             TYPE      REASON                              OBJECT                                            MESSAGE
23h                   Warning   CgroupV1                            Node/docker-desktop                               cgroup v1 support is in maintenance mode, please migrate to cgroup v2
23h                   Normal    NodeAllocatableEnforced             Node/docker-desktop                               Updated Node Allocatable limit across pods
23h (x7 over 23h)     Normal    NodeHasSufficientPID                Node/docker-desktop                               Node docker-desktop status is now: NodeHasSufficientPID
23h (x8 over 23h)     Normal    NodeHasNoDiskPressure               Node/docker-desktop                               Node docker-desktop status is now: NodeHasNoDiskPressure
23h (x8 over 23h)     Normal    NodeHasSufficientMemory             Node/docker-desktop                               Node docker-desktop status is now: NodeHasSufficientMemory
23h                   Warning   InvalidDiskCapacity                 Node/docker-desktop                               invalid capacity 0 on image filesystem
23h                   Normal    Starting                            Node/docker-desktop                               Starting kubelet.
23h                   Normal    RegisteredNode                      Node/docker-desktop                               Node docker-desktop event: Registered Node docker-desktop in Controller
23h                   Normal    Starting                            Node/docker-desktop                               
16m                   Normal    Starting                            Node/docker-desktop                               Starting kubelet.
16m                   Normal    NodeAllocatableEnforced             Node/docker-desktop                               Updated Node Allocatable limit across pods
16m (x7 over 16m)     Normal    NodeHasSufficientPID                Node/docker-desktop                               Node docker-desktop status is now: NodeHasSufficientPID
16m (x8 over 16m)     Normal    NodeHasNoDiskPressure               Node/docker-desktop                               Node docker-desktop status is now: NodeHasNoDiskPressure
16m (x8 over 16m)     Normal    NodeHasSufficientMemory             Node/docker-desktop                               Node docker-desktop status is now: NodeHasSufficientMemory
16m                   Warning   InvalidDiskCapacity                 Node/docker-desktop                               invalid capacity 0 on image filesystem
16m                   Warning   CgroupV1                            Node/docker-desktop                               cgroup v1 support is in maintenance mode, please migrate to cgroup v2
16m                   Warning   PossibleMemoryBackedVolumesOnDisk   Node/docker-desktop                               The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
16m                   Warning   Rebooted                            Node/docker-desktop                               Node docker-desktop has been rebooted, boot id: 72e1fd00-9075-416a-93ce-bb4a92ad29a8
16m                   Normal    Starting                            Node/docker-desktop                               
16m                   Normal    RegisteredNode                      Node/docker-desktop                               Node docker-desktop event: Registered Node docker-desktop in Controller
14m                   Normal    Starting                            Node/docker-desktop                               
14m                   Normal    Scheduled                           Pod/probe-test-app                                Successfully assigned default/probe-test-app to docker-desktop
14m                   Normal    Pulling                             Pod/probe-test-app                                Pulling image "mirror.gcr.io/jasonumiker/probe-test-app:v1"
13m                   Normal    Pulled                              Pod/probe-test-app                                Successfully pulled image "mirror.gcr.io/jasonumiker/probe-test-app:v1" in 30.214s (30.214s including waiting). Image size: 1024950162 bytes.
13m                   Normal    Created                             Pod/probe-test-app                                Created container: probe-test-app
13m                   Normal    Started                             Pod/probe-test-app                                Started container probe-test-app
13m                   Normal    Pulled                              Pod/probe-test-app-2                              Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
13m                   Normal    Started                             Pod/probe-test-app-2                              Started container probe-test-app
13m                   Normal    Created                             Pod/probe-test-app-2                              Created container: probe-test-app
13m                   Normal    Scheduled                           Pod/probe-test-app-2                              Successfully assigned default/probe-test-app-2 to docker-desktop
13m                   Normal    Killing                             Pod/probe-test-app                                Stopping container probe-test-app
13m                   Normal    Killing                             Pod/probe-test-app-2                              Stopping container probe-test-app
13m                   Warning   Unhealthy                           Pod/probe-test-app                                Readiness probe failed: Get "http://10.1.0.12:8080/readyz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
13m                   Normal    SuccessfulCreate                    ReplicaSet/probe-test-app                         Created pod: probe-test-app-rlk2g
13m                   Normal    SuccessfulCreate                    ReplicaSet/probe-test-app                         Created pod: probe-test-app-b7t45
13m                   Normal    SuccessfulCreate                    ReplicaSet/probe-test-app                         Created pod: probe-test-app-f9n8z
13m                   Normal    Scheduled                           Pod/probe-test-app-rlk2g                          Successfully assigned default/probe-test-app-rlk2g to docker-desktop
13m                   Normal    Scheduled                           Pod/probe-test-app-b7t45                          Successfully assigned default/probe-test-app-b7t45 to docker-desktop
13m                   Normal    Scheduled                           Pod/probe-test-app-f9n8z                          Successfully assigned default/probe-test-app-f9n8z to docker-desktop
13m                   Normal    Created                             Pod/probe-test-app-f9n8z                          Created container: probe-test-app
13m                   Warning   Unhealthy                           Pod/probe-test-app-f9n8z                          Readiness probe failed: Get "http://10.1.0.15:8080/readyz": dial tcp 10.1.0.15:8080: connect: connection refused
13m                   Normal    Started                             Pod/probe-test-app-rlk2g                          Started container probe-test-app
13m                   Normal    Created                             Pod/probe-test-app-rlk2g                          Created container: probe-test-app
13m                   Normal    Pulled                              Pod/probe-test-app-rlk2g                          Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
13m                   Normal    Created                             Pod/probe-test-app-b7t45                          Created container: probe-test-app
13m                   Normal    Started                             Pod/probe-test-app-b7t45                          Started container probe-test-app
13m                   Normal    Pulled                              Pod/probe-test-app-b7t45                          Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
13m                   Normal    Started                             Pod/probe-test-app-f9n8z                          Started container probe-test-app
13m                   Warning   Unhealthy                           Pod/probe-test-app-rlk2g                          Readiness probe failed: Get "http://10.1.0.14:8080/readyz": dial tcp 10.1.0.14:8080: connect: connection refused
13m                   Normal    Pulled                              Pod/probe-test-app-f9n8z                          Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
13m                   Normal    SuccessfulDelete                    ReplicaSet/probe-test-app                         Deleted pod: probe-test-app-f9n8z
13m                   Normal    Killing                             Pod/probe-test-app-f9n8z                          Stopping container probe-test-app
13m                   Normal    Killing                             Pod/probe-test-app-b7t45                          Stopping container probe-test-app
13m                   Normal    Killing                             Pod/probe-test-app-rlk2g                          Stopping container probe-test-app
13m                   Normal    ScalingReplicaSet                   Deployment/probe-test-app                         Scaled up replica set probe-test-app-685956d4cf from 0 to 3
13m                   Normal    SuccessfulCreate                    ReplicaSet/probe-test-app-685956d4cf              Created pod: probe-test-app-685956d4cf-tmgvm
13m                   Normal    SuccessfulCreate                    ReplicaSet/probe-test-app-685956d4cf              Created pod: probe-test-app-685956d4cf-9q5z7
13m                   Normal    SuccessfulCreate                    ReplicaSet/probe-test-app-685956d4cf              Created pod: probe-test-app-685956d4cf-mvqpz
13m                   Normal    Scheduled                           Pod/probe-test-app-685956d4cf-9q5z7               Successfully assigned default/probe-test-app-685956d4cf-9q5z7 to docker-desktop
13m                   Normal    Scheduled                           Pod/probe-test-app-685956d4cf-tmgvm               Successfully assigned default/probe-test-app-685956d4cf-tmgvm to docker-desktop
13m                   Normal    Scheduled                           Pod/probe-test-app-685956d4cf-mvqpz               Successfully assigned default/probe-test-app-685956d4cf-mvqpz to docker-desktop
13m                   Normal    Created                             Pod/probe-test-app-685956d4cf-tmgvm               Created container: probe-test-app
13m                   Normal    Pulled                              Pod/probe-test-app-685956d4cf-mvqpz               Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
13m                   Normal    Pulled                              Pod/probe-test-app-685956d4cf-tmgvm               Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
13m                   Normal    Started                             Pod/probe-test-app-685956d4cf-tmgvm               Started container probe-test-app
13m                   Normal    Started                             Pod/probe-test-app-685956d4cf-mvqpz               Started container probe-test-app
13m                   Normal    Created                             Pod/probe-test-app-685956d4cf-mvqpz               Created container: probe-test-app
13m                   Normal    Started                             Pod/probe-test-app-685956d4cf-9q5z7               Started container probe-test-app
13m                   Normal    Created                             Pod/probe-test-app-685956d4cf-9q5z7               Created container: probe-test-app
13m                   Normal    Pulled                              Pod/probe-test-app-685956d4cf-9q5z7               Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
13m                   Normal    ScalingReplicaSet                   Deployment/probe-test-app                         Scaled up replica set probe-test-app-68d99fdc94 from 0 to 1
13m                   Normal    SuccessfulCreate                    ReplicaSet/probe-test-app-68d99fdc94              Created pod: probe-test-app-68d99fdc94-hsl28
13m                   Normal    Scheduled                           Pod/probe-test-app-68d99fdc94-hsl28               Successfully assigned default/probe-test-app-68d99fdc94-hsl28 to docker-desktop
13m                   Normal    Pulling                             Pod/probe-test-app-68d99fdc94-hsl28               Pulling image "jasonumiker/probe-test-app:v2"
13m                   Normal    Created                             Pod/probe-test-app-68d99fdc94-hsl28               Created container: probe-test-app
13m                   Normal    Pulled                              Pod/probe-test-app-68d99fdc94-hsl28               Successfully pulled image "jasonumiker/probe-test-app:v2" in 6.932s (6.932s including waiting). Image size: 1024950162 bytes.
13m                   Normal    Started                             Pod/probe-test-app-68d99fdc94-hsl28               Started container probe-test-app
13m                   Normal    SuccessfulDelete                    ReplicaSet/probe-test-app-685956d4cf              Deleted pod: probe-test-app-685956d4cf-9q5z7
13m                   Normal    Killing                             Pod/probe-test-app-685956d4cf-9q5z7               Stopping container probe-test-app
13m                   Normal    SuccessfulCreate                    ReplicaSet/probe-test-app-68d99fdc94              Created pod: probe-test-app-68d99fdc94-rdzbn
13m                   Normal    ScalingReplicaSet                   Deployment/probe-test-app                         Scaled down replica set probe-test-app-685956d4cf from 3 to 2
13m                   Normal    ScalingReplicaSet                   Deployment/probe-test-app                         Scaled up replica set probe-test-app-68d99fdc94 from 1 to 2
12m                   Normal    Scheduled                           Pod/probe-test-app-68d99fdc94-rdzbn               Successfully assigned default/probe-test-app-68d99fdc94-rdzbn to docker-desktop
12m                   Warning   Unhealthy                           Pod/probe-test-app-68d99fdc94-rdzbn               Readiness probe failed: Get "http://10.1.0.21:8080/readyz": dial tcp 10.1.0.21:8080: connect: connection refused
12m                   Normal    ScalingReplicaSet                   Deployment/probe-test-app                         Scaled down replica set probe-test-app-685956d4cf from 2 to 1
12m                   Normal    Pulled                              Pod/probe-test-app-68d99fdc94-rdzbn               Container image "jasonumiker/probe-test-app:v2" already present on machine
12m                   Normal    Created                             Pod/probe-test-app-68d99fdc94-rdzbn               Created container: probe-test-app
12m                   Normal    SuccessfulCreate                    ReplicaSet/probe-test-app-68d99fdc94              Created pod: probe-test-app-68d99fdc94-6qp4t
12m                   Normal    Started                             Pod/probe-test-app-68d99fdc94-rdzbn               Started container probe-test-app
12m                   Normal    SuccessfulDelete                    ReplicaSet/probe-test-app-685956d4cf              Deleted pod: probe-test-app-685956d4cf-tmgvm
12m                   Normal    Killing                             Pod/probe-test-app-685956d4cf-tmgvm               Stopping container probe-test-app
12m                   Normal    ScalingReplicaSet                   Deployment/probe-test-app                         Scaled up replica set probe-test-app-68d99fdc94 from 2 to 3
12m                   Normal    Scheduled                           Pod/probe-test-app-68d99fdc94-6qp4t               Successfully assigned default/probe-test-app-68d99fdc94-6qp4t to docker-desktop
12m                   Normal    Started                             Pod/probe-test-app-68d99fdc94-6qp4t               Started container probe-test-app
12m                   Warning   Unhealthy                           Pod/probe-test-app-68d99fdc94-6qp4t               Readiness probe failed: Get "http://10.1.0.22:8080/readyz": dial tcp 10.1.0.22:8080: connect: connection refused
12m                   Normal    Created                             Pod/probe-test-app-68d99fdc94-6qp4t               Created container: probe-test-app
12m                   Normal    Pulled                              Pod/probe-test-app-68d99fdc94-6qp4t               Container image "jasonumiker/probe-test-app:v2" already present on machine
12m                   Normal    Killing                             Pod/probe-test-app-685956d4cf-mvqpz               Stopping container probe-test-app
12m                   Normal    SuccessfulDelete                    ReplicaSet/probe-test-app-685956d4cf              Deleted pod: probe-test-app-685956d4cf-mvqpz
12m                   Normal    ScalingReplicaSet                   Deployment/probe-test-app                         Scaled down replica set probe-test-app-685956d4cf from 1 to 0
12m                   Normal    ScalingReplicaSet                   Deployment/probe-test-app                         Scaled up replica set probe-test-app-685956d4cf from 0 to 1
12m                   Normal    SuccessfulCreate                    ReplicaSet/probe-test-app-685956d4cf              Created pod: probe-test-app-685956d4cf-cm59g
12m                   Normal    Scheduled                           Pod/probe-test-app-685956d4cf-cm59g               Successfully assigned default/probe-test-app-685956d4cf-cm59g to docker-desktop
12m                   Normal    Started                             Pod/probe-test-app-685956d4cf-cm59g               Started container probe-test-app
12m                   Normal    Created                             Pod/probe-test-app-685956d4cf-cm59g               Created container: probe-test-app
12m                   Normal    Pulled                              Pod/probe-test-app-685956d4cf-cm59g               Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
12m                   Normal    SuccessfulDelete                    ReplicaSet/probe-test-app-68d99fdc94              Deleted pod: probe-test-app-68d99fdc94-6qp4t
12m                   Normal    SuccessfulCreate                    ReplicaSet/probe-test-app-685956d4cf              Created pod: probe-test-app-685956d4cf-hvcmn
12m                   Normal    Killing                             Pod/probe-test-app-68d99fdc94-6qp4t               Stopping container probe-test-app
12m                   Normal    ScalingReplicaSet                   Deployment/probe-test-app                         Scaled down replica set probe-test-app-68d99fdc94 from 3 to 2
12m                   Normal    Scheduled                           Pod/probe-test-app-685956d4cf-hvcmn               Successfully assigned default/probe-test-app-685956d4cf-hvcmn to docker-desktop
12m                   Warning   Unhealthy                           Pod/probe-test-app-685956d4cf-hvcmn               Readiness probe failed: Get "http://10.1.0.24:8080/readyz": dial tcp 10.1.0.24:8080: connect: connection refused
12m                   Normal    Started                             Pod/probe-test-app-685956d4cf-hvcmn               Started container probe-test-app
12m                   Normal    Created                             Pod/probe-test-app-685956d4cf-hvcmn               Created container: probe-test-app
12m                   Normal    Pulled                              Pod/probe-test-app-685956d4cf-hvcmn               Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
12m                   Normal    SuccessfulDelete                    ReplicaSet/probe-test-app-68d99fdc94              Deleted pod: probe-test-app-68d99fdc94-hsl28
12m                   Normal    SuccessfulCreate                    ReplicaSet/probe-test-app-685956d4cf              Created pod: probe-test-app-685956d4cf-xrcbz
12m                   Normal    Killing                             Pod/probe-test-app-68d99fdc94-hsl28               Stopping container probe-test-app
12m                   Normal    Scheduled                           Pod/probe-test-app-685956d4cf-xrcbz               Successfully assigned default/probe-test-app-685956d4cf-xrcbz to docker-desktop
12m                   Normal    Pulled                              Pod/probe-test-app-685956d4cf-xrcbz               Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
12m                   Normal    Created                             Pod/probe-test-app-685956d4cf-xrcbz               Created container: probe-test-app
12m                   Normal    Started                             Pod/probe-test-app-685956d4cf-xrcbz               Started container probe-test-app
12m                   Warning   Unhealthy                           Pod/probe-test-app-685956d4cf-xrcbz               Readiness probe failed: Get "http://10.1.0.25:8080/readyz": dial tcp 10.1.0.25:8080: connect: connection refused
12m                   Normal    SuccessfulDelete                    ReplicaSet/probe-test-app-68d99fdc94              Deleted pod: probe-test-app-68d99fdc94-rdzbn
12m (x4 over 12m)     Normal    ScalingReplicaSet                   Deployment/probe-test-app                         (combined from similar events): Scaled down replica set probe-test-app-68d99fdc94 from 1 to 0
12m                   Normal    Killing                             Pod/probe-test-app-68d99fdc94-rdzbn               Stopping container probe-test-app
12m                   Normal    Pulling                             Pod/pod-with-sidecar                              Pulling image "mirror.gcr.io/alpine:3.21.3"
12m                   Normal    Scheduled                           Pod/pod-with-sidecar                              Successfully assigned default/pod-with-sidecar to docker-desktop
12m                   Normal    Pulled                              Pod/pod-with-sidecar                              Successfully pulled image "mirror.gcr.io/alpine:3.21.3" in 6.083s (6.083s including waiting). Image size: 7834312 bytes.
12m                   Normal    Pulling                             Pod/pod-with-sidecar                              Pulling image "mirror.gcr.io/nginx:1.27.4-bookworm"
12m                   Normal    Started                             Pod/pod-with-sidecar                              Started container app-container
12m                   Normal    Created                             Pod/pod-with-sidecar                              Created container: app-container
12m                   Normal    Pulled                              Pod/pod-with-sidecar                              Successfully pulled image "mirror.gcr.io/nginx:1.27.4-bookworm" in 10.326s (10.326s including waiting). Image size: 192056179 bytes.
12m                   Normal    Started                             Pod/pod-with-sidecar                              Started container sidecar-container
12m                   Normal    Created                             Pod/pod-with-sidecar                              Created container: sidecar-container
12m                   Normal    Pulling                             Pod/myapp-pod                                     Pulling image "mirror.gcr.io/busybox:1.37.0"
12m                   Normal    Scheduled                           Pod/myapp-pod                                     Successfully assigned default/myapp-pod to docker-desktop
12m                   Normal    Pulled                              Pod/myapp-pod                                     Successfully pulled image "mirror.gcr.io/busybox:1.37.0" in 5.452s (5.452s including waiting). Image size: 4277910 bytes.
12m                   Normal    Created                             Pod/myapp-pod                                     Created container: init-myservice
12m                   Normal    Started                             Pod/myapp-pod                                     Started container init-myservice
12m                   Normal    Created                             Pod/myapp-pod                                     Created container: init-mydb
12m                   Normal    Started                             Pod/myapp-pod                                     Started container init-mydb
12m                   Normal    Pulled                              Pod/myapp-pod                                     Container image "mirror.gcr.io/busybox:1.37.0" already present on machine
12m                   Normal    Started                             Pod/myapp-pod                                     Started container myapp-container
12m                   Normal    Created                             Pod/myapp-pod                                     Created container: myapp-container
12m                   Normal    Pulled                              Pod/myapp-pod                                     Container image "mirror.gcr.io/busybox:1.37.0" already present on machine
12m                   Normal    Killing                             Pod/myapp-pod                                     Stopping container myapp-container
11m                   Normal    Killing                             Pod/pod-with-sidecar                              Stopping container app-container
11m                   Normal    Killing                             Pod/pod-with-sidecar                              Stopping container sidecar-container
11m                   Normal    WaitForFirstConsumer                PersistentVolumeClaim/test-pvc                    waiting for first consumer to be created before binding
11m                   Normal    ExternalProvisioning                PersistentVolumeClaim/test-pvc                    Waiting for a volume to be created either by the external provisioner 'microk8s.io/hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
10m                   Normal    Provisioning                        PersistentVolumeClaim/test-pvc                    External provisioner is provisioning volume for claim "default/test-pvc"
10m                   Normal    ProvisioningSucceeded               PersistentVolumeClaim/test-pvc                    Successfully provisioned volume pvc-fd27e180-9d95-4985-9ce2-0315ca6143ed
10m                   Normal    Scheduled                           Pod/nginx                                         Successfully assigned default/nginx to docker-desktop
10m                   Normal    Started                             Pod/nginx                                         Started container nginx
10m                   Normal    Pulled                              Pod/nginx                                         Container image "mirror.gcr.io/nginx:1.27.4-bookworm" already present on machine
10m                   Normal    Created                             Pod/nginx                                         Created container: nginx
10m                   Normal    Killing                             Pod/nginx                                         Stopping container nginx
10m                   Normal    Pulled                              Pod/nginx                                         Container image "mirror.gcr.io/nginx:1.27.4-bookworm" already present on machine
10m                   Normal    Scheduled                           Pod/nginx                                         Successfully assigned default/nginx to docker-desktop
10m                   Normal    Started                             Pod/nginx                                         Started container nginx
10m                   Normal    Created                             Pod/nginx                                         Created container: nginx
10m                   Normal    Killing                             Pod/nginx                                         Stopping container nginx
10m                   Normal    WaitForFirstConsumer                PersistentVolumeClaim/rabbitmq-data-rabbitmq-0    waiting for first consumer to be created before binding
10m                   Normal    SuccessfulCreate                    StatefulSet/rabbitmq                              create Claim rabbitmq-data-rabbitmq-0 Pod rabbitmq-0 in StatefulSet rabbitmq success
10m                   Normal    Provisioning                        PersistentVolumeClaim/rabbitmq-data-rabbitmq-0    External provisioner is provisioning volume for claim "default/rabbitmq-data-rabbitmq-0"
10m                   Normal    ExternalProvisioning                PersistentVolumeClaim/rabbitmq-data-rabbitmq-0    Waiting for a volume to be created either by the external provisioner 'microk8s.io/hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
10m                   Normal    ProvisioningSucceeded               PersistentVolumeClaim/rabbitmq-data-rabbitmq-0    Successfully provisioned volume pvc-478cebd3-fb98-49c5-a1ea-1514769ceadd
10m                   Normal    Created                             Pod/rabbitmq-0                                    Created container: rabbitmq-config
10m                   Normal    Pulled                              Pod/rabbitmq-0                                    Container image "mirror.gcr.io/busybox:1.37.0" already present on machine
10m                   Normal    Started                             Pod/rabbitmq-0                                    Started container rabbitmq-config
10m                   Normal    Scheduled                           Pod/rabbitmq-0                                    Successfully assigned default/rabbitmq-0 to docker-desktop
10m                   Normal    Pulling                             Pod/rabbitmq-0                                    Pulling image "mirror.gcr.io/rabbitmq:3.8.34"
9m59s                 Normal    Pulled                              Pod/rabbitmq-0                                    Successfully pulled image "mirror.gcr.io/rabbitmq:3.8.34" in 10.563s (10.563s including waiting). Image size: 220131704 bytes.
9m58s                 Normal    Started                             Pod/rabbitmq-0                                    Started container rabbitmq
9m58s                 Normal    Created                             Pod/rabbitmq-0                                    Created container: rabbitmq
<unknown>             Normal    Created                             RabbitMQ/pod/rabbitmq-0                           Node rabbit@rabbitmq-0 is registered
8m53s                 Normal    Killing                             Pod/rabbitmq-0                                    Stopping container rabbitmq
8m51s (x2 over 10m)   Normal    SuccessfulCreate                    StatefulSet/rabbitmq                              create Pod rabbitmq-0 in StatefulSet rabbitmq successful
8m50s                 Normal    Scheduled                           Pod/rabbitmq-0                                    Successfully assigned default/rabbitmq-0 to docker-desktop
8m50s                 Normal    Pulled                              Pod/rabbitmq-0                                    Container image "mirror.gcr.io/rabbitmq:3.8.34" already present on machine
8m50s                 Normal    Started                             Pod/rabbitmq-0                                    Started container rabbitmq-config
8m50s                 Normal    Created                             Pod/rabbitmq-0                                    Created container: rabbitmq-config
8m50s                 Normal    Pulled                              Pod/rabbitmq-0                                    Container image "mirror.gcr.io/busybox:1.37.0" already present on machine
8m49s                 Normal    Created                             Pod/rabbitmq-0                                    Created container: rabbitmq
8m49s                 Normal    Started                             Pod/rabbitmq-0                                    Started container rabbitmq
8m40s                 Normal    Starting                            Node/docker-desktop                               
<unknown>             Normal    Created                             RabbitMQ/pod/rabbitmq-0                           Node rabbit@rabbitmq-0 is registered
8m25s                 Normal    RegisteredNode                      Node/docker-desktop                               Node docker-desktop event: Registered Node docker-desktop in Controller
2m17s                 Normal    SuccessfulCreate                    ReplicaSet/generate-load-app                      Created pod: generate-load-app-p5wzq
2m17s                 Normal    SuccessfulCreate                    ReplicaSet/generate-load-app                      Created pod: generate-load-app-djx4q
2m17s                 Normal    SuccessfulCreate                    ReplicaSet/generate-load-app                      Created pod: generate-load-app-2gqqc
2m17s                 Normal    SuccessfulCreate                    ReplicaSet/generate-load-app                      Created pod: generate-load-app-9l7zz
2m17s                 Normal    SuccessfulCreate                    ReplicaSet/generate-load-app                      Created pod: generate-load-app-sx48m
2m16s                 Normal    Scheduled                           Pod/generate-load-app-sx48m                       Successfully assigned default/generate-load-app-sx48m to docker-desktop
2m16s                 Normal    Scheduled                           Pod/generate-load-app-2gqqc                       Successfully assigned default/generate-load-app-2gqqc to docker-desktop
2m16s                 Normal    Scheduled                           Pod/generate-load-app-9l7zz                       Successfully assigned default/generate-load-app-9l7zz to docker-desktop
2m16s                 Normal    Scheduled                           Pod/generate-load-app-djx4q                       Successfully assigned default/generate-load-app-djx4q to docker-desktop
2m16s                 Normal    Scheduled                           Pod/generate-load-app-p5wzq                       Successfully assigned default/generate-load-app-p5wzq to docker-desktop
2m16s                 Normal    Started                             Pod/generate-load-app-sx48m                       Started container generate-load-app
2m16s                 Normal    Created                             Pod/generate-load-app-9l7zz                       Created container: generate-load-app
2m16s                 Normal    Created                             Pod/generate-load-app-2gqqc                       Created container: generate-load-app
2m16s                 Normal    Pulled                              Pod/generate-load-app-2gqqc                       Container image "mirror.gcr.io/busybox:1.37.0" already present on machine
2m16s                 Normal    Started                             Pod/generate-load-app-djx4q                       Started container generate-load-app
2m16s                 Normal    Pulled                              Pod/generate-load-app-9l7zz                       Container image "mirror.gcr.io/busybox:1.37.0" already present on machine
2m16s                 Normal    Started                             Pod/generate-load-app-p5wzq                       Started container generate-load-app
2m16s                 Normal    Created                             Pod/generate-load-app-djx4q                       Created container: generate-load-app
2m16s                 Normal    Pulled                              Pod/generate-load-app-djx4q                       Container image "mirror.gcr.io/busybox:1.37.0" already present on machine
2m16s                 Normal    Created                             Pod/generate-load-app-p5wzq                       Created container: generate-load-app
2m16s                 Normal    Pulled                              Pod/generate-load-app-p5wzq                       Container image "mirror.gcr.io/busybox:1.37.0" already present on machine
2m16s                 Normal    Started                             Pod/generate-load-app-9l7zz                       Started container generate-load-app
2m16s                 Normal    Created                             Pod/generate-load-app-sx48m                       Created container: generate-load-app
2m16s                 Normal    Pulled                              Pod/generate-load-app-sx48m                       Container image "mirror.gcr.io/busybox:1.37.0" already present on machine
2m16s                 Normal    Started                             Pod/generate-load-app-2gqqc                       Started container generate-load-app
91s                   Normal    Killing                             Pod/generate-load-app-sx48m                       Stopping container generate-load-app
91s                   Normal    Killing                             Pod/generate-load-app-9l7zz                       Stopping container generate-load-app
91s                   Normal    Killing                             Pod/generate-load-app-p5wzq                       Stopping container generate-load-app
91s                   Normal    Killing                             Pod/generate-load-app-djx4q                       Stopping container generate-load-app
91s                   Normal    Killing                             Pod/generate-load-app-2gqqc                       Stopping container generate-load-app
80s                   Normal    SuccessfulRescale                   HorizontalPodAutoscaler/probe-test-app            New size: 4; reason: cpu resource utilization (percentage of request) above target
80s                   Normal    ScalingReplicaSet                   Deployment/probe-test-app                         Scaled up replica set probe-test-app-685956d4cf from 3 to 4
80s                   Normal    Created                             Pod/probe-test-app-685956d4cf-hnhlg               Created container: probe-test-app
80s                   Normal    SuccessfulCreate                    ReplicaSet/probe-test-app-685956d4cf              Created pod: probe-test-app-685956d4cf-hnhlg
80s                   Normal    Pulled                              Pod/probe-test-app-685956d4cf-hnhlg               Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
80s                   Normal    Scheduled                           Pod/probe-test-app-685956d4cf-hnhlg               Successfully assigned default/probe-test-app-685956d4cf-hnhlg to docker-desktop
79s                   Normal    Started                             Pod/probe-test-app-685956d4cf-hnhlg               Started container probe-test-app
45s                   Normal    SuccessfulCreate                    ReplicaSet/cpu-stressor-5d8fff5687                Created pod: cpu-stressor-5d8fff5687-gqfwk
45s                   Normal    ScalingReplicaSet                   Deployment/cpu-stressor                           Scaled up replica set cpu-stressor-5d8fff5687 from 0 to 1
44s                   Normal    Scheduled                           Pod/cpu-stressor-5d8fff5687-gqfwk                 Successfully assigned default/cpu-stressor-5d8fff5687-gqfwk to docker-desktop
44s                   Normal    Pulling                             Pod/cpu-stressor-5d8fff5687-gqfwk                 Pulling image "mirror.gcr.io/narmidm/k8s-pod-cpu-stressor:v1.2.0"
37s                   Normal    Started                             Pod/cpu-stressor-5d8fff5687-gqfwk                 Started container cpu-stressor
37s                   Normal    Created                             Pod/cpu-stressor-5d8fff5687-gqfwk                 Created container: cpu-stressor
37s                   Normal    Pulled                              Pod/cpu-stressor-5d8fff5687-gqfwk                 Successfully pulled image "mirror.gcr.io/narmidm/k8s-pod-cpu-stressor:v1.2.0" in 7.046s (7.046s including waiting). Image size: 9842826 bytes.
34s                   Normal    Killing                             Pod/cpu-stressor-5d8fff5687-gqfwk                 Stopping container cpu-stressor
33s                   Normal    Pulling                             Pod/memory-stressor                               Pulling image "mirror.gcr.io/polinux/stress:1.0.4"
33s                   Normal    Scheduled                           Pod/memory-stressor                               Successfully assigned default/memory-stressor to docker-desktop
27s (x2 over 27s)     Normal    Created                             Pod/memory-stressor                               Created container: memory-stressor
27s                   Normal    Pulled                              Pod/memory-stressor                               Container image "mirror.gcr.io/polinux/stress:1.0.4" already present on machine
27s                   Normal    Pulled                              Pod/memory-stressor                               Successfully pulled image "mirror.gcr.io/polinux/stress:1.0.4" in 5.523s (5.523s including waiting). Image size: 9744175 bytes.
27s (x2 over 27s)     Normal    Started                             Pod/memory-stressor                               Started container memory-stressor
25s (x2 over 26s)     Warning   BackOff                             Pod/memory-stressor                               Back-off restarting failed container memory-stressor in pod memory-stressor_default(811b9aae-a1cf-411d-8026-ad679ca8f8dc)
23s                   Normal    SuccessfulRescale                   HorizontalPodAutoscaler/probe-test-app            New size: 5; reason: cpu resource utilization (percentage of request) above target
23s                   Normal    ScalingReplicaSet                   Deployment/probe-test-app                         Scaled up replica set probe-test-app-685956d4cf from 4 to 5
22s                   Normal    Created                             Pod/probe-test-app-685956d4cf-q6zq5               Created container: probe-test-app
22s                   Normal    SuccessfulCreate                    ReplicaSet/probe-test-app-685956d4cf              Created pod: probe-test-app-685956d4cf-q6zq5
22s                   Normal    Pulled                              Pod/probe-test-app-685956d4cf-q6zq5               Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
22s                   Normal    Started                             Pod/probe-test-app-685956d4cf-q6zq5               Started container probe-test-app
22s                   Normal    Scheduled                           Pod/probe-test-app-685956d4cf-q6zq5               Successfully assigned default/probe-test-app-685956d4cf-q6zq5 to docker-desktop
21s                   Warning   Unhealthy                           Pod/probe-test-app-685956d4cf-q6zq5               Readiness probe failed: Get "http://10.1.0.53:8080/readyz": dial tcp 10.1.0.53:8080: connect: connection refused
5s                    Normal    ScalingReplicaSet                   Deployment/rabbitmq-consumer                      Scaled up replica set rabbitmq-consumer-54454cf965 from 0 to 1
5s                    Normal    SuccessfulCreate                    ReplicaSet/rabbitmq-consumer-54454cf965           Created pod: rabbitmq-consumer-54454cf965-66mjc
4s                    Normal    Scheduled                           Pod/rabbitmq-consumer-54454cf965-66mjc            Successfully assigned default/rabbitmq-consumer-54454cf965-66mjc to docker-desktop
4s                    Normal    Pulling                             Pod/rabbitmq-consumer-54454cf965-66mjc            Pulling image "ghcr.io/kedacore/rabbitmq-client:v1.0"
2s                    Normal    Pulling                             Pod/rabbitmq-publish-kbq2x                        Pulling image "ghcr.io/kedacore/rabbitmq-client:v1.0"
2s                    Normal    SuccessfulCreate                    Job/rabbitmq-publish                              Created pod: rabbitmq-publish-kbq2x
2s                    Normal    Scheduled                           Pod/rabbitmq-publish-kbq2x                        Successfully assigned default/rabbitmq-publish-kbq2x to docker-desktop
1s                    Normal    TriggerAuthenticationAdded          TriggerAuthentication/rabbitmq-consumer-trigger   New TriggerAuthentication configured
--------------------
kubectl describe job rabbitmq-publish
Name:             rabbitmq-publish
Namespace:        default
Selector:         batch.kubernetes.io/controller-uid=7d1da6fd-e43c-4d6f-b50c-cbeb0f1faffb
Labels:           batch.kubernetes.io/controller-uid=7d1da6fd-e43c-4d6f-b50c-cbeb0f1faffb
                  batch.kubernetes.io/job-name=rabbitmq-publish
                  controller-uid=7d1da6fd-e43c-4d6f-b50c-cbeb0f1faffb
                  job-name=rabbitmq-publish
Annotations:      <none>
Parallelism:      1
Completions:      1
Completion Mode:  NonIndexed
Suspend:          false
Backoff Limit:    4
Start Time:       Tue, 15 Apr 2025 22:37:04 +1000
Pods Statuses:    1 Active (0 Ready) / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  batch.kubernetes.io/controller-uid=7d1da6fd-e43c-4d6f-b50c-cbeb0f1faffb
           batch.kubernetes.io/job-name=rabbitmq-publish
           controller-uid=7d1da6fd-e43c-4d6f-b50c-cbeb0f1faffb
           job-name=rabbitmq-publish
  Containers:
   rabbitmq-client:
    Image:      ghcr.io/kedacore/rabbitmq-client:v1.0
    Port:       <none>
    Host Port:  <none>
    Command:
      send
      $(rabbitmq_host)
      300
    Environment:
      rabbitmq_host:  <set to the key 'host' in secret 'rabbitmq-consumer-secret'>  Optional: false
    Mounts:           <none>
  Volumes:            <none>
  Node-Selectors:     <none>
  Tolerations:        <none>
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  3s    job-controller  Created pod: rabbitmq-publish-kbq2x
--------------------
cd ../cronjob
--------------------
kubectl apply -f cronjob.yaml
cronjob.batch/hello created
--------------------
kubectl get pods
NAME                                 READY   STATUS      RESTARTS   AGE
hello-29078678-nncqn                 0/1     Completed   0          69s
hello-29078679-xk5x4                 0/1     Completed   0          9s
probe-test-app-685956d4cf-cm59g      1/1     Running     0          14m
probe-test-app-685956d4cf-hnhlg      1/1     Running     0          3m23s
probe-test-app-685956d4cf-hvcmn      1/1     Running     0          14m
probe-test-app-685956d4cf-q6zq5      1/1     Running     0          2m26s
probe-test-app-685956d4cf-xrcbz      1/1     Running     0          14m
rabbitmq-0                           1/1     Running     0          10m
rabbitmq-consumer-54454cf965-2fhdz   1/1     Running     0          40s
rabbitmq-consumer-54454cf965-66mjc   1/1     Running     0          2m8s
rabbitmq-consumer-54454cf965-kfkms   1/1     Running     0          40s
rabbitmq-consumer-54454cf965-nbsvl   1/1     Running     0          40s
rabbitmq-publish-kbq2x               0/1     Completed   0          2m5s
--------------------
kubectl get cronjob
NAME    SCHEDULE    TIMEZONE   SUSPEND   ACTIVE   LAST SCHEDULE   AGE
hello   * * * * *   <none>     False     0        10s             2m2s
--------------------
kubectl delete cronjob hello
cronjob.batch "hello" deleted
--------------------
kubectl get pods -A
NAMESPACE     NAME                                                     READY   STATUS      RESTARTS        AGE
default       probe-test-app-685956d4cf-cm59g                          1/1     Running     0               15m
default       probe-test-app-685956d4cf-hnhlg                          1/1     Running     0               3m27s
default       probe-test-app-685956d4cf-hvcmn                          1/1     Running     0               14m
default       probe-test-app-685956d4cf-q6zq5                          1/1     Running     0               2m30s
default       probe-test-app-685956d4cf-xrcbz                          1/1     Running     0               14m
default       rabbitmq-0                                               1/1     Running     0               10m
default       rabbitmq-consumer-54454cf965-2fhdz                       1/1     Running     0               44s
default       rabbitmq-consumer-54454cf965-66mjc                       1/1     Running     0               2m12s
default       rabbitmq-consumer-54454cf965-kfkms                       1/1     Running     0               44s
default       rabbitmq-consumer-54454cf965-nbsvl                       1/1     Running     0               44s
default       rabbitmq-publish-kbq2x                                   0/1     Completed   0               2m9s
keda          keda-admission-webhooks-657667f97d-hw8vg                 1/1     Running     0               2m23s
keda          keda-operator-887598c9b-b9r7s                            1/1     Running     1 (2m7s ago)    2m23s
keda          keda-operator-metrics-apiserver-7f5d6df58b-zcchg         1/1     Running     0               2m23s
kube-system   coredns-668d6bf9bc-7zcr5                                 1/1     Running     0               16m
kube-system   coredns-668d6bf9bc-hcts5                                 1/1     Running     0               16m
kube-system   etcd-docker-desktop                                      1/1     Running     0               8m48s
kube-system   hostpath-provisioner-7c8dddfbc4-vqzqx                    1/1     Running     1 (9m49s ago)   13m
kube-system   kube-apiserver-docker-desktop                            1/1     Running     1 (18m ago)     16m
kube-system   kube-controller-manager-docker-desktop                   1/1     Running     0               10m
kube-system   kube-proxy-kxzqm                                         1/1     Running     0               10m
kube-system   kube-scheduler-docker-desktop                            1/1     Running     1 (9m43s ago)   10m
monitoring    adapter-prometheus-adapter-57b7646776-7d8vp              1/1     Running     0               7m19s
monitoring    alertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running     0               6m55s
monitoring    prometheus-grafana-6854b47bf4-cx985                      3/3     Running     0               7m28s
monitoring    prometheus-kube-prometheus-operator-7f8d744cd7-st2b4     1/1     Running     0               7m28s
monitoring    prometheus-kube-state-metrics-f699c577d-8pz9c            1/1     Running     0               7m28s
monitoring    prometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running     0               6m55s
monitoring    prometheus-prometheus-node-exporter-v6xj6                1/1     Running     0               7m28s
--------------------
kubectl api-resources
NAME                                SHORTNAMES               APIVERSION                        NAMESPACED   KIND
bindings                                                     v1                                true         Binding
componentstatuses                   cs                       v1                                false        ComponentStatus
configmaps                          cm                       v1                                true         ConfigMap
endpoints                           ep                       v1                                true         Endpoints
events                              ev                       v1                                true         Event
limitranges                         limits                   v1                                true         LimitRange
namespaces                          ns                       v1                                false        Namespace
nodes                               no                       v1                                false        Node
persistentvolumeclaims              pvc                      v1                                true         PersistentVolumeClaim
persistentvolumes                   pv                       v1                                false        PersistentVolume
pods                                po                       v1                                true         Pod
podtemplates                                                 v1                                true         PodTemplate
replicationcontrollers              rc                       v1                                true         ReplicationController
resourcequotas                      quota                    v1                                true         ResourceQuota
secrets                                                      v1                                true         Secret
serviceaccounts                     sa                       v1                                true         ServiceAccount
services                            svc                      v1                                true         Service
mutatingwebhookconfigurations                                admissionregistration.k8s.io/v1   false        MutatingWebhookConfiguration
validatingadmissionpolicies                                  admissionregistration.k8s.io/v1   false        ValidatingAdmissionPolicy
validatingadmissionpolicybindings                            admissionregistration.k8s.io/v1   false        ValidatingAdmissionPolicyBinding
validatingwebhookconfigurations                              admissionregistration.k8s.io/v1   false        ValidatingWebhookConfiguration
customresourcedefinitions           crd,crds                 apiextensions.k8s.io/v1           false        CustomResourceDefinition
apiservices                                                  apiregistration.k8s.io/v1         false        APIService
controllerrevisions                                          apps/v1                           true         ControllerRevision
daemonsets                          ds                       apps/v1                           true         DaemonSet
deployments                         deploy                   apps/v1                           true         Deployment
replicasets                         rs                       apps/v1                           true         ReplicaSet
statefulsets                        sts                      apps/v1                           true         StatefulSet
selfsubjectreviews                                           authentication.k8s.io/v1          false        SelfSubjectReview
tokenreviews                                                 authentication.k8s.io/v1          false        TokenReview
localsubjectaccessreviews                                    authorization.k8s.io/v1           true         LocalSubjectAccessReview
selfsubjectaccessreviews                                     authorization.k8s.io/v1           false        SelfSubjectAccessReview
selfsubjectrulesreviews                                      authorization.k8s.io/v1           false        SelfSubjectRulesReview
subjectaccessreviews                                         authorization.k8s.io/v1           false        SubjectAccessReview
horizontalpodautoscalers            hpa                      autoscaling/v2                    true         HorizontalPodAutoscaler
cronjobs                            cj                       batch/v1                          true         CronJob
jobs                                                         batch/v1                          true         Job
certificatesigningrequests          csr                      certificates.k8s.io/v1            false        CertificateSigningRequest
leases                                                       coordination.k8s.io/v1            true         Lease
endpointslices                                               discovery.k8s.io/v1               true         EndpointSlice
cloudeventsources                                            eventing.keda.sh/v1alpha1         true         CloudEventSource
clustercloudeventsources                                     eventing.keda.sh/v1alpha1         false        ClusterCloudEventSource
events                              ev                       events.k8s.io/v1                  true         Event
flowschemas                                                  flowcontrol.apiserver.k8s.io/v1   false        FlowSchema
prioritylevelconfigurations                                  flowcontrol.apiserver.k8s.io/v1   false        PriorityLevelConfiguration
clustertriggerauthentications       cta,clustertriggerauth   keda.sh/v1alpha1                  false        ClusterTriggerAuthentication
scaledjobs                          sj                       keda.sh/v1alpha1                  true         ScaledJob
scaledobjects                       so                       keda.sh/v1alpha1                  true         ScaledObject
triggerauthentications              ta,triggerauth           keda.sh/v1alpha1                  true         TriggerAuthentication
nodes                                                        metrics.k8s.io/v1beta1            false        NodeMetrics
pods                                                         metrics.k8s.io/v1beta1            true         PodMetrics
alertmanagerconfigs                 amcfg                    monitoring.coreos.com/v1alpha1    true         AlertmanagerConfig
alertmanagers                       am                       monitoring.coreos.com/v1          true         Alertmanager
podmonitors                         pmon                     monitoring.coreos.com/v1          true         PodMonitor
probes                              prb                      monitoring.coreos.com/v1          true         Probe
prometheusagents                    promagent                monitoring.coreos.com/v1alpha1    true         PrometheusAgent
prometheuses                        prom                     monitoring.coreos.com/v1          true         Prometheus
prometheusrules                     promrule                 monitoring.coreos.com/v1          true         PrometheusRule
scrapeconfigs                       scfg                     monitoring.coreos.com/v1alpha1    true         ScrapeConfig
servicemonitors                     smon                     monitoring.coreos.com/v1          true         ServiceMonitor
thanosrulers                        ruler                    monitoring.coreos.com/v1          true         ThanosRuler
ingressclasses                                               networking.k8s.io/v1              false        IngressClass
ingresses                           ing                      networking.k8s.io/v1              true         Ingress
networkpolicies                     netpol                   networking.k8s.io/v1              true         NetworkPolicy
runtimeclasses                                               node.k8s.io/v1                    false        RuntimeClass
poddisruptionbudgets                pdb                      policy/v1                         true         PodDisruptionBudget
clusterrolebindings                                          rbac.authorization.k8s.io/v1      false        ClusterRoleBinding
clusterroles                                                 rbac.authorization.k8s.io/v1      false        ClusterRole
rolebindings                                                 rbac.authorization.k8s.io/v1      true         RoleBinding
roles                                                        rbac.authorization.k8s.io/v1      true         Role
priorityclasses                     pc                       scheduling.k8s.io/v1              false        PriorityClass
csidrivers                                                   storage.k8s.io/v1                 false        CSIDriver
csinodes                                                     storage.k8s.io/v1                 false        CSINode
csistoragecapacities                                         storage.k8s.io/v1                 true         CSIStorageCapacity
storageclasses                      sc                       storage.k8s.io/v1                 false        StorageClass
volumeattachments                                            storage.k8s.io/v1                 false        VolumeAttachment
--------------------
kubectl get clusterrole admin -o yaml
aggregationRule:
  clusterRoleSelectors:
  - matchLabels:
      rbac.authorization.k8s.io/aggregate-to-admin: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  creationTimestamp: "2025-04-14T12:59:35Z"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: admin
  resourceVersion: "343"
  uid: 9b55513d-ee2a-4778-94c4-49415e5d6a68
rules:
- apiGroups:
  - ""
  resources:
  - pods/attach
  - pods/exec
  - pods/portforward
  - pods/proxy
  - secrets
  - services/proxy
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - serviceaccounts
  verbs:
  - impersonate
- apiGroups:
  - ""
  resources:
  - pods
  - pods/attach
  - pods/exec
  - pods/portforward
  - pods/proxy
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - ""
  resources:
  - pods/eviction
  verbs:
  - create
- apiGroups:
  - ""
  resources:
  - configmaps
  - events
  - persistentvolumeclaims
  - replicationcontrollers
  - replicationcontrollers/scale
  - secrets
  - serviceaccounts
  - services
  - services/proxy
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - ""
  resources:
  - serviceaccounts/token
  verbs:
  - create
- apiGroups:
  - apps
  resources:
  - daemonsets
  - deployments
  - deployments/rollback
  - deployments/scale
  - replicasets
  - replicasets/scale
  - statefulsets
  - statefulsets/scale
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - autoscaling
  resources:
  - horizontalpodautoscalers
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - batch
  resources:
  - cronjobs
  - jobs
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - extensions
  resources:
  - daemonsets
  - deployments
  - deployments/rollback
  - deployments/scale
  - ingresses
  - networkpolicies
  - replicasets
  - replicasets/scale
  - replicationcontrollers/scale
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  - networkpolicies
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - ""
  resources:
  - configmaps
  - endpoints
  - persistentvolumeclaims
  - persistentvolumeclaims/status
  - pods
  - replicationcontrollers
  - replicationcontrollers/scale
  - serviceaccounts
  - services
  - services/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - bindings
  - events
  - limitranges
  - namespaces/status
  - pods/log
  - pods/status
  - replicationcontrollers/status
  - resourcequotas
  - resourcequotas/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - discovery.k8s.io
  resources:
  - endpointslices
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - apps
  resources:
  - controllerrevisions
  - daemonsets
  - daemonsets/status
  - deployments
  - deployments/scale
  - deployments/status
  - replicasets
  - replicasets/scale
  - replicasets/status
  - statefulsets
  - statefulsets/scale
  - statefulsets/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - autoscaling
  resources:
  - horizontalpodautoscalers
  - horizontalpodautoscalers/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - batch
  resources:
  - cronjobs
  - cronjobs/status
  - jobs
  - jobs/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - extensions
  resources:
  - daemonsets
  - daemonsets/status
  - deployments
  - deployments/scale
  - deployments/status
  - ingresses
  - ingresses/status
  - networkpolicies
  - replicasets
  - replicasets/scale
  - replicasets/status
  - replicationcontrollers/scale
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets
  - poddisruptionbudgets/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  - ingresses/status
  - networkpolicies
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - authorization.k8s.io
  resources:
  - localsubjectaccessreviews
  verbs:
  - create
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  - rolebindings
  - roles
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch
--------------------
kubectl get clusterrole admin -o yaml | wc -l
315
--------------------
cd ../k8s-authz
--------------------
./setup-tokens-on-cluster.sh
--------------------
./add-users-kubeconfig.sh
Context "docker-desktop-jane" created.
Context "docker-desktop-john" created.
--------------------
cat team1.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: "team1"
  labels:
    name: "team1"
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: admin
  namespace: team1
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["*"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: admin
  namespace: team1
subjects:
- kind: User
  name: jane
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: admin
  apiGroup: rbac.authorization.k8s.io--------------------
kubectl apply -f team1.yaml && kubectl apply -f team2.yaml
namespace/team1 created
role.rbac.authorization.k8s.io/admin created
rolebinding.rbac.authorization.k8s.io/admin created
namespace/team2 created
role.rbac.authorization.k8s.io/admin created
rolebinding.rbac.authorization.k8s.io/admin created
--------------------
kubectl config get-contexts
CURRENT   NAME                  CLUSTER          AUTHINFO         NAMESPACE
*         docker-desktop        docker-desktop   docker-desktop   
          docker-desktop-jane   docker-desktop   jane             team1
          docker-desktop-john   docker-desktop   john             team2
--------------------
kubectl config use-context docker-desktop-jane
Switched to context "docker-desktop-jane".
--------------------
kubectl get pods -A
Error from server (Forbidden): pods is forbidden: User "jane" cannot list resource "pods" in API group "" at the cluster scope
--------------------
kubectl get pods
No resources found in team1 namespace.
--------------------
kubectl config use-context docker-desktop-john
Switched to context "docker-desktop-john".
--------------------
kubectl get pods
No resources found in team2 namespace.
--------------------
kubectl get pods --namespace=team1
Error from server (Forbidden): pods is forbidden: User "john" cannot list resource "pods" in API group "" in the namespace "team1"
--------------------
kubectl config use-context docker-desktop
Switched to context "docker-desktop".
--------------------
Cleaning up Jane and John...
deleted user jane from /home/jumiker/.kube/config
deleted user john from /home/jumiker/.kube/config
deleted context docker-desktop-jane from /home/jumiker/.kube/config
deleted context docker-desktop-john from /home/jumiker/.kube/config
--------------------
cd ../ingress
--------------------
./install-nginx.sh
"ingress-nginx" already exists with the same configuration, skipping
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "kedacore" chart repository
...Successfully got an update from the "ingress-nginx" chart repository
...Successfully got an update from the "argo-helm" chart repository
...Successfully got an update from the "kiali" chart repository
...Successfully got an update from the "istio" chart repository
...Successfully got an update from the "gatekeeper" chart repository
...Successfully got an update from the "prometheus-community" chart repository
Update Complete. ⎈Happy Helming!⎈
NAME: ingress
LAST DEPLOYED: Tue Apr 15 22:40:15 2025
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
The ingress-nginx controller has been installed.
It may take a few minutes for the load balancer IP to be available.
You can watch the status by running 'kubectl get service --namespace default ingress-ingress-nginx-controller --output wide --watch'

An example Ingress that makes use of the controller:
  apiVersion: networking.k8s.io/v1
  kind: Ingress
  metadata:
    name: example
    namespace: foo
  spec:
    ingressClassName: nginx
    rules:
      - host: www.example.com
        http:
          paths:
            - pathType: Prefix
              backend:
                service:
                  name: exampleService
                  port:
                    number: 80
              path: /
    # This section is only required if TLS is to be enabled for the Ingress
    tls:
      - hosts:
        - www.example.com
        secretName: example-tls

If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided:

  apiVersion: v1
  kind: Secret
  metadata:
    name: example-tls
    namespace: foo
  data:
    tls.crt: <base64 encoded cert>
    tls.key: <base64 encoded key>
  type: kubernetes.io/tls
Waiting for deployment "ingress-ingress-nginx-controller" rollout to finish: 0 of 1 updated replicas are available...
deployment "ingress-ingress-nginx-controller" successfully rolled out
--------------------
kubectl apply -f probe-test-app-ingress.yaml
ingress.networking.k8s.io/probe-test-app created
--------------------
curl http://localhost
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
curl: (7) Failed to connect to localhost port 80 after 0 ms: Couldn't connect to server
--------------------
kubectl apply -f nyancat.yaml
deployment.apps/nyancat created
service/nyancat created
--------------------
kubectl rollout status deployment nyancat -n default
Waiting for deployment "nyancat" rollout to finish: 0 of 1 updated replicas are available...
deployment "nyancat" successfully rolled out
--------------------
kubectl apply -f nyancat-ingress.yaml
ingress.networking.k8s.io/probe-test-app configured
--------------------
curl http://localhost/nyancat/
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
curl: (7) Failed to connect to localhost port 80 after 0 ms: Couldn't connect to server
--------------------
kubectl delete ingress probe-test-app
ingress.networking.k8s.io "probe-test-app" deleted
--------------------
helm uninstall ingress
release "ingress" uninstalled
--------------------
Cleaning up probe-test-app and nyancat...
horizontalpodautoscaler.autoscaling "probe-test-app" deleted
deployment.apps "probe-test-app" deleted
deployment.apps "nyancat" deleted
service "probe-test-app" deleted
service "nyancat" deleted
--------------------
cd ../istio
--------------------
./install-istio.sh
"istio" already exists with the same configuration, skipping
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "kiali" chart repository
...Successfully got an update from the "gatekeeper" chart repository
...Successfully got an update from the "kedacore" chart repository
...Successfully got an update from the "ingress-nginx" chart repository
...Successfully got an update from the "argo-helm" chart repository
...Successfully got an update from the "prometheus-community" chart repository
...Successfully got an update from the "istio" chart repository
Update Complete. ⎈Happy Helming!⎈
namespace/istio-system created
NAME: istio-base
LAST DEPLOYED: Tue Apr 15 22:42:05 2025
NAMESPACE: istio-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Istio base successfully installed!

To learn more about the release, try:
  $ helm status istio-base -n istio-system
  $ helm get all istio-base -n istio-system
NAME: istiod
LAST DEPLOYED: Tue Apr 15 22:42:08 2025
NAMESPACE: istio-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
"istiod" successfully installed!

To learn more about the release, try:
  $ helm status istiod -n istio-system
  $ helm get all istiod -n istio-system

Next steps:
  * Deploy a Gateway: https://istio.io/latest/docs/setup/additional-setup/gateway/
  * Try out our tasks to get started on common configurations:
    * https://istio.io/latest/docs/tasks/traffic-management
    * https://istio.io/latest/docs/tasks/security/
    * https://istio.io/latest/docs/tasks/policy-enforcement/
  * Review the list of actively supported releases, CVE publications and our hardening guide:
    * https://istio.io/latest/docs/releases/supported-releases/
    * https://istio.io/latest/news/security/
    * https://istio.io/latest/docs/ops/best-practices/security/

For further documentation see https://istio.io website
"kiali" already exists with the same configuration, skipping
NAME: kiali-server
LAST DEPLOYED: Tue Apr 15 22:42:23 2025
NAMESPACE: istio-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Welcome to Kiali! For more details on Kiali, see: https://kiali.io

The Kiali Server [v2.8.0] has been installed in namespace [istio-system]. It will be ready soon.

When installing with "deployment.cluster_wide_access=false" using this Kiali Server Helm Chart,
it is your responsibility to manually create the proper Roles and RoleBindings for the Kiali Server
to have the correct permissions to access the service mesh namespaces.

(Helm: Chart=[kiali-server], Release=[kiali-server], Version=[2.8.0])
servicemonitor.monitoring.coreos.com/prometheus-oper-istio-controlplane created
servicemonitor.monitoring.coreos.com/prometheus-oper-istio-dataplane created
customresourcedefinition.apiextensions.k8s.io/gatewayclasses.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/gateways.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/grpcroutes.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/httproutes.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/referencegrants.gateway.networking.k8s.io created
--------------------
kubectl label namespace default istio-injection=enabled
namespace/default labeled
--------------------
kubectl apply -f bookinfo/platform/kube/bookinfo.yaml
service/details created
serviceaccount/bookinfo-details created
deployment.apps/details-v1 created
service/ratings created
serviceaccount/bookinfo-ratings created
deployment.apps/ratings-v1 created
service/reviews created
serviceaccount/bookinfo-reviews created
deployment.apps/reviews-v1 created
deployment.apps/reviews-v2 created
deployment.apps/reviews-v3 created
service/productpage created
serviceaccount/bookinfo-productpage created
deployment.apps/productpage-v1 created
--------------------
kubectl apply -f bookinfo/gateway-api/bookinfo-gateway.yaml
gateway.gateway.networking.k8s.io/bookinfo-gateway created
httproute.gateway.networking.k8s.io/bookinfo created
--------------------
kubectl apply -f bookinfo/platform/kube/bookinfo-versions.yaml
service/reviews-v1 created
service/reviews-v2 created
service/reviews-v3 created
service/productpage-v1 created
service/ratings-v1 created
service/details-v1 created
--------------------
kubectl apply -f bookinfo/gateway-api/route-all-v1.yaml
httproute.gateway.networking.k8s.io/reviews created
httproute.gateway.networking.k8s.io/productpage created
httproute.gateway.networking.k8s.io/ratings created
httproute.gateway.networking.k8s.io/details created
--------------------
kubectl apply -f bookinfo/gateway-api/route-reviews-90-10.yaml
httproute.gateway.networking.k8s.io/reviews configured
--------------------
kubectl apply -f bookinfo/gateway-api/route-jason-v2.yaml
httproute.gateway.networking.k8s.io/reviews configured
--------------------
bookinfo/platform/kube/cleanup.sh
using NAMESPACE=default
gateway.gateway.networking.k8s.io "bookinfo-gateway" deleted
httproute.gateway.networking.k8s.io "bookinfo" deleted
httproute.gateway.networking.k8s.io "details" deleted
httproute.gateway.networking.k8s.io "productpage" deleted
httproute.gateway.networking.k8s.io "ratings" deleted
httproute.gateway.networking.k8s.io "reviews" deleted
Application cleanup may take up to one minute
service "details" deleted
serviceaccount "bookinfo-details" deleted
deployment.apps "details-v1" deleted
service "ratings" deleted
serviceaccount "bookinfo-ratings" deleted
deployment.apps "ratings-v1" deleted
service "reviews" deleted
serviceaccount "bookinfo-reviews" deleted
deployment.apps "reviews-v1" deleted
deployment.apps "reviews-v2" deleted
deployment.apps "reviews-v3" deleted
service "productpage" deleted
serviceaccount "bookinfo-productpage" deleted
deployment.apps "productpage-v1" deleted
Application cleanup successful
--------------------
cd ../kustomize
--------------------
kustomize build prod
apiVersion: v1
kind: Service
metadata:
  labels:
    run: my-nginx
  name: prod-my-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
  selector:
    run: my-nginx
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prod-my-nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      run: my-nginx
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - image: mirror.gcr.io/nginx:1.27.4-bookworm
        name: my-nginx
--------------------
kubectl apply -k prod
service/prod-my-nginx created
deployment.apps/prod-my-nginx created
--------------------
kubectl get pods
NAME                             READY   STATUS      RESTARTS   AGE
prod-my-nginx-5c8c9cc974-9hzx2   0/2     Init:0/1    0          2s
prod-my-nginx-5c8c9cc974-fpv5j   0/2     Init:0/1    0          2s
rabbitmq-0                       1/1     Running     0          15m
rabbitmq-publish-kbq2x           0/1     Completed   0          6m35s
--------------------
kubectl apply -k dev
service/dev-my-nginx created
deployment.apps/dev-my-nginx created
--------------------
kubectl get pods
NAME                             READY   STATUS            RESTARTS   AGE
dev-my-nginx-5c8c9cc974-gtcnk    0/2     PodInitializing   0          1s
prod-my-nginx-5c8c9cc974-9hzx2   2/2     Running           0          4s
prod-my-nginx-5c8c9cc974-fpv5j   2/2     Running           0          4s
rabbitmq-0                       1/1     Running           0          15m
rabbitmq-publish-kbq2x           0/1     Completed         0          6m37s
--------------------
Cleaning up Kustomization example...
service "prod-my-nginx" deleted
deployment.apps "prod-my-nginx" deleted
service "dev-my-nginx" deleted
deployment.apps "dev-my-nginx" deleted
--------------------
helm ls -A
NAME        	NAMESPACE   	REVISION	UPDATED                                 	STATUS  	CHART                       	APP VERSION
adapter     	monitoring  	1       	2025-04-15 22:31:53.781020847 +1000 AEST	deployed	prometheus-adapter-4.14.1   	v0.12.0    
istio-base  	istio-system	1       	2025-04-15 22:42:05.674896129 +1000 AEST	deployed	base-1.25.1                 	1.25.1     
istiod      	istio-system	1       	2025-04-15 22:42:08.035385612 +1000 AEST	deployed	istiod-1.25.1               	1.25.1     
keda        	keda        	1       	2025-04-15 22:36:49.556056174 +1000 AEST	deployed	keda-2.17.0                 	2.17.0     
kiali-server	istio-system	1       	2025-04-15 22:42:23.550287788 +1000 AEST	deployed	kiali-server-2.8.0          	v2.8.0     
prometheus  	monitoring  	1       	2025-04-15 22:30:48.197176837 +1000 AEST	deployed	kube-prometheus-stack-70.4.1	v0.81.0    
--------------------
helm upgrade prometheus prometheus-community/kube-prometheus-stack --version 70.4.2 -n monitoring
Release "prometheus" has been upgraded. Happy Helming!
NAME: prometheus
LAST DEPLOYED: Tue Apr 15 22:43:48 2025
NAMESPACE: monitoring
STATUS: deployed
REVISION: 2
NOTES:
kube-prometheus-stack has been installed. Check its status by running:
  kubectl --namespace monitoring get pods -l "release=prometheus"

Get Grafana 'admin' user password by running:

  kubectl --namespace monitoring get secrets prometheus-grafana -o jsonpath="{.data.admin-password}" | base64 -d ; echo

Access Grafana local instance:

  export POD_NAME=$(kubectl --namespace monitoring get pod -l "app.kubernetes.io/name=grafana,app.kubernetes.io/instance=prometheus" -oname)
  kubectl --namespace monitoring port-forward $POD_NAME 3000

Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create & configure Alertmanager and Prometheus instances using the Operator.
--------------------
helm get values prometheus -n monitoring
USER-SUPPLIED VALUES:
crds:
  upgradeJob:
    enabled: true
grafana:
  service:
    port: 3000
    type: LoadBalancer
kubelet:
  serviceMonitor:
    cAdvisorMetricRelabelings: null
prometheus:
  service:
    type: LoadBalancer
prometheus-node-exporter:
  hostRootFsMount:
    enabled: false
  prometheus:
    monitor:
      attachMetadata:
        node: true
      relabelings:
      - action: replace
        regex: (.+)
        replacement: ${1}
        sourceLabels:
        - __meta_kubernetes_endpoint_node_name
        targetLabel: node
--------------------
helm repo add gatekeeper https://open-policy-agent.github.io/gatekeeper/charts
"gatekeeper" already exists with the same configuration, skipping
--------------------
helm install gatekeeper/gatekeeper --name-template=gatekeeper --namespace gatekeeper-system --create-namespace --version 3.19.0
NAME: gatekeeper
LAST DEPLOYED: Tue Apr 15 22:44:27 2025
NAMESPACE: gatekeeper-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
--------------------
cd ../opa-gatekeeper
--------------------
kubectl apply -f k8srequiredlabels-constraint-template.yaml
constrainttemplate.templates.gatekeeper.sh/k8srequiredlabels created
--------------------
kubectl apply -f pods-in-default-must-have-owner.yaml
k8srequiredlabels.constraints.gatekeeper.sh/pods-in-default-must-have-owner created
--------------------
kubectl apply -f ../probe-test-app/probe-test-app-pod.yaml
Error from server (Forbidden): error when creating "../probe-test-app/probe-test-app-pod.yaml": admission webhook "validation.gatekeeper.sh" denied the request: [pods-in-default-must-have-owner] missing required label, requires all of: owner
[pods-in-default-must-have-owner] regex mismatch
--------------------
kubectl apply -f probe-test-app-pod.yaml
pod/probe-test-app created
--------------------
kubectl delete constraint pods-in-default-must-have-owner
k8srequiredlabels.constraints.gatekeeper.sh "pods-in-default-must-have-owner" deleted
--------------------
kubectl delete pod probe-test-app
pod "probe-test-app" deleted
--------------------
helm repo add argo-helm https://argoproj.github.io/argo-helm
"argo-helm" already exists with the same configuration, skipping
--------------------
helm install argo-cd argo-helm/argo-cd --namespace argocd --create-namespace --version 7.8.24
NAME: argo-cd
LAST DEPLOYED: Tue Apr 15 22:45:43 2025
NAMESPACE: argocd
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
In order to access the server UI you have the following options:

1. kubectl port-forward service/argo-cd-argocd-server -n argocd 8080:443

    and then open the browser on http://localhost:8080 and accept the certificate

2. enable ingress in the values file `server.ingress.enabled` and either
      - Add the annotation for ssl passthrough: https://argo-cd.readthedocs.io/en/stable/operator-manual/ingress/#option-1-ssl-passthrough
      - Set the `configs.params."server.insecure"` in the values file and terminate SSL at your ingress: https://argo-cd.readthedocs.io/en/stable/operator-manual/ingress/#option-2-multiple-ingress-objects-and-hosts


After reaching the UI the first time you can login with username: admin and the random password generated during the installation. You can find the password by running:

kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d

(You should delete the initial secret afterwards as suggested by the Getting Started Guide: https://argo-cd.readthedocs.io/en/stable/getting_started/#4-login-using-the-cli)
--------------------
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath={.data.password} | base64 -d
-IyF-lEvTB2wJc5v--------------------
cd ../argocd
--------------------
kubectl apply -f probe-test-app.yaml -n argocd
application.argoproj.io/probe-test-app created
--------------------
kubectl apply -f argo-rollouts-app.yaml -n argocd
application.argoproj.io/argo-rollouts created
--------------------
kubectl delete deployment probe-test-app
deployment.apps "probe-test-app" deleted
--------------------
kubectl get pods
NAME                              READY   STATUS      RESTARTS   AGE
probe-test-app-685956d4cf-22csj   2/2     Running     0          10s
probe-test-app-685956d4cf-8z5zj   2/2     Running     0          10s
probe-test-app-685956d4cf-j892h   2/2     Running     0          10s
rabbitmq-0                        1/1     Running     0          18m
rabbitmq-publish-kbq2x            0/1     Completed   0          9m58s
--------------------
kubectl delete application probe-test-app -n argocd
application.argoproj.io "probe-test-app" deleted
--------------------
cd ../argo-rollouts
--------------------
kubectl apply -f bluegreen-service.yaml
service/bluegreen-demo created
--------------------
kubectl apply -f bluegreen-gateway.yaml
gateway.gateway.networking.k8s.io/bluegreen-demo created
httproute.gateway.networking.k8s.io/bluegreen-demo created
--------------------
kubectl apply -f bluegreen-preview-service.yaml
service/bluegreen-demo-preview created
--------------------
kubectl apply -f bluegreen-preview-gateway.yaml
gateway.gateway.networking.k8s.io/bluegreen-demo-preview created
httproute.gateway.networking.k8s.io/bluegreen-demo-preview created
--------------------
kubectl apply -f bluegreen-rollout-manual.yaml
rollout.argoproj.io/bluegreen-demo created
--------------------
kubectl apply -f bluegreen-rollout-manual-green.yaml
rollout.argoproj.io/bluegreen-demo configured
--------------------
kubectl argo rollouts get rollout bluegreen-demo
Name:            bluegreen-demo
Namespace:       default
Status:          [32m✔[0m Healthy
Strategy:        BlueGreen
Images:          mirror.gcr.io/argoproj/rollouts-demo:blue
                 mirror.gcr.io/argoproj/rollouts-demo:green ([32mstable[0m, [32mactive[0m)
Replicas:
  Desired:       1
  Current:       2
  Updated:       1
  Ready:         1
  Available:     1

NAME                                        KIND        STATUS     AGE  INFO
⟳ bluegreen-demo                            Rollout     [32m✔[0m Healthy  12s  
├──# revision:2                                                         
│  └──⧉ [32m[32mbluegreen-demo-754b69fb9c[0m[0m           ReplicaSet  [32m✔[0m Healthy  11s  [32mstable[0m,[32mactive[0m
│     └──□ bluegreen-demo-754b69fb9c-5tthn  Pod         [32m✔[0m Running  11s  ready:2/2
└──# revision:1                                                         
   └──⧉ bluegreen-demo-84bf8f6dd6           ReplicaSet  [32m✔[0m Healthy  12s  delay:4m48s
      └──□ bluegreen-demo-84bf8f6dd6-lzjln  Pod         [32m✔[0m Running  12s  ready:2/2
--------------------
kubectl argo rollouts promote bluegreen-demo
rollout 'bluegreen-demo' promoted
--------------------
kubectl argo rollouts get rollout bluegreen-demo
Name:            bluegreen-demo
Namespace:       default
Status:          [32m✔[0m Healthy
Strategy:        BlueGreen
Images:          mirror.gcr.io/argoproj/rollouts-demo:blue
                 mirror.gcr.io/argoproj/rollouts-demo:green ([32mstable[0m, [32mactive[0m)
Replicas:
  Desired:       1
  Current:       2
  Updated:       1
  Ready:         1
  Available:     1

NAME                                        KIND        STATUS     AGE  INFO
⟳ bluegreen-demo                            Rollout     [32m✔[0m Healthy  31s  
├──# revision:2                                                         
│  └──⧉ [32m[32mbluegreen-demo-754b69fb9c[0m[0m           ReplicaSet  [32m✔[0m Healthy  30s  [32mstable[0m,[32mactive[0m
│     └──□ bluegreen-demo-754b69fb9c-5tthn  Pod         [32m✔[0m Running  30s  ready:2/2
└──# revision:1                                                         
   └──⧉ bluegreen-demo-84bf8f6dd6           ReplicaSet  [32m✔[0m Healthy  31s  delay:4m29s
      └──□ bluegreen-demo-84bf8f6dd6-lzjln  Pod         [32m✔[0m Running  31s  ready:2/2
--------------------
kubectl argo rollouts undo bluegreen-demo
time="2025-04-15T23:01:29+10:00" level=info msg="unknown field \"spec.template.metadata.creationTimestamp\""
rollout 'bluegreen-demo' undo
--------------------
