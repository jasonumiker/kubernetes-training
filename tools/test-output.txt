kubectl config get-contexts
CURRENT   NAME             CLUSTER          AUTHINFO         NAMESPACE
*         docker-desktop   docker-desktop   docker-desktop   
--------------------
kubectl get nodes
NAME             STATUS   ROLES           AGE   VERSION
docker-desktop   Ready    control-plane   57s   v1.32.2
--------------------
cd probe-test-app
--------------------
kubectl apply -f probe-test-app-pod.yaml
pod/probe-test-app created
pod/probe-test-app condition met
--------------------
kubectl get pods -o wide
NAME             READY   STATUS    RESTARTS   AGE   IP         NODE             NOMINATED NODE   READINESS GATES
probe-test-app   1/1     Running   0          33s   10.1.0.6   docker-desktop   <none>           <none>
--------------------
kubectl apply -f probe-test-app-service.yaml
service/probe-test-app created
--------------------
kubectl get services -o wide
NAME             TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE   SELECTOR
kubernetes       ClusterIP      10.96.0.1        <none>        443/TCP          93s   <none>
probe-test-app   LoadBalancer   10.105.115.102   localhost     8000:30862/TCP   2s    app.kubernetes.io/name=probe-test-app
--------------------
kubectl get endpoints
NAME             ENDPOINTS           AGE
kubernetes       192.168.65.3:6443   94s
probe-test-app   10.1.0.6:8080       3s
--------------------
kubectl apply -f probe-test-app-pod-2.yaml
pod/probe-test-app-2 created
pod/probe-test-app-2 condition met
--------------------
kubectl get endpoints
NAME             ENDPOINTS                     AGE
kubernetes       192.168.65.3:6443             98s
probe-test-app   10.1.0.6:8080,10.1.0.7:8080   7s
--------------------
kubectl delete pods --all
pod "probe-test-app" deleted
pod "probe-test-app-2" deleted
--------------------
kubectl apply -f probe-test-app-replicaset.yaml
replicaset.apps/probe-test-app created
pod/probe-test-app-76gnb condition met
pod/probe-test-app-g4rcl condition met
pod/probe-test-app-ts5fr condition met
--------------------
kubectl scale replicaset probe-test-app --replicas=2
replicaset.apps/probe-test-app scaled
--------------------
kubectl get pods
NAME                   READY   STATUS        RESTARTS   AGE
probe-test-app-76gnb   1/1     Running       0          4s
probe-test-app-g4rcl   1/1     Terminating   0          4s
probe-test-app-ts5fr   1/1     Running       0          4s
--------------------
kubectl delete replicaset probe-test-app
replicaset.apps "probe-test-app" deleted
--------------------
kubectl apply -f probe-test-app-deployment.yaml
deployment.apps/probe-test-app created
Waiting for deployment "probe-test-app" rollout to finish: 0 of 3 updated replicas are available...
Waiting for deployment "probe-test-app" rollout to finish: 1 of 3 updated replicas are available...
Waiting for deployment "probe-test-app" rollout to finish: 2 of 3 updated replicas are available...
deployment "probe-test-app" successfully rolled out
--------------------
kubectl get pods
NAME                              READY   STATUS    RESTARTS   AGE
probe-test-app-685956d4cf-5wsjr   1/1     Running   0          3s
probe-test-app-685956d4cf-75g8d   1/1     Running   0          3s
probe-test-app-685956d4cf-rd6qf   1/1     Running   0          3s
--------------------
kubectl get replicasets
NAME                        DESIRED   CURRENT   READY   AGE
probe-test-app-685956d4cf   3         3         3       4s
--------------------
kubectl set image deployment/probe-test-app probe-test-app=jasonumiker/probe-test-app:v2
deployment.apps/probe-test-app image updated
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "probe-test-app" rollout to finish: 1 old replicas are pending termination...
deployment "probe-test-app" successfully rolled out
--------------------
kubectl events
LAST SEEN               TYPE      REASON                              OBJECT                                 MESSAGE
2m21s                   Warning   PossibleMemoryBackedVolumesOnDisk   Node/docker-desktop                    The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
2m21s                   Normal    Starting                            Node/docker-desktop                    Starting kubelet.
2m21s                   Warning   CgroupV1                            Node/docker-desktop                    cgroup v1 support is in maintenance mode, please migrate to cgroup v2
2m21s                   Warning   InvalidDiskCapacity                 Node/docker-desktop                    invalid capacity 0 on image filesystem
2m21s (x8 over 2m21s)   Normal    NodeHasSufficientMemory             Node/docker-desktop                    Node docker-desktop status is now: NodeHasSufficientMemory
2m21s (x8 over 2m21s)   Normal    NodeHasNoDiskPressure               Node/docker-desktop                    Node docker-desktop status is now: NodeHasNoDiskPressure
2m21s (x7 over 2m21s)   Normal    NodeHasSufficientPID                Node/docker-desktop                    Node docker-desktop status is now: NodeHasSufficientPID
2m21s                   Normal    NodeAllocatableEnforced             Node/docker-desktop                    Updated Node Allocatable limit across pods
2m12s                   Normal    RegisteredNode                      Node/docker-desktop                    Node docker-desktop event: Registered Node docker-desktop in Controller
2m9s                    Normal    Starting                            Node/docker-desktop                    
79s                     Normal    Scheduled                           Pod/probe-test-app                     Successfully assigned default/probe-test-app to docker-desktop
78s                     Normal    Pulling                             Pod/probe-test-app                     Pulling image "mirror.gcr.io/jasonumiker/probe-test-app:v1"
49s                     Normal    Pulled                              Pod/probe-test-app                     Successfully pulled image "mirror.gcr.io/jasonumiker/probe-test-app:v1" in 29.405s (29.405s including waiting). Image size: 1024950162 bytes.
48s                     Normal    Created                             Pod/probe-test-app                     Created container: probe-test-app
47s                     Normal    Started                             Pod/probe-test-app                     Started container probe-test-app
41s                     Normal    Pulled                              Pod/probe-test-app-2                   Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
41s                     Normal    Created                             Pod/probe-test-app-2                   Created container: probe-test-app
41s                     Normal    Scheduled                           Pod/probe-test-app-2                   Successfully assigned default/probe-test-app-2 to docker-desktop
40s                     Normal    Started                             Pod/probe-test-app-2                   Started container probe-test-app
37s                     Normal    Killing                             Pod/probe-test-app-2                   Stopping container probe-test-app
37s                     Normal    Killing                             Pod/probe-test-app                     Stopping container probe-test-app
36s                     Warning   Unhealthy                           Pod/probe-test-app                     Readiness probe failed: Get "http://10.1.0.6:8080/readyz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
34s                     Normal    Created                             Pod/probe-test-app-g4rcl               Created container: probe-test-app
34s                     Normal    Scheduled                           Pod/probe-test-app-g4rcl               Successfully assigned default/probe-test-app-g4rcl to docker-desktop
34s                     Normal    Pulled                              Pod/probe-test-app-76gnb               Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
34s                     Normal    Created                             Pod/probe-test-app-76gnb               Created container: probe-test-app
34s                     Normal    Started                             Pod/probe-test-app-ts5fr               Started container probe-test-app
34s                     Normal    SuccessfulCreate                    ReplicaSet/probe-test-app              Created pod: probe-test-app-g4rcl
34s                     Normal    Pulled                              Pod/probe-test-app-g4rcl               Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
34s                     Normal    SuccessfulCreate                    ReplicaSet/probe-test-app              Created pod: probe-test-app-ts5fr
34s                     Normal    Scheduled                           Pod/probe-test-app-ts5fr               Successfully assigned default/probe-test-app-ts5fr to docker-desktop
34s                     Normal    Pulled                              Pod/probe-test-app-ts5fr               Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
34s                     Normal    Created                             Pod/probe-test-app-ts5fr               Created container: probe-test-app
34s                     Normal    SuccessfulCreate                    ReplicaSet/probe-test-app              Created pod: probe-test-app-76gnb
34s                     Normal    Scheduled                           Pod/probe-test-app-76gnb               Successfully assigned default/probe-test-app-76gnb to docker-desktop
33s                     Warning   Unhealthy                           Pod/probe-test-app-g4rcl               Readiness probe failed: Get "http://10.1.0.9:8080/readyz": dial tcp 10.1.0.9:8080: connect: connection refused
33s                     Warning   Unhealthy                           Pod/probe-test-app-ts5fr               Readiness probe failed: Get "http://10.1.0.8:8080/readyz": dial tcp 10.1.0.8:8080: connect: connection refused
33s                     Normal    Started                             Pod/probe-test-app-76gnb               Started container probe-test-app
33s                     Normal    Started                             Pod/probe-test-app-g4rcl               Started container probe-test-app
31s                     Normal    SuccessfulDelete                    ReplicaSet/probe-test-app              Deleted pod: probe-test-app-g4rcl
29s                     Normal    Killing                             Pod/probe-test-app-g4rcl               Stopping container probe-test-app
29s                     Normal    Killing                             Pod/probe-test-app-ts5fr               Stopping container probe-test-app
29s                     Normal    Killing                             Pod/probe-test-app-76gnb               Stopping container probe-test-app
28s                     Normal    SuccessfulCreate                    ReplicaSet/probe-test-app-685956d4cf   Created pod: probe-test-app-685956d4cf-rd6qf
28s                     Normal    SuccessfulCreate                    ReplicaSet/probe-test-app-685956d4cf   Created pod: probe-test-app-685956d4cf-75g8d
28s                     Normal    ScalingReplicaSet                   Deployment/probe-test-app              Scaled up replica set probe-test-app-685956d4cf from 0 to 3
28s                     Normal    Scheduled                           Pod/probe-test-app-685956d4cf-5wsjr    Successfully assigned default/probe-test-app-685956d4cf-5wsjr to docker-desktop
28s                     Normal    SuccessfulCreate                    ReplicaSet/probe-test-app-685956d4cf   Created pod: probe-test-app-685956d4cf-5wsjr
28s                     Normal    Scheduled                           Pod/probe-test-app-685956d4cf-rd6qf    Successfully assigned default/probe-test-app-685956d4cf-rd6qf to docker-desktop
28s                     Normal    Scheduled                           Pod/probe-test-app-685956d4cf-75g8d    Successfully assigned default/probe-test-app-685956d4cf-75g8d to docker-desktop
27s                     Normal    Created                             Pod/probe-test-app-685956d4cf-75g8d    Created container: probe-test-app
27s                     Normal    Started                             Pod/probe-test-app-685956d4cf-5wsjr    Started container probe-test-app
27s                     Normal    Pulled                              Pod/probe-test-app-685956d4cf-rd6qf    Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
27s                     Normal    Pulled                              Pod/probe-test-app-685956d4cf-5wsjr    Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
27s                     Normal    Created                             Pod/probe-test-app-685956d4cf-rd6qf    Created container: probe-test-app
27s                     Normal    Started                             Pod/probe-test-app-685956d4cf-rd6qf    Started container probe-test-app
27s                     Normal    Pulled                              Pod/probe-test-app-685956d4cf-75g8d    Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
27s                     Normal    Created                             Pod/probe-test-app-685956d4cf-5wsjr    Created container: probe-test-app
27s                     Normal    Started                             Pod/probe-test-app-685956d4cf-75g8d    Started container probe-test-app
23s                     Normal    ScalingReplicaSet                   Deployment/probe-test-app              Scaled up replica set probe-test-app-68d99fdc94 from 0 to 1
23s                     Normal    SuccessfulCreate                    ReplicaSet/probe-test-app-68d99fdc94   Created pod: probe-test-app-68d99fdc94-lvxsx
22s                     Normal    Pulling                             Pod/probe-test-app-68d99fdc94-lvxsx    Pulling image "jasonumiker/probe-test-app:v2"
22s                     Normal    Scheduled                           Pod/probe-test-app-68d99fdc94-lvxsx    Successfully assigned default/probe-test-app-68d99fdc94-lvxsx to docker-desktop
17s                     Normal    Created                             Pod/probe-test-app-68d99fdc94-lvxsx    Created container: probe-test-app
17s                     Normal    Pulled                              Pod/probe-test-app-68d99fdc94-lvxsx    Successfully pulled image "jasonumiker/probe-test-app:v2" in 5.059s (5.059s including waiting). Image size: 1024950162 bytes.
17s                     Normal    Started                             Pod/probe-test-app-68d99fdc94-lvxsx    Started container probe-test-app
16s                     Normal    ScalingReplicaSet                   Deployment/probe-test-app              Scaled down replica set probe-test-app-685956d4cf from 3 to 2
16s                     Normal    Scheduled                           Pod/probe-test-app-68d99fdc94-rbs6s    Successfully assigned default/probe-test-app-68d99fdc94-rbs6s to docker-desktop
16s                     Normal    SuccessfulCreate                    ReplicaSet/probe-test-app-68d99fdc94   Created pod: probe-test-app-68d99fdc94-rbs6s
16s                     Normal    SuccessfulDelete                    ReplicaSet/probe-test-app-685956d4cf   Deleted pod: probe-test-app-685956d4cf-rd6qf
16s                     Normal    Killing                             Pod/probe-test-app-685956d4cf-rd6qf    Stopping container probe-test-app
16s                     Normal    ScalingReplicaSet                   Deployment/probe-test-app              Scaled up replica set probe-test-app-68d99fdc94 from 1 to 2
15s                     Normal    Pulled                              Pod/probe-test-app-68d99fdc94-rbs6s    Container image "jasonumiker/probe-test-app:v2" already present on machine
15s                     Normal    Created                             Pod/probe-test-app-68d99fdc94-rbs6s    Created container: probe-test-app
15s                     Normal    Started                             Pod/probe-test-app-68d99fdc94-rbs6s    Started container probe-test-app
5s                      Normal    SuccessfulDelete                    ReplicaSet/probe-test-app-685956d4cf   Deleted pod: probe-test-app-685956d4cf-75g8d
5s                      Normal    Killing                             Pod/probe-test-app-685956d4cf-75g8d    Stopping container probe-test-app
5s                      Normal    Scheduled                           Pod/probe-test-app-68d99fdc94-6xd5h    Successfully assigned default/probe-test-app-68d99fdc94-6xd5h to docker-desktop
5s                      Normal    SuccessfulCreate                    ReplicaSet/probe-test-app-68d99fdc94   Created pod: probe-test-app-68d99fdc94-6xd5h
5s                      Normal    ScalingReplicaSet                   Deployment/probe-test-app              Scaled down replica set probe-test-app-685956d4cf from 2 to 1
5s                      Normal    ScalingReplicaSet                   Deployment/probe-test-app              Scaled up replica set probe-test-app-68d99fdc94 from 2 to 3
4s                      Normal    Pulled                              Pod/probe-test-app-68d99fdc94-6xd5h    Container image "jasonumiker/probe-test-app:v2" already present on machine
4s                      Normal    Created                             Pod/probe-test-app-68d99fdc94-6xd5h    Created container: probe-test-app
4s                      Normal    Started                             Pod/probe-test-app-68d99fdc94-6xd5h    Started container probe-test-app
4s                      Warning   Unhealthy                           Pod/probe-test-app-68d99fdc94-6xd5h    Readiness probe failed: Get "http://10.1.0.16:8080/readyz": dial tcp 10.1.0.16:8080: connect: connection refused
3s                      Normal    SuccessfulDelete                    ReplicaSet/probe-test-app-685956d4cf   Deleted pod: probe-test-app-685956d4cf-5wsjr
3s                      Normal    Killing                             Pod/probe-test-app-685956d4cf-5wsjr    Stopping container probe-test-app
3s                      Normal    ScalingReplicaSet                   Deployment/probe-test-app              Scaled down replica set probe-test-app-685956d4cf from 1 to 0
--------------------
kubectl rollout undo deployment/probe-test-app
deployment.apps/probe-test-app rolled back
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "probe-test-app" rollout to finish: 1 old replicas are pending termination...
deployment "probe-test-app" successfully rolled out
--------------------
kubectl get pods
NAME                              READY   STATUS    RESTARTS   AGE
probe-test-app-685956d4cf-cmprb   1/1     Running   0          7s
probe-test-app-685956d4cf-pvtn6   1/1     Running   0          5s
probe-test-app-685956d4cf-vvv54   1/1     Running   0          3s
--------------------
kubectl describe replicaset probe-test-app
Name:           probe-test-app-685956d4cf
Namespace:      default
Selector:       app.kubernetes.io/name=probe-test-app,pod-template-hash=685956d4cf
Labels:         app.kubernetes.io/name=probe-test-app
                pod-template-hash=685956d4cf
Annotations:    deployment.kubernetes.io/desired-replicas: 3
                deployment.kubernetes.io/max-replicas: 4
                deployment.kubernetes.io/revision: 3
                deployment.kubernetes.io/revision-history: 1
Controlled By:  Deployment/probe-test-app
Replicas:       3 current / 3 desired
Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app.kubernetes.io/name=probe-test-app
           pod-template-hash=685956d4cf
  Containers:
   probe-test-app:
    Image:      mirror.gcr.io/jasonumiker/probe-test-app:v1
    Port:       8080/TCP
    Host Port:  0/TCP
    Requests:
      cpu:         50m
      memory:      52Mi
    Liveness:      http-get http://:8080/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:     http-get http://:8080/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  39s   replicaset-controller  Created pod: probe-test-app-685956d4cf-rd6qf
  Normal  SuccessfulCreate  39s   replicaset-controller  Created pod: probe-test-app-685956d4cf-75g8d
  Normal  SuccessfulCreate  39s   replicaset-controller  Created pod: probe-test-app-685956d4cf-5wsjr
  Normal  SuccessfulDelete  27s   replicaset-controller  Deleted pod: probe-test-app-685956d4cf-rd6qf
  Normal  SuccessfulDelete  16s   replicaset-controller  Deleted pod: probe-test-app-685956d4cf-75g8d
  Normal  SuccessfulDelete  14s   replicaset-controller  Deleted pod: probe-test-app-685956d4cf-5wsjr
  Normal  SuccessfulCreate  10s   replicaset-controller  Created pod: probe-test-app-685956d4cf-cmprb
  Normal  SuccessfulCreate  8s    replicaset-controller  Created pod: probe-test-app-685956d4cf-pvtn6
  Normal  SuccessfulCreate  6s    replicaset-controller  Created pod: probe-test-app-685956d4cf-vvv54

Name:           probe-test-app-68d99fdc94
Namespace:      default
Selector:       app.kubernetes.io/name=probe-test-app,pod-template-hash=68d99fdc94
Labels:         app.kubernetes.io/name=probe-test-app
                pod-template-hash=68d99fdc94
Annotations:    deployment.kubernetes.io/desired-replicas: 3
                deployment.kubernetes.io/max-replicas: 4
                deployment.kubernetes.io/revision: 2
Controlled By:  Deployment/probe-test-app
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app.kubernetes.io/name=probe-test-app
           pod-template-hash=68d99fdc94
  Containers:
   probe-test-app:
    Image:      jasonumiker/probe-test-app:v2
    Port:       8080/TCP
    Host Port:  0/TCP
    Requests:
      cpu:         50m
      memory:      52Mi
    Liveness:      http-get http://:8080/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:     http-get http://:8080/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  34s   replicaset-controller  Created pod: probe-test-app-68d99fdc94-lvxsx
  Normal  SuccessfulCreate  27s   replicaset-controller  Created pod: probe-test-app-68d99fdc94-rbs6s
  Normal  SuccessfulCreate  16s   replicaset-controller  Created pod: probe-test-app-68d99fdc94-6xd5h
  Normal  SuccessfulDelete  8s    replicaset-controller  Deleted pod: probe-test-app-68d99fdc94-6xd5h
  Normal  SuccessfulDelete  6s    replicaset-controller  Deleted pod: probe-test-app-68d99fdc94-rbs6s
  Normal  SuccessfulDelete  4s    replicaset-controller  Deleted pod: probe-test-app-68d99fdc94-lvxsx

--------------------
cd ../sidecar-and-init-containers
--------------------
kubectl apply -f sidecar.yaml
pod/pod-with-sidecar created
pod/pod-with-sidecar condition met
--------------------
kubectl apply -f init.yaml
pod/myapp-pod created
--------------------
kubectl get pod myapp-pod
NAME        READY   STATUS     RESTARTS   AGE
myapp-pod   0/1     Init:0/2   0          1s
--------------------
kubectl apply -f services-init-requires.yaml
service/myservice created
service/mydb created
--------------------
NAME        READY   STATUS    RESTARTS   AGE
myapp-pod   1/1     Running   0          13s
--------------------
pod "myapp-pod" deleted
pod "pod-with-sidecar" deleted
service "myservice" deleted
service "mydb" deleted
--------------------
cd ../pvs-and-statefulsets
--------------------
kubectl apply -f hostpath-provisioner.yaml
deployment.apps/hostpath-provisioner created
storageclass.storage.k8s.io/hostpath-provisioner created
serviceaccount/hostpath-provisioner created
clusterrole.rbac.authorization.k8s.io/hostpath-provisioner created
clusterrolebinding.rbac.authorization.k8s.io/hostpath-provisioner created
--------------------
kubectl get storageclass
NAME                   PROVISIONER            RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
hostpath (default)     docker.io/hostpath     Delete          Immediate              false                  4m2s
hostpath-provisioner   microk8s.io/hostpath   Delete          WaitForFirstConsumer   false                  1s
--------------------
kubectl apply -f pvc.yaml
persistentvolumeclaim/test-pvc created
--------------------
kubectl get pvc
NAME       STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS           VOLUMEATTRIBUTESCLASS   AGE
test-pvc   Pending                                      hostpath-provisioner   <unset>                 1s
--------------------
kubectl apply -f pod.yaml
pod/nginx created
pod/nginx condition met
--------------------
kubectl get pvc
NAME       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS           VOLUMEATTRIBUTESCLASS   AGE
test-pvc   Bound    pvc-5220e38e-f419-435e-8de3-93c5bd1229e8   1Gi        RWO            hostpath-provisioner   <unset>                 18s
--------------------
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS           VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-5220e38e-f419-435e-8de3-93c5bd1229e8   1Gi        RWO            Delete           Bound    default/test-pvc   hostpath-provisioner   <unset>                          4s
--------------------
kubectl apply -f service.yaml
service/nginx created
--------------------
curl http://localhost:8001
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   153  100   153    0     0   102k      0 --:--:-- --:--:-- --:--:--  149k
<html>
<head><title>403 Forbidden</title></head>
<body>
<center><h1>403 Forbidden</h1></center>
<hr><center>nginx/1.27.4</center>
</body>
</html>
--------------------
kubectl exec -it nginx  -- bash -c "echo 'Data on PV' > /usr/share/nginx/html/index.html"
--------------------
curl http://localhost:8001
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100    11  100    11    0     0   9490      0 --:--:-- --:--:-- --:--:-- 11000
Data on PV
--------------------
kubectl delete pod nginx
pod "nginx" deleted
--------------------
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS           VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-5220e38e-f419-435e-8de3-93c5bd1229e8   1Gi        RWO            Delete           Bound    default/test-pvc   hostpath-provisioner   <unset>                          22s
--------------------
kubectl apply -f pod.yaml
pod/nginx created
pod/nginx condition met
--------------------
curl http://localhost:8001
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100    11  100    11    0     0   7762      0 --:--:-- --:--:-- --:--:-- 11000
Data on PV
--------------------
kubectl delete service nginx
service "nginx" deleted
--------------------
kubectl delete pod nginx
pod "nginx" deleted
--------------------
kubectl delete pvc test-pvc
persistentvolumeclaim "test-pvc" deleted
--------------------
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM              STORAGECLASS           VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-5220e38e-f419-435e-8de3-93c5bd1229e8   1Gi        RWO            Delete           Released   default/test-pvc   hostpath-provisioner   <unset>                          30s
--------------------
--------------------
kubectl apply -k .
serviceaccount/rabbitmq created
role.rbac.authorization.k8s.io/rabbitmq created
rolebinding.rbac.authorization.k8s.io/rabbitmq created
configmap/rabbitmq-config created
secret/erlang-cookie created
secret/rabbitmq-admin created
service/rabbitmq-client created
service/rabbitmq-headless created
statefulset.apps/rabbitmq created
Waiting for 1 pods to be ready...
partitioned roll out complete: 1 new pods have been updated...
--------------------
kubectl describe statefulset rabbitmq
Name:               rabbitmq
Namespace:          default
CreationTimestamp:  Mon, 14 Apr 2025 21:10:38 +1000
Selector:           app=rabbitmq
Labels:             <none>
Annotations:        <none>
Replicas:           1 desired | 1 total
Update Strategy:    RollingUpdate
  Partition:        0
Pods Status:        1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=rabbitmq
  Service Account:  rabbitmq
  Init Containers:
   rabbitmq-config:
    Image:      mirror.gcr.io/busybox:1.37.0
    Port:       <none>
    Host Port:  <none>
    Command:
      sh
      -c
      cp /tmp/rabbitmq/rabbitmq.conf /etc/rabbitmq/rabbitmq.conf && echo '' >> /etc/rabbitmq/rabbitmq.conf; cp /tmp/rabbitmq/enabled_plugins /etc/rabbitmq/enabled_plugins
    Environment:  <none>
    Mounts:
      /etc/rabbitmq from rabbitmq-config-rw (rw)
      /tmp/rabbitmq from rabbitmq-config (rw)
  Containers:
   rabbitmq:
    Image:       mirror.gcr.io/rabbitmq:3.8.34
    Ports:       5672/TCP, 15672/TCP, 15692/TCP, 4369/TCP
    Host Ports:  0/TCP, 0/TCP, 0/TCP, 0/TCP
    Liveness:    exec [rabbitmq-diagnostics status] delay=60s timeout=15s period=60s #success=1 #failure=3
    Readiness:   exec [rabbitmq-diagnostics ping] delay=20s timeout=10s period=60s #success=1 #failure=3
    Environment:
      RABBITMQ_DEFAULT_PASS:   <set to the key 'pass' in secret 'rabbitmq-admin'>   Optional: false
      RABBITMQ_DEFAULT_USER:   <set to the key 'user' in secret 'rabbitmq-admin'>   Optional: false
      RABBITMQ_ERLANG_COOKIE:  <set to the key 'cookie' in secret 'erlang-cookie'>  Optional: false
    Mounts:
      /etc/rabbitmq from rabbitmq-config-rw (rw)
      /var/lib/rabbitmq/mnesia from rabbitmq-data (rw)
  Volumes:
   rabbitmq-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      rabbitmq-config
    Optional:  false
   rabbitmq-config-rw:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
   rabbitmq-data:
    Type:          PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:     rabbitmq-data
    ReadOnly:      false
  Node-Selectors:  <none>
  Tolerations:     <none>
Volume Claims:
  Name:          rabbitmq-data
  StorageClass:  hostpath-provisioner
  Labels:        <none>
  Annotations:   <none>
  Capacity:      3Gi
  Access Modes:  [ReadWriteOnce]
Events:
  Type    Reason            Age   From                    Message
  ----    ------            ----  ----                    -------
  Normal  SuccessfulCreate  82s   statefulset-controller  create Claim rabbitmq-data-rabbitmq-0 Pod rabbitmq-0 in StatefulSet rabbitmq success
  Normal  SuccessfulCreate  82s   statefulset-controller  create Pod rabbitmq-0 in StatefulSet rabbitmq successful
--------------------
kubectl get pods
NAME                              READY   STATUS    RESTARTS   AGE
probe-test-app-685956d4cf-cmprb   1/1     Running   0          4m4s
probe-test-app-685956d4cf-pvtn6   1/1     Running   0          4m2s
probe-test-app-685956d4cf-vvv54   1/1     Running   0          4m
rabbitmq-0                        1/1     Running   0          83s
--------------------
kubectl get pvc
NAME                       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS           VOLUMEATTRIBUTESCLASS   AGE
rabbitmq-data-rabbitmq-0   Bound    pvc-8c88ca44-f7a2-4392-92d8-14770662a886   3Gi        RWO            hostpath-provisioner   <unset>                 84s
--------------------
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                              STORAGECLASS           VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-8c88ca44-f7a2-4392-92d8-14770662a886   3Gi        RWO            Delete           Bound    default/rabbitmq-data-rabbitmq-0   hostpath-provisioner   <unset>                          80s
--------------------
kubectl delete pod rabbitmq-0
pod "rabbitmq-0" deleted
--------------------
kubectl get pods
NAME                              READY   STATUS            RESTARTS   AGE
probe-test-app-685956d4cf-cmprb   1/1     Running           0          4m11s
probe-test-app-685956d4cf-pvtn6   1/1     Running           0          4m9s
probe-test-app-685956d4cf-vvv54   1/1     Running           0          4m7s
rabbitmq-0                        0/1     PodInitializing   0          1s
--------------------
cd ../../monitoring
--------------------
./install-prometheus.sh

Updating docker-desktop pods to expose metrics endpoints
This will involve several kube-system pod restarts

Fetching debian image to run nsenter on the docker-desktop host...
12.10: Pulling from debian
23b7d26ef1d2: Already exists
Digest: sha256:00cd074b40c4d99ff0c24540bdde0533ca3791edcdac0de36d6b9fb3260d89e2
Status: Downloaded newer image for mirror.gcr.io/debian:12.10
mirror.gcr.io/debian:12.10
Host Node IP: 192.168.65.3
Updating kube-proxy configmap...
configmap "kube-proxy" deleted
configmap/kube-proxy created
Restarting the kube-proxy pod
pod "kube-proxy-g8q9w" deleted
pod/kube-proxy-p7hk4 condition met
kube-proxy pod restarted.
Updating bind-address on kube-controller-manager...
Waiting for kube-controller-manager to restart, this can take some time...
pod/kube-controller-manager-docker-desktop condition met
pod/kube-controller-manager-docker-desktop condition met
kube-controller-manager pod restarted.
Updating bind-address on kube-scheduler
Waiting for kube-scheduler to restart, this can take some time...
pod/kube-scheduler-docker-desktop condition met
pod/kube-scheduler-docker-desktop condition met
kube-scheduler pod restarted.
Adding node ip to listen-metrics-urls on etcd
Waiting for etcd to restart, this can take some time...
Error from server (Timeout): the server was unable to return a response in the time allotted, but may still be processing the request (get pods)
pod/etcd-docker-desktop condition met

Done! You can now deploy the monitoring components.

"prometheus-community" already exists with the same configuration, skipping
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "argo-helm" chart repository
...Successfully got an update from the "kedacore" chart repository
...Successfully got an update from the "kiali" chart repository
...Successfully got an update from the "gatekeeper" chart repository
...Successfully got an update from the "istio" chart repository
...Successfully got an update from the "prometheus-community" chart repository
...Successfully got an update from the "ingress-nginx" chart repository
Update Complete. ⎈Happy Helming!⎈
namespace/monitoring created
NAME: prometheus
LAST DEPLOYED: Mon Apr 14 21:14:44 2025
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
NOTES:
kube-prometheus-stack has been installed. Check its status by running:
  kubectl --namespace monitoring get pods -l "release=prometheus"

Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create & configure Alertmanager and Prometheus instances using the Operator.
NAME: adapter
LAST DEPLOYED: Mon Apr 14 21:15:12 2025
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
adapter-prometheus-adapter has been deployed.
In a few minutes you should be able to list metrics using the following command(s):

  kubectl get --raw /apis/metrics.k8s.io/v1beta1
  kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1
Waiting for deployment "adapter-prometheus-adapter" rollout to finish: 0 of 1 updated replicas are available...
deployment "adapter-prometheus-adapter" successfully rolled out
--------------------
kubectl top nodes
error: metrics not available yet
--------------------
kubectl top pods
NAME                              CPU(cores)   MEMORY(bytes)   
probe-test-app-685956d4cf-cmprb   0m           40Mi            
probe-test-app-685956d4cf-pvtn6   0m           40Mi            
probe-test-app-685956d4cf-vvv54   0m           40Mi            
rabbitmq-0                        12m          202Mi           
--------------------
kubectl top pods -n monitoring
NAME                                                     CPU(cores)   MEMORY(bytes)   
adapter-prometheus-adapter-66b7469965-4zzkf              1m           28Mi            
alertmanager-prometheus-kube-prometheus-alertmanager-0   0m           31Mi            
prometheus-grafana-8564d589ff-znv4m                      1m           259Mi           
prometheus-kube-prometheus-operator-cf55d947c-4wvx5      0m           29Mi            
prometheus-kube-state-metrics-59b844b9c-f8kx4            0m           18Mi            
prometheus-prometheus-kube-prometheus-prometheus-0       7m           115Mi           
prometheus-prometheus-node-exporter-7rq7m                0m           8Mi             
--------------------
cd ../probe-test-app
--------------------
kubectl apply -f probe-test-app-hpa.yaml
horizontalpodautoscaler.autoscaling/probe-test-app created
--------------------
kubectl apply -f generate-load-app-replicaset.yaml
replicaset.apps/generate-load-app created
./tools/test-all-steps.sh: line 695: unexpected EOF while looking for matching `"'
