kubectl config get-contexts
CURRENT   NAME             CLUSTER          AUTHINFO         NAMESPACE
*         docker-desktop   docker-desktop   docker-desktop   
--------------------
kubectl get nodes
NAME             STATUS   ROLES           AGE   VERSION
docker-desktop   Ready    control-plane   47s   v1.30.2
--------------------
cd probe-test-app
--------------------
kubectl apply -f probe-test-app-pod.yaml
pod/probe-test-app created
pod/probe-test-app condition met
--------------------
kubectl get pods -o wide
NAME             READY   STATUS    RESTARTS   AGE   IP         NODE             NOMINATED NODE   READINESS GATES
probe-test-app   1/1     Running   0          17s   10.1.0.6   docker-desktop   <none>           <none>
--------------------
kubectl apply -f probe-test-app-service.yaml
service/probe-test-app created
--------------------
kubectl get services -o wide
NAME             TYPE           CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE   SELECTOR
kubernetes       ClusterIP      10.96.0.1    <none>        443/TCP          67s   <none>
probe-test-app   LoadBalancer   10.97.81.7   localhost     8000:30721/TCP   1s    app.kubernetes.io/name=probe-test-app
--------------------
kubectl get endpoints
NAME             ENDPOINTS           AGE
kubernetes       192.168.65.3:6443   69s
probe-test-app   10.1.0.6:8080       3s
--------------------
kubectl apply -f probe-test-app-pod-2.yaml
pod/probe-test-app-2 created
pod/probe-test-app-2 condition met
--------------------
kubectl get endpoints
NAME             ENDPOINTS                     AGE
kubernetes       192.168.65.3:6443             72s
probe-test-app   10.1.0.6:8080,10.1.0.7:8080   6s
--------------------
kubectl delete pods --all
pod "probe-test-app" deleted
pod "probe-test-app-2" deleted
--------------------
kubectl apply -f probe-test-app-replicaset.yaml
replicaset.apps/probe-test-app created
pod/probe-test-app-5vwsg condition met
pod/probe-test-app-jttds condition met
pod/probe-test-app-mfchw condition met
--------------------
kubectl scale replicaset probe-test-app --replicas=2
replicaset.apps/probe-test-app scaled
--------------------
kubectl get pods
NAME                   READY   STATUS        RESTARTS   AGE
probe-test-app-5vwsg   1/1     Running       0          5s
probe-test-app-jttds   0/1     Terminating   0          5s
probe-test-app-mfchw   1/1     Running       0          5s
--------------------
kubectl delete replicaset probe-test-app
replicaset.apps "probe-test-app" deleted
--------------------
kubectl apply -f probe-test-app-deployment.yaml
deployment.apps/probe-test-app created
Waiting for deployment "probe-test-app" rollout to finish: 0 of 3 updated replicas are available...
Waiting for deployment "probe-test-app" rollout to finish: 1 of 3 updated replicas are available...
Waiting for deployment "probe-test-app" rollout to finish: 2 of 3 updated replicas are available...
deployment "probe-test-app" successfully rolled out
--------------------
kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
probe-test-app-fc7776cb8-gsqlg   1/1     Running   0          3s
probe-test-app-fc7776cb8-kchcv   1/1     Running   0          3s
probe-test-app-fc7776cb8-m4td4   1/1     Running   0          3s
--------------------
kubectl get replicasets
NAME                       DESIRED   CURRENT   READY   AGE
probe-test-app-fc7776cb8   3         3         3       4s
--------------------
kubectl set image deployment/probe-test-app probe-test-app=jasonumiker/probe-test-app:v2
deployment.apps/probe-test-app image updated
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "probe-test-app" rollout to finish: 1 old replicas are pending termination...
deployment "probe-test-app" successfully rolled out
--------------------
kubectl events
LAST SEEN             TYPE      REASON                    OBJECT                                MESSAGE
119s                  Normal    Starting                  Node/docker-desktop                   Starting kubelet.
119s                  Warning   InvalidDiskCapacity       Node/docker-desktop                   invalid capacity 0 on image filesystem
119s (x8 over 119s)   Normal    NodeHasSufficientMemory   Node/docker-desktop                   Node docker-desktop status is now: NodeHasSufficientMemory
119s (x7 over 119s)   Normal    NodeHasNoDiskPressure     Node/docker-desktop                   Node docker-desktop status is now: NodeHasNoDiskPressure
119s (x7 over 119s)   Normal    NodeHasSufficientPID      Node/docker-desktop                   Node docker-desktop status is now: NodeHasSufficientPID
119s                  Normal    NodeAllocatableEnforced   Node/docker-desktop                   Updated Node Allocatable limit across pods
110s                  Normal    RegisteredNode            Node/docker-desktop                   Node docker-desktop event: Registered Node docker-desktop in Controller
109s                  Normal    Starting                  Node/docker-desktop                   
68s                   Normal    Scheduled                 Pod/probe-test-app                    Successfully assigned default/probe-test-app to docker-desktop
68s                   Normal    Pulling                   Pod/probe-test-app                    Pulling image "jasonumiker/probe-test-app:v1"
53s                   Warning   Unhealthy                 Pod/probe-test-app                    Readiness probe failed: Get "http://10.1.0.6:8080/readyz": dial tcp 10.1.0.6:8080: connect: connection refused
53s                   Normal    Pulled                    Pod/probe-test-app                    Successfully pulled image "jasonumiker/probe-test-app:v1" in 14.405s (14.405s including waiting). Image size: 1025410707 bytes.
53s                   Normal    Created                   Pod/probe-test-app                    Created container probe-test-app
53s                   Normal    Started                   Pod/probe-test-app                    Started container probe-test-app
46s                   Normal    Created                   Pod/probe-test-app-2                  Created container probe-test-app
46s                   Normal    Scheduled                 Pod/probe-test-app-2                  Successfully assigned default/probe-test-app-2 to docker-desktop
46s                   Normal    Pulled                    Pod/probe-test-app-2                  Container image "jasonumiker/probe-test-app:v1" already present on machine
46s                   Normal    Started                   Pod/probe-test-app-2                  Started container probe-test-app
43s                   Normal    Killing                   Pod/probe-test-app                    Stopping container probe-test-app
42s                   Normal    Killing                   Pod/probe-test-app-2                  Stopping container probe-test-app
40s                   Normal    Scheduled                 Pod/probe-test-app-jttds              Successfully assigned default/probe-test-app-jttds to docker-desktop
40s                   Normal    Scheduled                 Pod/probe-test-app-mfchw              Successfully assigned default/probe-test-app-mfchw to docker-desktop
40s                   Normal    SuccessfulCreate          ReplicaSet/probe-test-app             Created pod: probe-test-app-5vwsg
40s                   Normal    SuccessfulCreate          ReplicaSet/probe-test-app             Created pod: probe-test-app-jttds
40s                   Normal    SuccessfulCreate          ReplicaSet/probe-test-app             Created pod: probe-test-app-mfchw
40s                   Normal    Scheduled                 Pod/probe-test-app-5vwsg              Successfully assigned default/probe-test-app-5vwsg to docker-desktop
39s                   Normal    Created                   Pod/probe-test-app-mfchw              Created container probe-test-app
39s                   Normal    Started                   Pod/probe-test-app-jttds              Started container probe-test-app
39s                   Normal    Pulled                    Pod/probe-test-app-5vwsg              Container image "jasonumiker/probe-test-app:v1" already present on machine
39s                   Normal    Created                   Pod/probe-test-app-5vwsg              Created container probe-test-app
39s                   Normal    Pulled                    Pod/probe-test-app-mfchw              Container image "jasonumiker/probe-test-app:v1" already present on machine
39s                   Normal    Started                   Pod/probe-test-app-5vwsg              Started container probe-test-app
39s                   Warning   Unhealthy                 Pod/probe-test-app-mfchw              Readiness probe failed: Get "http://10.1.0.10:8080/readyz": dial tcp 10.1.0.10:8080: connect: connection refused
39s                   Normal    Pulled                    Pod/probe-test-app-jttds              Container image "jasonumiker/probe-test-app:v1" already present on machine
39s                   Normal    Created                   Pod/probe-test-app-jttds              Created container probe-test-app
39s                   Warning   Unhealthy                 Pod/probe-test-app-5vwsg              Readiness probe failed: Get "http://10.1.0.9:8080/readyz": dial tcp 10.1.0.9:8080: connect: connection refused
39s                   Normal    Started                   Pod/probe-test-app-mfchw              Started container probe-test-app
39s                   Warning   Unhealthy                 Pod/probe-test-app-jttds              Readiness probe failed: Get "http://10.1.0.8:8080/readyz": dial tcp 10.1.0.8:8080: connect: connection refused
36s                   Normal    Killing                   Pod/probe-test-app-jttds              Stopping container probe-test-app
36s                   Normal    SuccessfulDelete          ReplicaSet/probe-test-app             Deleted pod: probe-test-app-jttds
34s                   Warning   FailedToUpdateEndpoint    Endpoints/probe-test-app              Failed to update endpoint default/probe-test-app: Operation cannot be fulfilled on endpoints "probe-test-app": the object has been modified; please apply your changes to the latest version and try again
34s                   Normal    Killing                   Pod/probe-test-app-5vwsg              Stopping container probe-test-app
34s                   Normal    Killing                   Pod/probe-test-app-mfchw              Stopping container probe-test-app
33s                   Normal    Created                   Pod/probe-test-app-fc7776cb8-kchcv    Created container probe-test-app
33s                   Normal    Scheduled                 Pod/probe-test-app-fc7776cb8-kchcv    Successfully assigned default/probe-test-app-fc7776cb8-kchcv to docker-desktop
33s                   Normal    Scheduled                 Pod/probe-test-app-fc7776cb8-m4td4    Successfully assigned default/probe-test-app-fc7776cb8-m4td4 to docker-desktop
33s                   Normal    Pulled                    Pod/probe-test-app-fc7776cb8-m4td4    Container image "jasonumiker/probe-test-app:v1" already present on machine
33s                   Normal    Created                   Pod/probe-test-app-fc7776cb8-m4td4    Created container probe-test-app
33s                   Normal    Started                   Pod/probe-test-app-fc7776cb8-m4td4    Started container probe-test-app
33s                   Normal    Scheduled                 Pod/probe-test-app-fc7776cb8-gsqlg    Successfully assigned default/probe-test-app-fc7776cb8-gsqlg to docker-desktop
33s                   Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8   Created pod: probe-test-app-fc7776cb8-kchcv
33s                   Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8   Created pod: probe-test-app-fc7776cb8-m4td4
33s                   Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8   Created pod: probe-test-app-fc7776cb8-gsqlg
33s                   Normal    Pulled                    Pod/probe-test-app-fc7776cb8-gsqlg    Container image "jasonumiker/probe-test-app:v1" already present on machine
33s                   Normal    Created                   Pod/probe-test-app-fc7776cb8-gsqlg    Created container probe-test-app
33s                   Normal    Started                   Pod/probe-test-app-fc7776cb8-gsqlg    Started container probe-test-app
33s                   Normal    Started                   Pod/probe-test-app-fc7776cb8-kchcv    Started container probe-test-app
33s                   Normal    Pulled                    Pod/probe-test-app-fc7776cb8-kchcv    Container image "jasonumiker/probe-test-app:v1" already present on machine
33s                   Normal    ScalingReplicaSet         Deployment/probe-test-app             Scaled up replica set probe-test-app-fc7776cb8 to 3
28s                   Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fb95466cc   Created pod: probe-test-app-fb95466cc-zqsg5
28s                   Normal    ScalingReplicaSet         Deployment/probe-test-app             Scaled up replica set probe-test-app-fb95466cc to 1
28s                   Normal    Scheduled                 Pod/probe-test-app-fb95466cc-zqsg5    Successfully assigned default/probe-test-app-fb95466cc-zqsg5 to docker-desktop
28s                   Normal    Pulling                   Pod/probe-test-app-fb95466cc-zqsg5    Pulling image "jasonumiker/probe-test-app:v2"
23s                   Normal    Started                   Pod/probe-test-app-fb95466cc-zqsg5    Started container probe-test-app
23s                   Normal    Created                   Pod/probe-test-app-fb95466cc-zqsg5    Created container probe-test-app
23s                   Normal    Pulled                    Pod/probe-test-app-fb95466cc-zqsg5    Successfully pulled image "jasonumiker/probe-test-app:v2" in 4.658s (4.658s including waiting). Image size: 1025410707 bytes.
22s                   Normal    Killing                   Pod/probe-test-app-fc7776cb8-kchcv    Stopping container probe-test-app
22s                   Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fb95466cc   Created pod: probe-test-app-fb95466cc-bbsbp
22s                   Normal    ScalingReplicaSet         Deployment/probe-test-app             Scaled up replica set probe-test-app-fb95466cc to 2 from 1
22s                   Normal    Started                   Pod/probe-test-app-fb95466cc-bbsbp    Started container probe-test-app
22s                   Normal    SuccessfulDelete          ReplicaSet/probe-test-app-fc7776cb8   Deleted pod: probe-test-app-fc7776cb8-kchcv
22s                   Normal    ScalingReplicaSet         Deployment/probe-test-app             Scaled down replica set probe-test-app-fc7776cb8 to 2 from 3
22s                   Normal    Scheduled                 Pod/probe-test-app-fb95466cc-bbsbp    Successfully assigned default/probe-test-app-fb95466cc-bbsbp to docker-desktop
22s                   Normal    Pulled                    Pod/probe-test-app-fb95466cc-bbsbp    Container image "jasonumiker/probe-test-app:v2" already present on machine
22s                   Normal    Created                   Pod/probe-test-app-fb95466cc-bbsbp    Created container probe-test-app
12s                   Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fb95466cc   Created pod: probe-test-app-fb95466cc-tkq4p
12s                   Normal    Scheduled                 Pod/probe-test-app-fb95466cc-tkq4p    Successfully assigned default/probe-test-app-fb95466cc-tkq4p to docker-desktop
12s                   Normal    Pulled                    Pod/probe-test-app-fb95466cc-tkq4p    Container image "jasonumiker/probe-test-app:v2" already present on machine
12s                   Normal    Created                   Pod/probe-test-app-fb95466cc-tkq4p    Created container probe-test-app
12s                   Normal    Started                   Pod/probe-test-app-fb95466cc-tkq4p    Started container probe-test-app
12s                   Normal    Killing                   Pod/probe-test-app-fc7776cb8-gsqlg    Stopping container probe-test-app
12s                   Normal    SuccessfulDelete          ReplicaSet/probe-test-app-fc7776cb8   Deleted pod: probe-test-app-fc7776cb8-gsqlg
12s                   Normal    ScalingReplicaSet         Deployment/probe-test-app             Scaled down replica set probe-test-app-fc7776cb8 to 1 from 2
12s                   Normal    ScalingReplicaSet         Deployment/probe-test-app             Scaled up replica set probe-test-app-fb95466cc to 3 from 2
2s                    Normal    SuccessfulDelete          ReplicaSet/probe-test-app-fc7776cb8   Deleted pod: probe-test-app-fc7776cb8-m4td4
2s                    Normal    Killing                   Pod/probe-test-app-fc7776cb8-m4td4    Stopping container probe-test-app
2s                    Normal    ScalingReplicaSet         Deployment/probe-test-app             Scaled down replica set probe-test-app-fc7776cb8 to 0 from 1
--------------------
kubectl rollout undo deployment/probe-test-app
deployment.apps/probe-test-app rolled back
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "probe-test-app" rollout to finish: 1 old replicas are pending termination...
deployment "probe-test-app" successfully rolled out
--------------------
kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
probe-test-app-fc7776cb8-5xsfr   1/1     Running   0          3s
probe-test-app-fc7776cb8-rtgdp   1/1     Running   0          4s
probe-test-app-fc7776cb8-zzlg2   1/1     Running   0          2s
--------------------
kubectl describe replicaset probe-test-app
Name:           probe-test-app-fb95466cc
Namespace:      default
Selector:       app.kubernetes.io/name=probe-test-app,pod-template-hash=fb95466cc
Labels:         app.kubernetes.io/name=probe-test-app
                pod-template-hash=fb95466cc
Annotations:    deployment.kubernetes.io/desired-replicas: 3
                deployment.kubernetes.io/max-replicas: 4
                deployment.kubernetes.io/revision: 2
Controlled By:  Deployment/probe-test-app
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app.kubernetes.io/name=probe-test-app
           pod-template-hash=fb95466cc
  Containers:
   probe-test-app:
    Image:      jasonumiker/probe-test-app:v2
    Port:       8080/TCP
    Host Port:  0/TCP
    Requests:
      cpu:         50m
      memory:      52Mi
    Liveness:      http-get http://:8080/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:     http-get http://:8080/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  37s   replicaset-controller  Created pod: probe-test-app-fb95466cc-zqsg5
  Normal  SuccessfulCreate  31s   replicaset-controller  Created pod: probe-test-app-fb95466cc-bbsbp
  Normal  SuccessfulCreate  21s   replicaset-controller  Created pod: probe-test-app-fb95466cc-tkq4p
  Normal  SuccessfulDelete  6s    replicaset-controller  Deleted pod: probe-test-app-fb95466cc-tkq4p
  Normal  SuccessfulDelete  5s    replicaset-controller  Deleted pod: probe-test-app-fb95466cc-bbsbp
  Normal  SuccessfulDelete  4s    replicaset-controller  Deleted pod: probe-test-app-fb95466cc-zqsg5

Name:           probe-test-app-fc7776cb8
Namespace:      default
Selector:       app.kubernetes.io/name=probe-test-app,pod-template-hash=fc7776cb8
Labels:         app.kubernetes.io/name=probe-test-app
                pod-template-hash=fc7776cb8
Annotations:    deployment.kubernetes.io/desired-replicas: 3
                deployment.kubernetes.io/max-replicas: 4
                deployment.kubernetes.io/revision: 3
                deployment.kubernetes.io/revision-history: 1
Controlled By:  Deployment/probe-test-app
Replicas:       3 current / 3 desired
Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app.kubernetes.io/name=probe-test-app
           pod-template-hash=fc7776cb8
  Containers:
   probe-test-app:
    Image:      jasonumiker/probe-test-app:v1
    Port:       8080/TCP
    Host Port:  0/TCP
    Requests:
      cpu:         50m
      memory:      52Mi
    Liveness:      http-get http://:8080/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:     http-get http://:8080/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  42s   replicaset-controller  Created pod: probe-test-app-fc7776cb8-kchcv
  Normal  SuccessfulCreate  42s   replicaset-controller  Created pod: probe-test-app-fc7776cb8-m4td4
  Normal  SuccessfulCreate  42s   replicaset-controller  Created pod: probe-test-app-fc7776cb8-gsqlg
  Normal  SuccessfulDelete  31s   replicaset-controller  Deleted pod: probe-test-app-fc7776cb8-kchcv
  Normal  SuccessfulDelete  21s   replicaset-controller  Deleted pod: probe-test-app-fc7776cb8-gsqlg
  Normal  SuccessfulDelete  11s   replicaset-controller  Deleted pod: probe-test-app-fc7776cb8-m4td4
  Normal  SuccessfulCreate  7s    replicaset-controller  Created pod: probe-test-app-fc7776cb8-rtgdp
  Normal  SuccessfulCreate  6s    replicaset-controller  Created pod: probe-test-app-fc7776cb8-5xsfr
  Normal  SuccessfulCreate  5s    replicaset-controller  Created pod: probe-test-app-fc7776cb8-zzlg2

--------------------
cd ../sidecar-and-init-containers
--------------------
kubectl apply -f sidecar.yaml
pod/pod-with-sidecar created
error: timed out waiting for the condition on pods/pod-with-sidecar
--------------------
kubectl apply -f init.yaml
pod/myapp-pod created
--------------------
kubectl get pod myapp-pod
NAME        READY   STATUS     RESTARTS   AGE
myapp-pod   0/1     Init:0/2   0          1s
--------------------
kubectl apply -f services-init-requires.yaml
service/myservice created
service/mydb created
--------------------
NAME        READY   STATUS    RESTARTS   AGE
myapp-pod   1/1     Running   0          13s
--------------------
pod "myapp-pod" deleted
pod "pod-with-sidecar" deleted
service "myservice" deleted
service "mydb" deleted
--------------------
cd ../pvs-and-statefulsets
--------------------
kubectl apply -f hostpath-provisioner.yaml
deployment.apps/hostpath-provisioner created
storageclass.storage.k8s.io/hostpath-provisioner created
serviceaccount/hostpath-provisioner created
clusterrole.rbac.authorization.k8s.io/hostpath-provisioner created
clusterrolebinding.rbac.authorization.k8s.io/hostpath-provisioner created
--------------------
kubectl get storageclass
NAME                   PROVISIONER            RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
hostpath (default)     docker.io/hostpath     Delete          Immediate              false                  3m51s
hostpath-provisioner   microk8s.io/hostpath   Delete          WaitForFirstConsumer   false                  1s
--------------------
kubectl apply -f pvc.yaml
persistentvolumeclaim/test-pvc created
--------------------
kubectl get pvc
NAME       STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS           VOLUMEATTRIBUTESCLASS   AGE
test-pvc   Pending                                      hostpath-provisioner   <unset>                 1s
--------------------
kubectl apply -f pod.yaml
pod/nginx created
pod/nginx condition met
--------------------
kubectl get pvc
NAME       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS           VOLUMEATTRIBUTESCLASS   AGE
test-pvc   Bound    pvc-3afa0558-d64a-4ccb-801c-11316db42eab   1Gi        RWO            hostpath-provisioner   <unset>                 13s
--------------------
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS           VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-3afa0558-d64a-4ccb-801c-11316db42eab   1Gi        RWO            Delete           Bound    default/test-pvc   hostpath-provisioner   <unset>                          6s
--------------------
kubectl apply -f service.yaml
service/nginx created
--------------------
curl http://localhost:8001
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100   153  100   153    0     0  34749      0 --:--:-- --:--:-- --:--:-- 38250
<html>
<head><title>403 Forbidden</title></head>
<body>
<center><h1>403 Forbidden</h1></center>
<hr><center>nginx/1.27.2</center>
</body>
</html>
--------------------
kubectl exec -it nginx  -- bash -c "echo 'Data on PV' > /usr/share/nginx/html/index.html"
--------------------
curl http://localhost:8001
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100    11  100    11    0     0   2077      0 --:--:-- --:--:-- --:--:--  2200
Data on PV
--------------------
kubectl delete pod nginx
pod "nginx" deleted
--------------------
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS           VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-3afa0558-d64a-4ccb-801c-11316db42eab   1Gi        RWO            Delete           Bound    default/test-pvc   hostpath-provisioner   <unset>                          23s
--------------------
kubectl apply -f pod.yaml
pod/nginx created
pod/nginx condition met
--------------------
curl http://localhost:8001
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100    11  100    11    0     0   1752      0 --:--:-- --:--:-- --:--:--  1833
Data on PV
--------------------
kubectl delete service nginx
service "nginx" deleted
--------------------
kubectl delete pod nginx
pod "nginx" deleted
--------------------
kubectl delete pvc test-pvc
persistentvolumeclaim "test-pvc" deleted
--------------------
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM              STORAGECLASS           VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-3afa0558-d64a-4ccb-801c-11316db42eab   1Gi        RWO            Delete           Released   default/test-pvc   hostpath-provisioner   <unset>                          32s
--------------------
--------------------
kubectl apply -k .
serviceaccount/rabbitmq created
role.rbac.authorization.k8s.io/rabbitmq created
rolebinding.rbac.authorization.k8s.io/rabbitmq created
configmap/rabbitmq-config created
secret/erlang-cookie created
secret/rabbitmq-admin created
service/rabbitmq-client created
service/rabbitmq-headless created
statefulset.apps/rabbitmq created
Waiting for 1 pods to be ready...
partitioned roll out complete: 1 new pods have been updated...
--------------------
kubectl describe statefulset rabbitmq
Name:               rabbitmq
Namespace:          default
CreationTimestamp:  Sat, 23 Nov 2024 17:36:17 +1100
Selector:           app=rabbitmq
Labels:             <none>
Annotations:        <none>
Replicas:           1 desired | 1 total
Update Strategy:    RollingUpdate
  Partition:        0
Pods Status:        1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=rabbitmq
  Service Account:  rabbitmq
  Init Containers:
   rabbitmq-config:
    Image:      busybox:1.37.0
    Port:       <none>
    Host Port:  <none>
    Command:
      sh
      -c
      cp /tmp/rabbitmq/rabbitmq.conf /etc/rabbitmq/rabbitmq.conf && echo '' >> /etc/rabbitmq/rabbitmq.conf; cp /tmp/rabbitmq/enabled_plugins /etc/rabbitmq/enabled_plugins
    Environment:  <none>
    Mounts:
      /etc/rabbitmq from rabbitmq-config-rw (rw)
      /tmp/rabbitmq from rabbitmq-config (rw)
  Containers:
   rabbitmq:
    Image:       rabbitmq:3.8.34
    Ports:       5672/TCP, 15672/TCP, 15692/TCP, 4369/TCP
    Host Ports:  0/TCP, 0/TCP, 0/TCP, 0/TCP
    Liveness:    exec [rabbitmq-diagnostics status] delay=60s timeout=15s period=60s #success=1 #failure=3
    Readiness:   exec [rabbitmq-diagnostics ping] delay=20s timeout=10s period=60s #success=1 #failure=3
    Environment:
      RABBITMQ_DEFAULT_PASS:   <set to the key 'pass' in secret 'rabbitmq-admin'>   Optional: false
      RABBITMQ_DEFAULT_USER:   <set to the key 'user' in secret 'rabbitmq-admin'>   Optional: false
      RABBITMQ_ERLANG_COOKIE:  <set to the key 'cookie' in secret 'erlang-cookie'>  Optional: false
    Mounts:
      /etc/rabbitmq from rabbitmq-config-rw (rw)
      /var/lib/rabbitmq/mnesia from rabbitmq-data (rw)
  Volumes:
   rabbitmq-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      rabbitmq-config
    Optional:  false
   rabbitmq-config-rw:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
   rabbitmq-data:
    Type:          PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:     rabbitmq-data
    ReadOnly:      false
  Node-Selectors:  <none>
  Tolerations:     <none>
Volume Claims:
  Name:          rabbitmq-data
  StorageClass:  hostpath-provisioner
  Labels:        <none>
  Annotations:   <none>
  Capacity:      3Gi
  Access Modes:  [ReadWriteOnce]
Events:
  Type    Reason            Age   From                    Message
  ----    ------            ----  ----                    -------
  Normal  SuccessfulCreate  68s   statefulset-controller  create Claim rabbitmq-data-rabbitmq-0 Pod rabbitmq-0 in StatefulSet rabbitmq success
  Normal  SuccessfulCreate  68s   statefulset-controller  create Pod rabbitmq-0 in StatefulSet rabbitmq successful
--------------------
kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
probe-test-app-fc7776cb8-5xsfr   1/1     Running   0          3m52s
probe-test-app-fc7776cb8-rtgdp   1/1     Running   0          3m53s
probe-test-app-fc7776cb8-zzlg2   1/1     Running   0          3m51s
rabbitmq-0                       1/1     Running   0          69s
--------------------
kubectl get pvc
NAME                       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS           VOLUMEATTRIBUTESCLASS   AGE
rabbitmq-data-rabbitmq-0   Bound    pvc-13760883-7618-46ce-a70f-ad0cfdc33eb8   3Gi        RWO            hostpath-provisioner   <unset>                 70s
--------------------
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                              STORAGECLASS           VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-13760883-7618-46ce-a70f-ad0cfdc33eb8   3Gi        RWO            Delete           Bound    default/rabbitmq-data-rabbitmq-0   hostpath-provisioner   <unset>                          66s
--------------------
kubectl delete pod rabbitmq-0
pod "rabbitmq-0" deleted
--------------------
kubectl get pods
NAME                             READY   STATUS            RESTARTS   AGE
probe-test-app-fc7776cb8-5xsfr   1/1     Running           0          3m57s
probe-test-app-fc7776cb8-rtgdp   1/1     Running           0          3m58s
probe-test-app-fc7776cb8-zzlg2   1/1     Running           0          3m56s
rabbitmq-0                       0/1     PodInitializing   0          1s
--------------------
cd ../../monitoring
--------------------
./install-prometheus.sh

Updating docker-desktop pods to expose metrics endpoints
This will involve several kube-system pod restarts

Fetching debian image to run nsenter on the docker-desktop host...
12.8: Pulling from library/debian
1a3f1864ec54: Pulling fs layer
1a3f1864ec54: Verifying Checksum
1a3f1864ec54: Download complete
1a3f1864ec54: Pull complete
Digest: sha256:10901ccd8d249047f9761845b4594f121edef079cfd8224edebd9ea726f0a7f6
Status: Downloaded newer image for debian:12.8
docker.io/library/debian:12.8
Host Node IP: 192.168.65.3
Updating kube-proxy configmap...
configmap "kube-proxy" deleted
configmap/kube-proxy created
Restarting the kube-proxy pod
pod "kube-proxy-7f8mr" deleted
pod/kube-proxy-fdl9f condition met
kube-proxy pod restarted.
Updating bind-address on kube-controller-manager...
Waiting for kube-controller-manager to restart, this can take some time...
pod/kube-controller-manager-docker-desktop condition met
pod/kube-controller-manager-docker-desktop condition met
kube-controller-manager pod restarted.
Updating bind-address on kube-scheduler
Waiting for kube-scheduler to restart, this can take some time...
pod/kube-scheduler-docker-desktop condition met
pod/kube-scheduler-docker-desktop condition met
kube-scheduler pod restarted.
Adding node ip to listen-metrics-urls on etcd
Waiting for etcd to restart, this can take some time...
Error from server (Timeout): the server was unable to return a response in the time allotted, but may still be processing the request (get pods)
Error from server (NotFound): pods "etcd-docker-desktop" not found
etcd pod did not restart in time - this may just be the api server still rebooting, give it a few minutes before panicking.

Done! You can now deploy the monitoring components.

"prometheus-community" already exists with the same configuration, skipping
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "kedacore" chart repository
...Successfully got an update from the "ingress-nginx" chart repository
...Successfully got an update from the "kiali" chart repository
...Successfully got an update from the "prometheus-community" chart repository
...Successfully got an update from the "istio" chart repository
Update Complete. ⎈Happy Helming!⎈
namespace/monitoring created
NAME: prometheus
LAST DEPLOYED: Sat Nov 23 17:39:52 2024
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
NOTES:
kube-prometheus-stack has been installed. Check its status by running:
  kubectl --namespace monitoring get pods -l "release=prometheus"

Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create & configure Alertmanager and Prometheus instances using the Operator.
NAME: adapter
LAST DEPLOYED: Sat Nov 23 17:40:20 2024
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
adapter-prometheus-adapter has been deployed.
In a few minutes you should be able to list metrics using the following command(s):

  kubectl get --raw /apis/metrics.k8s.io/v1beta1
  kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1
Waiting for deployment "adapter-prometheus-adapter" rollout to finish: 0 of 1 updated replicas are available...
deployment "adapter-prometheus-adapter" successfully rolled out
--------------------
kubectl top nodes
NAME             CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
docker-desktop   94m          0%     2743Mi          35%       
--------------------
kubectl top pods
NAME                             CPU(cores)   MEMORY(bytes)   
probe-test-app-fc7776cb8-5xsfr   0m           39Mi            
probe-test-app-fc7776cb8-rtgdp   0m           39Mi            
probe-test-app-fc7776cb8-zzlg2   0m           40Mi            
rabbitmq-0                       1m           119Mi           
--------------------
kubectl top pods -n monitoring
NAME                                                     CPU(cores)   MEMORY(bytes)   
adapter-prometheus-adapter-b84b78594-9j7w5               9m           51Mi            
alertmanager-prometheus-kube-prometheus-alertmanager-0   0m           30Mi            
prometheus-grafana-6b758d7b46-j9tpf                      1m           277Mi           
prometheus-kube-prometheus-operator-c5f7c5b6-b9lmx       0m           28Mi            
prometheus-kube-state-metrics-677845d566-l9vqc           0m           20Mi            
prometheus-prometheus-kube-prometheus-prometheus-0       11m          103Mi           
prometheus-prometheus-node-exporter-k66tn                0m           7Mi             
--------------------
cd ../probe-test-app
--------------------
kubectl apply -f probe-test-app-hpa.yaml
horizontalpodautoscaler.autoscaling/probe-test-app created
--------------------
kubectl apply -f generate-load-app-replicaset.yaml
replicaset.apps/generate-load-app created
--------------------
kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
generate-load-app-5w2bw          1/1     Running   0          31s
generate-load-app-9g9w4          1/1     Running   0          31s
generate-load-app-g2xjf          1/1     Running   0          31s
generate-load-app-mx67r          1/1     Running   0          31s
generate-load-app-r2pn4          1/1     Running   0          31s
probe-test-app-fc7776cb8-5xsfr   1/1     Running   0          9m54s
probe-test-app-fc7776cb8-rtgdp   1/1     Running   0          9m55s
probe-test-app-fc7776cb8-zzlg2   1/1     Running   0          9m53s
rabbitmq-0                       1/1     Running   0          5m58s
--------------------
kubectl delete replicaset generate-load-app
replicaset.apps "generate-load-app" deleted
--------------------
kubectl describe hpa probe-test-app
Name:                                                  probe-test-app
Namespace:                                             default
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Sat, 23 Nov 2024 17:42:56 +1100
Reference:                                             Deployment/probe-test-app
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  45% (22m) / 50%
Min replicas:                                          1
Max replicas:                                          5
Deployment pods:                                       3 current / 3 desired
Conditions:
  Type            Status  Reason              Message
  ----            ------  ------              -------
  AbleToScale     True    ReadyForNewScale    recommended size matches current size
  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)
  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range
Events:           <none>
--------------------
cd ../limit-examples
--------------------
kubectl apply -f cpu-stressor.yaml
deployment.apps/cpu-stressor created
Waiting for deployment "cpu-stressor" rollout to finish: 0 of 1 updated replicas are available...
deployment "cpu-stressor" successfully rolled out
--------------------
kubectl delete deployment cpu-stressor
deployment.apps "cpu-stressor" deleted
--------------------
kubectl apply -f memory-stressor.yaml
pod/memory-stressor created
--------------------
kubectl delete pod memory-stressor
pod "memory-stressor" deleted
--------------------
cd ../keda-example
--------------------
./install-keda.sh
"kedacore" already exists with the same configuration, skipping
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "kedacore" chart repository
...Successfully got an update from the "ingress-nginx" chart repository
...Successfully got an update from the "kiali" chart repository
...Successfully got an update from the "prometheus-community" chart repository
...Successfully got an update from the "istio" chart repository
Update Complete. ⎈Happy Helming!⎈
NAME: keda
LAST DEPLOYED: Sat Nov 23 17:44:28 2024
NAMESPACE: keda
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
:::^.     .::::^:     :::::::::::::::    .:::::::::.                   .^.                  
7???~   .^7????~.     7??????????????.   :?????????77!^.              .7?7.                 
7???~  ^7???7~.       ~!!!!!!!!!!!!!!.   :????!!!!7????7~.           .7???7.                
7???~^7????~.                            :????:    :~7???7.         :7?????7.               
7???7????!.           ::::::::::::.      :????:      .7???!        :7??77???7.              
7????????7:           7???????????~      :????:       :????:      :???7?5????7.             
7????!~????^          !77777777777^      :????:       :????:     ^???7?#P7????7.            
7???~  ^????~                            :????:      :7???!     ^???7J#@J7?????7.           
7???~   :7???!.                          :????:   .:~7???!.    ~???7Y&@#7777????7.          
7???~    .7???7:      !!!!!!!!!!!!!!!    :????7!!77????7^     ~??775@@@GJJYJ?????7.         
7???~     .!????^     7?????????????7.   :?????????7!~:      !????G@@@@@@@@5??????7:        
::::.       :::::     :::::::::::::::    .::::::::..        .::::JGGGB@@@&7:::::::::        
                                                                      ?@@#~                  
                                                                      P@B^                   
                                                                    :&G:                    
                                                                    !5.                     
                                                                    .Kubernetes Event-driven Autoscaling (KEDA) - Application autoscaling made simple.

Get started by deploying Scaled Objects to your cluster:
    - Information about Scaled Objects : https://keda.sh/docs/latest/concepts/
    - Samples: https://github.com/kedacore/samples

Get information about the deployed ScaledObjects:
  kubectl get scaledobject [--namespace <namespace>]

Get details about a deployed ScaledObject:
  kubectl describe scaledobject <scaled-object-name> [--namespace <namespace>]

Get information about the deployed ScaledObjects:
  kubectl get triggerauthentication [--namespace <namespace>]

Get details about a deployed ScaledObject:
  kubectl describe triggerauthentication <trigger-authentication-name> [--namespace <namespace>]

Get an overview of the Horizontal Pod Autoscalers (HPA) that KEDA is using behind the scenes:
  kubectl get hpa [--all-namespaces] [--namespace <namespace>]

Learn more about KEDA:
- Documentation: https://keda.sh/
- Support: https://keda.sh/support/
- File an issue: https://github.com/kedacore/keda/issues/new/choose
--------------------
kubectl apply -f consumer.yaml
secret/rabbitmq-consumer-secret created
deployment.apps/rabbitmq-consumer created
--------------------
kubectl apply -f keda-scaled-object.yaml
scaledobject.keda.sh/rabbitmq-consumer created
triggerauthentication.keda.sh/rabbitmq-consumer-trigger created
--------------------
kubectl apply -f publisher.yaml
job.batch/rabbitmq-publish created
--------------------
kubectl get pods
NAME                                READY   STATUS              RESTARTS   AGE
probe-test-app-fc7776cb8-5xsfr      1/1     Running             0          11m
probe-test-app-fc7776cb8-rtgdp      1/1     Running             0          11m
probe-test-app-fc7776cb8-zzlg2      1/1     Running             0          11m
rabbitmq-0                          1/1     Running             0          7m14s
rabbitmq-consumer-dd9d7cfd4-l5bb8   1/1     Running             0          4s
rabbitmq-publish-2f22w              0/1     ContainerCreating   0          1s
--------------------
kubectl events
LAST SEEN               TYPE      REASON                    OBJECT                                           MESSAGE
13m (x8 over 13m)       Normal    NodeHasSufficientMemory   Node/docker-desktop                              Node docker-desktop status is now: NodeHasSufficientMemory
13m                     Normal    NodeAllocatableEnforced   Node/docker-desktop                              Updated Node Allocatable limit across pods
13m (x7 over 13m)       Normal    NodeHasSufficientPID      Node/docker-desktop                              Node docker-desktop status is now: NodeHasSufficientPID
13m (x7 over 13m)       Normal    NodeHasNoDiskPressure     Node/docker-desktop                              Node docker-desktop status is now: NodeHasNoDiskPressure
13m                     Normal    Starting                  Node/docker-desktop                              Starting kubelet.
13m                     Warning   InvalidDiskCapacity       Node/docker-desktop                              invalid capacity 0 on image filesystem
13m                     Normal    RegisteredNode            Node/docker-desktop                              Node docker-desktop event: Registered Node docker-desktop in Controller
13m                     Normal    Starting                  Node/docker-desktop                              
12m                     Normal    Scheduled                 Pod/probe-test-app                               Successfully assigned default/probe-test-app to docker-desktop
12m                     Normal    Pulling                   Pod/probe-test-app                               Pulling image "jasonumiker/probe-test-app:v1"
12m                     Normal    Started                   Pod/probe-test-app                               Started container probe-test-app
12m                     Normal    Pulled                    Pod/probe-test-app                               Successfully pulled image "jasonumiker/probe-test-app:v1" in 14.405s (14.405s including waiting). Image size: 1025410707 bytes.
12m                     Normal    Created                   Pod/probe-test-app                               Created container probe-test-app
12m                     Warning   Unhealthy                 Pod/probe-test-app                               Readiness probe failed: Get "http://10.1.0.6:8080/readyz": dial tcp 10.1.0.6:8080: connect: connection refused
12m                     Normal    Started                   Pod/probe-test-app-2                             Started container probe-test-app
12m                     Normal    Created                   Pod/probe-test-app-2                             Created container probe-test-app
12m                     Normal    Pulled                    Pod/probe-test-app-2                             Container image "jasonumiker/probe-test-app:v1" already present on machine
12m                     Normal    Scheduled                 Pod/probe-test-app-2                             Successfully assigned default/probe-test-app-2 to docker-desktop
11m                     Normal    Killing                   Pod/probe-test-app                               Stopping container probe-test-app
11m                     Normal    Killing                   Pod/probe-test-app-2                             Stopping container probe-test-app
11m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app                        Created pod: probe-test-app-mfchw
11m                     Normal    Scheduled                 Pod/probe-test-app-mfchw                         Successfully assigned default/probe-test-app-mfchw to docker-desktop
11m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app                        Created pod: probe-test-app-5vwsg
11m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app                        Created pod: probe-test-app-jttds
11m                     Normal    Scheduled                 Pod/probe-test-app-5vwsg                         Successfully assigned default/probe-test-app-5vwsg to docker-desktop
11m                     Normal    Scheduled                 Pod/probe-test-app-jttds                         Successfully assigned default/probe-test-app-jttds to docker-desktop
11m                     Normal    Started                   Pod/probe-test-app-jttds                         Started container probe-test-app
11m                     Normal    Pulled                    Pod/probe-test-app-5vwsg                         Container image "jasonumiker/probe-test-app:v1" already present on machine
11m                     Normal    Created                   Pod/probe-test-app-mfchw                         Created container probe-test-app
11m                     Warning   Unhealthy                 Pod/probe-test-app-5vwsg                         Readiness probe failed: Get "http://10.1.0.9:8080/readyz": dial tcp 10.1.0.9:8080: connect: connection refused
11m                     Warning   Unhealthy                 Pod/probe-test-app-jttds                         Readiness probe failed: Get "http://10.1.0.8:8080/readyz": dial tcp 10.1.0.8:8080: connect: connection refused
11m                     Normal    Started                   Pod/probe-test-app-mfchw                         Started container probe-test-app
11m                     Normal    Created                   Pod/probe-test-app-jttds                         Created container probe-test-app
11m                     Normal    Pulled                    Pod/probe-test-app-jttds                         Container image "jasonumiker/probe-test-app:v1" already present on machine
11m                     Warning   Unhealthy                 Pod/probe-test-app-mfchw                         Readiness probe failed: Get "http://10.1.0.10:8080/readyz": dial tcp 10.1.0.10:8080: connect: connection refused
11m                     Normal    Started                   Pod/probe-test-app-5vwsg                         Started container probe-test-app
11m                     Normal    Pulled                    Pod/probe-test-app-mfchw                         Container image "jasonumiker/probe-test-app:v1" already present on machine
11m                     Normal    Created                   Pod/probe-test-app-5vwsg                         Created container probe-test-app
11m                     Normal    SuccessfulDelete          ReplicaSet/probe-test-app                        Deleted pod: probe-test-app-jttds
11m                     Normal    Killing                   Pod/probe-test-app-jttds                         Stopping container probe-test-app
11m                     Normal    Killing                   Pod/probe-test-app-mfchw                         Stopping container probe-test-app
11m                     Normal    Killing                   Pod/probe-test-app-5vwsg                         Stopping container probe-test-app
11m                     Warning   FailedToUpdateEndpoint    Endpoints/probe-test-app                         Failed to update endpoint default/probe-test-app: Operation cannot be fulfilled on endpoints "probe-test-app": the object has been modified; please apply your changes to the latest version and try again
11m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8              Created pod: probe-test-app-fc7776cb8-m4td4
11m                     Normal    Pulled                    Pod/probe-test-app-fc7776cb8-m4td4               Container image "jasonumiker/probe-test-app:v1" already present on machine
11m                     Normal    Scheduled                 Pod/probe-test-app-fc7776cb8-kchcv               Successfully assigned default/probe-test-app-fc7776cb8-kchcv to docker-desktop
11m                     Normal    Started                   Pod/probe-test-app-fc7776cb8-gsqlg               Started container probe-test-app
11m                     Normal    Created                   Pod/probe-test-app-fc7776cb8-gsqlg               Created container probe-test-app
11m                     Normal    Pulled                    Pod/probe-test-app-fc7776cb8-gsqlg               Container image "jasonumiker/probe-test-app:v1" already present on machine
11m                     Normal    Scheduled                 Pod/probe-test-app-fc7776cb8-gsqlg               Successfully assigned default/probe-test-app-fc7776cb8-gsqlg to docker-desktop
11m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled up replica set probe-test-app-fc7776cb8 to 3
11m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8              Created pod: probe-test-app-fc7776cb8-gsqlg
11m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8              Created pod: probe-test-app-fc7776cb8-kchcv
11m                     Normal    Created                   Pod/probe-test-app-fc7776cb8-kchcv               Created container probe-test-app
11m                     Normal    Started                   Pod/probe-test-app-fc7776cb8-kchcv               Started container probe-test-app
11m                     Normal    Scheduled                 Pod/probe-test-app-fc7776cb8-m4td4               Successfully assigned default/probe-test-app-fc7776cb8-m4td4 to docker-desktop
11m                     Normal    Pulled                    Pod/probe-test-app-fc7776cb8-kchcv               Container image "jasonumiker/probe-test-app:v1" already present on machine
11m                     Normal    Created                   Pod/probe-test-app-fc7776cb8-m4td4               Created container probe-test-app
11m                     Normal    Started                   Pod/probe-test-app-fc7776cb8-m4td4               Started container probe-test-app
11m                     Normal    Scheduled                 Pod/probe-test-app-fb95466cc-zqsg5               Successfully assigned default/probe-test-app-fb95466cc-zqsg5 to docker-desktop
11m                     Normal    Pulling                   Pod/probe-test-app-fb95466cc-zqsg5               Pulling image "jasonumiker/probe-test-app:v2"
11m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fb95466cc              Created pod: probe-test-app-fb95466cc-zqsg5
11m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled up replica set probe-test-app-fb95466cc to 1
11m                     Normal    Started                   Pod/probe-test-app-fb95466cc-zqsg5               Started container probe-test-app
11m                     Normal    Created                   Pod/probe-test-app-fb95466cc-zqsg5               Created container probe-test-app
11m                     Normal    Pulled                    Pod/probe-test-app-fb95466cc-zqsg5               Successfully pulled image "jasonumiker/probe-test-app:v2" in 4.658s (4.658s including waiting). Image size: 1025410707 bytes.
11m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled down replica set probe-test-app-fc7776cb8 to 2 from 3
11m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fb95466cc              Created pod: probe-test-app-fb95466cc-bbsbp
11m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled up replica set probe-test-app-fb95466cc to 2 from 1
11m                     Normal    Scheduled                 Pod/probe-test-app-fb95466cc-bbsbp               Successfully assigned default/probe-test-app-fb95466cc-bbsbp to docker-desktop
11m                     Normal    Pulled                    Pod/probe-test-app-fb95466cc-bbsbp               Container image "jasonumiker/probe-test-app:v2" already present on machine
11m                     Normal    Created                   Pod/probe-test-app-fb95466cc-bbsbp               Created container probe-test-app
11m                     Normal    SuccessfulDelete          ReplicaSet/probe-test-app-fc7776cb8              Deleted pod: probe-test-app-fc7776cb8-kchcv
11m                     Normal    Started                   Pod/probe-test-app-fb95466cc-bbsbp               Started container probe-test-app
11m                     Normal    Killing                   Pod/probe-test-app-fc7776cb8-kchcv               Stopping container probe-test-app
11m                     Normal    Killing                   Pod/probe-test-app-fc7776cb8-gsqlg               Stopping container probe-test-app
11m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fb95466cc              Created pod: probe-test-app-fb95466cc-tkq4p
11m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled down replica set probe-test-app-fc7776cb8 to 1 from 2
11m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled up replica set probe-test-app-fb95466cc to 3 from 2
11m                     Normal    Started                   Pod/probe-test-app-fb95466cc-tkq4p               Started container probe-test-app
11m                     Normal    Created                   Pod/probe-test-app-fb95466cc-tkq4p               Created container probe-test-app
11m                     Normal    Pulled                    Pod/probe-test-app-fb95466cc-tkq4p               Container image "jasonumiker/probe-test-app:v2" already present on machine
11m                     Normal    Scheduled                 Pod/probe-test-app-fb95466cc-tkq4p               Successfully assigned default/probe-test-app-fb95466cc-tkq4p to docker-desktop
11m                     Normal    SuccessfulDelete          ReplicaSet/probe-test-app-fc7776cb8              Deleted pod: probe-test-app-fc7776cb8-gsqlg
11m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled down replica set probe-test-app-fc7776cb8 to 0 from 1
11m                     Normal    Killing                   Pod/probe-test-app-fc7776cb8-m4td4               Stopping container probe-test-app
11m                     Normal    SuccessfulDelete          ReplicaSet/probe-test-app-fc7776cb8              Deleted pod: probe-test-app-fc7776cb8-m4td4
11m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled up replica set probe-test-app-fc7776cb8 to 1 from 0
11m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8              Created pod: probe-test-app-fc7776cb8-rtgdp
11m                     Normal    Scheduled                 Pod/probe-test-app-fc7776cb8-rtgdp               Successfully assigned default/probe-test-app-fc7776cb8-rtgdp to docker-desktop
11m                     Normal    Pulled                    Pod/probe-test-app-fc7776cb8-rtgdp               Container image "jasonumiker/probe-test-app:v1" already present on machine
11m                     Normal    Created                   Pod/probe-test-app-fc7776cb8-rtgdp               Created container probe-test-app
11m                     Normal    Started                   Pod/probe-test-app-fc7776cb8-rtgdp               Started container probe-test-app
11m                     Normal    Killing                   Pod/probe-test-app-fb95466cc-tkq4p               Stopping container probe-test-app
11m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8              Created pod: probe-test-app-fc7776cb8-5xsfr
11m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled down replica set probe-test-app-fb95466cc to 2 from 3
11m                     Normal    Created                   Pod/probe-test-app-fc7776cb8-5xsfr               Created container probe-test-app
11m                     Normal    Pulled                    Pod/probe-test-app-fc7776cb8-5xsfr               Container image "jasonumiker/probe-test-app:v1" already present on machine
11m                     Normal    Scheduled                 Pod/probe-test-app-fc7776cb8-5xsfr               Successfully assigned default/probe-test-app-fc7776cb8-5xsfr to docker-desktop
11m                     Normal    SuccessfulDelete          ReplicaSet/probe-test-app-fb95466cc              Deleted pod: probe-test-app-fb95466cc-tkq4p
11m                     Normal    Killing                   Pod/probe-test-app-fb95466cc-bbsbp               Stopping container probe-test-app
11m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8              Created pod: probe-test-app-fc7776cb8-zzlg2
11m                     Normal    Created                   Pod/probe-test-app-fc7776cb8-zzlg2               Created container probe-test-app
11m                     Normal    Pulled                    Pod/probe-test-app-fc7776cb8-zzlg2               Container image "jasonumiker/probe-test-app:v1" already present on machine
11m                     Normal    Started                   Pod/probe-test-app-fc7776cb8-5xsfr               Started container probe-test-app
11m                     Normal    Scheduled                 Pod/probe-test-app-fc7776cb8-zzlg2               Successfully assigned default/probe-test-app-fc7776cb8-zzlg2 to docker-desktop
11m                     Normal    SuccessfulDelete          ReplicaSet/probe-test-app-fb95466cc              Deleted pod: probe-test-app-fb95466cc-bbsbp
11m                     Normal    SuccessfulDelete          ReplicaSet/probe-test-app-fb95466cc              Deleted pod: probe-test-app-fb95466cc-zqsg5
11m                     Normal    Started                   Pod/probe-test-app-fc7776cb8-zzlg2               Started container probe-test-app
11m                     Normal    Killing                   Pod/probe-test-app-fb95466cc-zqsg5               Stopping container probe-test-app
11m (x4 over 11m)       Normal    ScalingReplicaSet         Deployment/probe-test-app                        (combined from similar events): Scaled down replica set probe-test-app-fb95466cc to 0 from 1
11m                     Normal    Scheduled                 Pod/pod-with-sidecar                             Successfully assigned default/pod-with-sidecar to docker-desktop
11m                     Normal    Pulling                   Pod/pod-with-sidecar                             Pulling image "alpine:3.20.3"
10m                     Normal    Created                   Pod/pod-with-sidecar                             Created container app-container
10m                     Normal    Pulled                    Pod/pod-with-sidecar                             Successfully pulled image "alpine:3.20.3" in 4.827s (4.827s including waiting). Image size: 8825163 bytes.
10m                     Normal    Started                   Pod/pod-with-sidecar                             Started container app-container
10m                     Normal    Pulling                   Pod/myapp-pod                                    Pulling image "busybox:1.28"
10m                     Normal    Scheduled                 Pod/myapp-pod                                    Successfully assigned default/myapp-pod to docker-desktop
10m                     Warning   Failed                    Pod/pod-with-sidecar                             Failed to pull image "nginx:1.27.2-bookworm": Error response from daemon: Head "https://registry-1.docker.io/v2/library/nginx/manifests/1.27.2-bookworm": proxyconnect tcp: dial tcp 192.168.65.1:3128: i/o timeout
10m                     Warning   Failed                    Pod/pod-with-sidecar                             Error: ErrImagePull
10m                     Normal    BackOff                   Pod/pod-with-sidecar                             Back-off pulling image "nginx:1.27.2-bookworm"
10m                     Warning   Failed                    Pod/pod-with-sidecar                             Error: ImagePullBackOff
10m                     Normal    Created                   Pod/myapp-pod                                    Created container init-myservice
10m                     Normal    Started                   Pod/myapp-pod                                    Started container init-myservice
10m                     Normal    Pulled                    Pod/myapp-pod                                    Successfully pulled image "busybox:1.28" in 5.635s (9.631s including waiting). Image size: 1279385 bytes.
10m                     Normal    Created                   Pod/myapp-pod                                    Created container init-mydb
10m                     Normal    Started                   Pod/myapp-pod                                    Started container init-mydb
10m                     Normal    Pulled                    Pod/myapp-pod                                    Container image "busybox:1.28" already present on machine
10m                     Normal    Created                   Pod/myapp-pod                                    Created container myapp-container
10m                     Normal    Started                   Pod/myapp-pod                                    Started container myapp-container
10m                     Normal    Pulled                    Pod/myapp-pod                                    Container image "busybox:1.28" already present on machine
10m                     Normal    Killing                   Pod/myapp-pod                                    Stopping container myapp-container
10m (x2 over 10m)       Normal    Pulling                   Pod/pod-with-sidecar                             Pulling image "nginx:1.27.2-bookworm"
10m                     Normal    Pulled                    Pod/pod-with-sidecar                             Successfully pulled image "nginx:1.27.2-bookworm" in 6.38s (6.38s including waiting). Image size: 196880043 bytes.
10m                     Normal    Created                   Pod/pod-with-sidecar                             Created container sidecar-container
10m                     Normal    Started                   Pod/pod-with-sidecar                             Started container sidecar-container
9m45s                   Normal    Killing                   Pod/pod-with-sidecar                             Stopping container app-container
9m45s                   Normal    Killing                   Pod/pod-with-sidecar                             Stopping container sidecar-container
9m10s                   Normal    WaitForFirstConsumer      PersistentVolumeClaim/test-pvc                   waiting for first consumer to be created before binding
9m7s                    Normal    Provisioning              PersistentVolumeClaim/test-pvc                   External provisioner is provisioning volume for claim "default/test-pvc"
9m4s (x2 over 9m8s)     Normal    ExternalProvisioning      PersistentVolumeClaim/test-pvc                   Waiting for a volume to be created either by the external provisioner 'microk8s.io/hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
9m2s                    Normal    ProvisioningSucceeded     PersistentVolumeClaim/test-pvc                   Successfully provisioned volume pvc-3afa0558-d64a-4ccb-801c-11316db42eab
9m2s                    Normal    Scheduled                 Pod/nginx                                        Successfully assigned default/nginx to docker-desktop
9m1s                    Normal    Pulling                   Pod/nginx                                        Pulling image "nginx:1.27.2"
8m59s                   Normal    Pulled                    Pod/nginx                                        Successfully pulled image "nginx:1.27.2" in 2.126s (2.126s including waiting). Image size: 196880043 bytes.
8m59s                   Normal    Created                   Pod/nginx                                        Created container nginx
8m59s                   Normal    Started                   Pod/nginx                                        Started container nginx
8m40s                   Normal    Killing                   Pod/nginx                                        Stopping container nginx
8m38s                   Normal    Scheduled                 Pod/nginx                                        Successfully assigned default/nginx to docker-desktop
8m37s                   Normal    Pulled                    Pod/nginx                                        Container image "nginx:1.27.2" already present on machine
8m37s                   Normal    Started                   Pod/nginx                                        Started container nginx
8m37s                   Normal    Created                   Pod/nginx                                        Created container nginx
8m34s                   Normal    Killing                   Pod/nginx                                        Stopping container nginx
8m28s                   Normal    SuccessfulCreate          StatefulSet/rabbitmq                             create Claim rabbitmq-data-rabbitmq-0 Pod rabbitmq-0 in StatefulSet rabbitmq success
8m28s                   Normal    WaitForFirstConsumer      PersistentVolumeClaim/rabbitmq-data-rabbitmq-0   waiting for first consumer to be created before binding
8m28s                   Normal    Provisioning              PersistentVolumeClaim/rabbitmq-data-rabbitmq-0   External provisioner is provisioning volume for claim "default/rabbitmq-data-rabbitmq-0"
8m28s                   Normal    ExternalProvisioning      PersistentVolumeClaim/rabbitmq-data-rabbitmq-0   Waiting for a volume to be created either by the external provisioner 'microk8s.io/hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
8m23s                   Normal    ProvisioningSucceeded     PersistentVolumeClaim/rabbitmq-data-rabbitmq-0   Successfully provisioned volume pvc-13760883-7618-46ce-a70f-ad0cfdc33eb8
8m22s                   Normal    Pulling                   Pod/rabbitmq-0                                   Pulling image "busybox:1.37.0"
8m22s                   Normal    Scheduled                 Pod/rabbitmq-0                                   Successfully assigned default/rabbitmq-0 to docker-desktop
8m17s                   Normal    Created                   Pod/rabbitmq-0                                   Created container rabbitmq-config
8m17s                   Normal    Pulled                    Pod/rabbitmq-0                                   Successfully pulled image "busybox:1.37.0" in 5.026s (5.026s including waiting). Image size: 4042190 bytes.
8m17s                   Normal    Started                   Pod/rabbitmq-0                                   Started container rabbitmq-config
8m17s                   Normal    Pulling                   Pod/rabbitmq-0                                   Pulling image "rabbitmq:3.8.34"
8m10s                   Normal    Pulled                    Pod/rabbitmq-0                                   Successfully pulled image "rabbitmq:3.8.34" in 7.01s (7.01s including waiting). Image size: 185945511 bytes.
8m9s                    Normal    Started                   Pod/rabbitmq-0                                   Started container rabbitmq
8m9s                    Normal    Created                   Pod/rabbitmq-0                                   Created container rabbitmq
<unknown>               Normal    Created                   RabbitMQ/pod/rabbitmq-0                          Node rabbit@rabbitmq-0 is registered
7m16s                   Normal    Killing                   Pod/rabbitmq-0                                   Stopping container rabbitmq
7m15s (x2 over 8m28s)   Normal    SuccessfulCreate          StatefulSet/rabbitmq                             create Pod rabbitmq-0 in StatefulSet rabbitmq successful
7m15s                   Normal    Scheduled                 Pod/rabbitmq-0                                   Successfully assigned default/rabbitmq-0 to docker-desktop
7m14s                   Normal    Created                   Pod/rabbitmq-0                                   Created container rabbitmq
7m14s                   Normal    Pulled                    Pod/rabbitmq-0                                   Container image "busybox:1.37.0" already present on machine
7m14s                   Normal    Created                   Pod/rabbitmq-0                                   Created container rabbitmq-config
7m14s                   Normal    Started                   Pod/rabbitmq-0                                   Started container rabbitmq-config
7m14s                   Normal    Pulled                    Pod/rabbitmq-0                                   Container image "rabbitmq:3.8.34" already present on machine
7m14s                   Normal    Started                   Pod/rabbitmq-0                                   Started container rabbitmq
<unknown>               Normal    Created                   RabbitMQ/pod/rabbitmq-0                          Node rabbit@rabbitmq-0 is registered
7m3s                    Normal    Starting                  Node/docker-desktop                              
6m49s                   Normal    RegisteredNode            Node/docker-desktop                              Node docker-desktop event: Registered Node docker-desktop in Controller
108s                    Normal    SuccessfulCreate          ReplicaSet/generate-load-app                     Created pod: generate-load-app-g2xjf
108s                    Normal    SuccessfulCreate          ReplicaSet/generate-load-app                     Created pod: generate-load-app-r2pn4
108s                    Normal    SuccessfulCreate          ReplicaSet/generate-load-app                     Created pod: generate-load-app-mx67r
108s                    Normal    SuccessfulCreate          ReplicaSet/generate-load-app                     Created pod: generate-load-app-5w2bw
108s                    Normal    SuccessfulCreate          ReplicaSet/generate-load-app                     Created pod: generate-load-app-9g9w4
107s                    Normal    Scheduled                 Pod/generate-load-app-g2xjf                      Successfully assigned default/generate-load-app-g2xjf to docker-desktop
107s                    Normal    Scheduled                 Pod/generate-load-app-9g9w4                      Successfully assigned default/generate-load-app-9g9w4 to docker-desktop
107s                    Normal    Scheduled                 Pod/generate-load-app-5w2bw                      Successfully assigned default/generate-load-app-5w2bw to docker-desktop
107s                    Normal    Scheduled                 Pod/generate-load-app-r2pn4                      Successfully assigned default/generate-load-app-r2pn4 to docker-desktop
107s                    Normal    Scheduled                 Pod/generate-load-app-mx67r                      Successfully assigned default/generate-load-app-mx67r to docker-desktop
107s                    Normal    Started                   Pod/generate-load-app-g2xjf                      Started container generate-load-app
107s                    Normal    Pulled                    Pod/generate-load-app-5w2bw                      Container image "busybox:1.37.0" already present on machine
107s                    Normal    Created                   Pod/generate-load-app-r2pn4                      Created container generate-load-app
107s                    Normal    Created                   Pod/generate-load-app-g2xjf                      Created container generate-load-app
107s                    Normal    Started                   Pod/generate-load-app-mx67r                      Started container generate-load-app
107s                    Normal    Pulled                    Pod/generate-load-app-r2pn4                      Container image "busybox:1.37.0" already present on machine
107s                    Normal    Pulled                    Pod/generate-load-app-mx67r                      Container image "busybox:1.37.0" already present on machine
107s                    Normal    Pulled                    Pod/generate-load-app-g2xjf                      Container image "busybox:1.37.0" already present on machine
107s                    Normal    Started                   Pod/generate-load-app-9g9w4                      Started container generate-load-app
107s                    Normal    Created                   Pod/generate-load-app-9g9w4                      Created container generate-load-app
107s                    Normal    Created                   Pod/generate-load-app-mx67r                      Created container generate-load-app
107s                    Normal    Started                   Pod/generate-load-app-r2pn4                      Started container generate-load-app
107s                    Normal    Created                   Pod/generate-load-app-5w2bw                      Created container generate-load-app
107s                    Normal    Started                   Pod/generate-load-app-5w2bw                      Started container generate-load-app
107s                    Normal    Pulled                    Pod/generate-load-app-9g9w4                      Container image "busybox:1.37.0" already present on machine
76s                     Normal    Killing                   Pod/generate-load-app-g2xjf                      Stopping container generate-load-app
76s                     Normal    Killing                   Pod/generate-load-app-9g9w4                      Stopping container generate-load-app
76s                     Normal    Killing                   Pod/generate-load-app-mx67r                      Stopping container generate-load-app
76s                     Normal    Killing                   Pod/generate-load-app-r2pn4                      Stopping container generate-load-app
76s                     Normal    Killing                   Pod/generate-load-app-5w2bw                      Stopping container generate-load-app
42s                     Normal    ScalingReplicaSet         Deployment/cpu-stressor                          Scaled up replica set cpu-stressor-8556c54f68 to 1
42s                     Normal    SuccessfulCreate          ReplicaSet/cpu-stressor-8556c54f68               Created pod: cpu-stressor-8556c54f68-m5tws
42s                     Normal    Pulling                   Pod/cpu-stressor-8556c54f68-m5tws                Pulling image "narmidm/k8s-pod-cpu-stressor:v1.2.0"
42s                     Normal    Scheduled                 Pod/cpu-stressor-8556c54f68-m5tws                Successfully assigned default/cpu-stressor-8556c54f68-m5tws to docker-desktop
37s                     Normal    Pulled                    Pod/cpu-stressor-8556c54f68-m5tws                Successfully pulled image "narmidm/k8s-pod-cpu-stressor:v1.2.0" in 4.714s (4.714s including waiting). Image size: 9842826 bytes.
37s                     Normal    Created                   Pod/cpu-stressor-8556c54f68-m5tws                Created container cpu-stressor
37s                     Normal    Started                   Pod/cpu-stressor-8556c54f68-m5tws                Started container cpu-stressor
34s                     Normal    Killing                   Pod/cpu-stressor-8556c54f68-m5tws                Stopping container cpu-stressor
32s                     Normal    Scheduled                 Pod/memory-stressor                              Successfully assigned default/memory-stressor to docker-desktop
32s                     Normal    Pulling                   Pod/memory-stressor                              Pulling image "polinux/stress:1.0.4"
28s                     Normal    Pulled                    Pod/memory-stressor                              Successfully pulled image "polinux/stress:1.0.4" in 4.113s (4.113s including waiting). Image size: 9744175 bytes.
28s                     Normal    Pulled                    Pod/memory-stressor                              Container image "polinux/stress:1.0.4" already present on machine
28s (x2 over 28s)       Normal    Started                   Pod/memory-stressor                              Started container memory-stressor
28s (x2 over 28s)       Normal    Created                   Pod/memory-stressor                              Created container memory-stressor
26s (x2 over 27s)       Warning   BackOff                   Pod/memory-stressor                              Back-off restarting failed container memory-stressor in pod memory-stressor_default(aac2c43f-60f2-4dbe-a22e-7586e75fffd2)
5s                      Normal    SuccessfulCreate          ReplicaSet/rabbitmq-consumer-dd9d7cfd4           Created pod: rabbitmq-consumer-dd9d7cfd4-l5bb8
5s                      Normal    ScalingReplicaSet         Deployment/rabbitmq-consumer                     Scaled up replica set rabbitmq-consumer-dd9d7cfd4 to 1
4s                      Normal    Scheduled                 Pod/rabbitmq-consumer-dd9d7cfd4-l5bb8            Successfully assigned default/rabbitmq-consumer-dd9d7cfd4-l5bb8 to docker-desktop
4s                      Normal    Pulling                   Pod/rabbitmq-consumer-dd9d7cfd4-l5bb8            Pulling image "ghcr.io/kedacore/rabbitmq-client:v1.0"
2s                      Normal    Pulling                   Pod/rabbitmq-publish-2f22w                       Pulling image "ghcr.io/kedacore/rabbitmq-client:v1.0"
2s                      Normal    SuccessfulCreate          Job/rabbitmq-publish                             Created pod: rabbitmq-publish-2f22w
2s                      Normal    Started                   Pod/rabbitmq-consumer-dd9d7cfd4-l5bb8            Started container rabbitmq-consumer
2s                      Normal    Created                   Pod/rabbitmq-consumer-dd9d7cfd4-l5bb8            Created container rabbitmq-consumer
2s                      Normal    Pulled                    Pod/rabbitmq-consumer-dd9d7cfd4-l5bb8            Successfully pulled image "ghcr.io/kedacore/rabbitmq-client:v1.0" in 2.322s (2.322s including waiting). Image size: 10241515 bytes.
2s                      Normal    Scheduled                 Pod/rabbitmq-publish-2f22w                       Successfully assigned default/rabbitmq-publish-2f22w to docker-desktop
1s                      Normal    Pulled                    Pod/rabbitmq-publish-2f22w                       Successfully pulled image "ghcr.io/kedacore/rabbitmq-client:v1.0" in 914ms (1.006s including waiting). Image size: 10241515 bytes.
1s                      Normal    Created                   Pod/rabbitmq-publish-2f22w                       Created container rabbitmq-client
1s                      Normal    Started                   Pod/rabbitmq-publish-2f22w                       Started container rabbitmq-client
--------------------
kubectl describe job rabbitmq-publish
Name:             rabbitmq-publish
Namespace:        default
Selector:         batch.kubernetes.io/controller-uid=7694ead5-30d6-4a9a-815a-91e5b2b8fcbd
Labels:           batch.kubernetes.io/controller-uid=7694ead5-30d6-4a9a-815a-91e5b2b8fcbd
                  batch.kubernetes.io/job-name=rabbitmq-publish
                  controller-uid=7694ead5-30d6-4a9a-815a-91e5b2b8fcbd
                  job-name=rabbitmq-publish
Annotations:      <none>
Parallelism:      1
Completions:      1
Completion Mode:  NonIndexed
Suspend:          false
Backoff Limit:    4
Start Time:       Sat, 23 Nov 2024 17:44:43 +1100
Pods Statuses:    1 Active (0 Ready) / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  batch.kubernetes.io/controller-uid=7694ead5-30d6-4a9a-815a-91e5b2b8fcbd
           batch.kubernetes.io/job-name=rabbitmq-publish
           controller-uid=7694ead5-30d6-4a9a-815a-91e5b2b8fcbd
           job-name=rabbitmq-publish
  Containers:
   rabbitmq-client:
    Image:      ghcr.io/kedacore/rabbitmq-client:v1.0
    Port:       <none>
    Host Port:  <none>
    Command:
      send
      $(rabbitmq_host)
      300
    Environment:
      rabbitmq_host:  <set to the key 'host' in secret 'rabbitmq-consumer-secret'>  Optional: false
    Mounts:           <none>
  Volumes:            <none>
  Node-Selectors:     <none>
  Tolerations:        <none>
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  3s    job-controller  Created pod: rabbitmq-publish-2f22w
--------------------
cd ../cronjob
--------------------
kubectl apply -f cronjob.yaml
cronjob.batch/hello created
--------------------
kubectl get pods
NAME                                READY   STATUS      RESTARTS   AGE
hello-28872405-jnlhk                0/1     Completed   0          114s
hello-28872406-tpc8k                0/1     Completed   0          54s
probe-test-app-fc7776cb8-5xsfr      1/1     Running     0          13m
probe-test-app-fc7776cb8-b4r56      1/1     Running     0          118s
probe-test-app-fc7776cb8-b8jzq      1/1     Running     0          118s
probe-test-app-fc7776cb8-rtgdp      1/1     Running     0          13m
probe-test-app-fc7776cb8-zzlg2      1/1     Running     0          13m
rabbitmq-0                          1/1     Running     0          9m24s
rabbitmq-consumer-dd9d7cfd4-29xpt   1/1     Running     0          58s
rabbitmq-consumer-dd9d7cfd4-7fzk6   1/1     Running     0          58s
rabbitmq-consumer-dd9d7cfd4-f4wmw   1/1     Running     0          58s
rabbitmq-consumer-dd9d7cfd4-l5bb8   1/1     Running     0          2m14s
rabbitmq-publish-2f22w              0/1     Completed   0          2m11s
--------------------
kubectl get cronjob
NAME    SCHEDULE    TIMEZONE   SUSPEND   ACTIVE   LAST SCHEDULE   AGE
hello   * * * * *   <none>     False     0        55s             2m7s
--------------------
kubectl delete cronjob hello
cronjob.batch "hello" deleted
--------------------
kubectl get pods -A
NAMESPACE     NAME                                                     READY   STATUS      RESTARTS        AGE
default       probe-test-app-fc7776cb8-5xsfr                           1/1     Running     0               13m
default       probe-test-app-fc7776cb8-b4r56                           1/1     Running     0               2m1s
default       probe-test-app-fc7776cb8-b8jzq                           1/1     Running     0               2m1s
default       probe-test-app-fc7776cb8-rtgdp                           1/1     Running     0               13m
default       probe-test-app-fc7776cb8-zzlg2                           1/1     Running     0               13m
default       rabbitmq-0                                               1/1     Running     0               9m27s
default       rabbitmq-consumer-dd9d7cfd4-29xpt                        1/1     Running     0               61s
default       rabbitmq-consumer-dd9d7cfd4-2lwrg                        1/1     Running     0               1s
default       rabbitmq-consumer-dd9d7cfd4-7fzk6                        1/1     Running     0               61s
default       rabbitmq-consumer-dd9d7cfd4-f4wmw                        1/1     Running     0               61s
default       rabbitmq-consumer-dd9d7cfd4-fq6hw                        1/1     Running     0               1s
default       rabbitmq-consumer-dd9d7cfd4-l5bb8                        1/1     Running     0               2m17s
default       rabbitmq-consumer-dd9d7cfd4-l5mk7                        1/1     Running     0               1s
default       rabbitmq-consumer-dd9d7cfd4-z8hgf                        1/1     Running     0               1s
default       rabbitmq-publish-2f22w                                   0/1     Completed   0               2m14s
keda          keda-admission-webhooks-6b7b75c487-6lgwm                 1/1     Running     0               2m28s
keda          keda-operator-86846bb678-5vzfp                           1/1     Running     1 (2m19s ago)   2m28s
keda          keda-operator-metrics-apiserver-5b677c7769-zlt6r         1/1     Running     0               2m28s
kube-system   coredns-7db6d8ff4d-cvqxt                                 1/1     Running     0               15m
kube-system   coredns-7db6d8ff4d-f4bbp                                 1/1     Running     0               15m
kube-system   etcd-docker-desktop                                      1/1     Running     0               7m14s
kube-system   hostpath-provisioner-6bb9769b5f-tmdh7                    1/1     Running     1 (8m18s ago)   11m
kube-system   kube-apiserver-docker-desktop                            1/1     Running     0               15m
kube-system   kube-controller-manager-docker-desktop                   1/1     Running     0               9m3s
kube-system   kube-proxy-fdl9f                                         1/1     Running     0               9m16s
kube-system   kube-scheduler-docker-desktop                            1/1     Running     1 (8m12s ago)   8m40s
kube-system   storage-provisioner                                      1/1     Running     1 (8m17s ago)   15m
kube-system   vpnkit-controller                                        1/1     Running     0               15m
monitoring    adapter-prometheus-adapter-b84b78594-9j7w5               1/1     Running     0               6m37s
monitoring    alertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running     0               6m32s
monitoring    prometheus-grafana-6b758d7b46-j9tpf                      3/3     Running     0               6m43s
monitoring    prometheus-kube-prometheus-operator-c5f7c5b6-b9lmx       1/1     Running     0               6m43s
monitoring    prometheus-kube-state-metrics-677845d566-l9vqc           1/1     Running     0               6m43s
monitoring    prometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running     0               6m32s
monitoring    prometheus-prometheus-node-exporter-k66tn                1/1     Running     0               6m43s
--------------------
kubectl api-resources
NAME                                SHORTNAMES               APIVERSION                        NAMESPACED   KIND
bindings                                                     v1                                true         Binding
componentstatuses                   cs                       v1                                false        ComponentStatus
configmaps                          cm                       v1                                true         ConfigMap
endpoints                           ep                       v1                                true         Endpoints
events                              ev                       v1                                true         Event
limitranges                         limits                   v1                                true         LimitRange
namespaces                          ns                       v1                                false        Namespace
nodes                               no                       v1                                false        Node
persistentvolumeclaims              pvc                      v1                                true         PersistentVolumeClaim
persistentvolumes                   pv                       v1                                false        PersistentVolume
pods                                po                       v1                                true         Pod
podtemplates                                                 v1                                true         PodTemplate
replicationcontrollers              rc                       v1                                true         ReplicationController
resourcequotas                      quota                    v1                                true         ResourceQuota
secrets                                                      v1                                true         Secret
serviceaccounts                     sa                       v1                                true         ServiceAccount
services                            svc                      v1                                true         Service
mutatingwebhookconfigurations                                admissionregistration.k8s.io/v1   false        MutatingWebhookConfiguration
validatingadmissionpolicies                                  admissionregistration.k8s.io/v1   false        ValidatingAdmissionPolicy
validatingadmissionpolicybindings                            admissionregistration.k8s.io/v1   false        ValidatingAdmissionPolicyBinding
validatingwebhookconfigurations                              admissionregistration.k8s.io/v1   false        ValidatingWebhookConfiguration
customresourcedefinitions           crd,crds                 apiextensions.k8s.io/v1           false        CustomResourceDefinition
apiservices                                                  apiregistration.k8s.io/v1         false        APIService
controllerrevisions                                          apps/v1                           true         ControllerRevision
daemonsets                          ds                       apps/v1                           true         DaemonSet
deployments                         deploy                   apps/v1                           true         Deployment
replicasets                         rs                       apps/v1                           true         ReplicaSet
statefulsets                        sts                      apps/v1                           true         StatefulSet
selfsubjectreviews                                           authentication.k8s.io/v1          false        SelfSubjectReview
tokenreviews                                                 authentication.k8s.io/v1          false        TokenReview
localsubjectaccessreviews                                    authorization.k8s.io/v1           true         LocalSubjectAccessReview
selfsubjectaccessreviews                                     authorization.k8s.io/v1           false        SelfSubjectAccessReview
selfsubjectrulesreviews                                      authorization.k8s.io/v1           false        SelfSubjectRulesReview
subjectaccessreviews                                         authorization.k8s.io/v1           false        SubjectAccessReview
horizontalpodautoscalers            hpa                      autoscaling/v2                    true         HorizontalPodAutoscaler
cronjobs                            cj                       batch/v1                          true         CronJob
jobs                                                         batch/v1                          true         Job
certificatesigningrequests          csr                      certificates.k8s.io/v1            false        CertificateSigningRequest
leases                                                       coordination.k8s.io/v1            true         Lease
endpointslices                                               discovery.k8s.io/v1               true         EndpointSlice
cloudeventsources                                            eventing.keda.sh/v1alpha1         true         CloudEventSource
clustercloudeventsources                                     eventing.keda.sh/v1alpha1         false        ClusterCloudEventSource
events                              ev                       events.k8s.io/v1                  true         Event
flowschemas                                                  flowcontrol.apiserver.k8s.io/v1   false        FlowSchema
prioritylevelconfigurations                                  flowcontrol.apiserver.k8s.io/v1   false        PriorityLevelConfiguration
clustertriggerauthentications       cta,clustertriggerauth   keda.sh/v1alpha1                  false        ClusterTriggerAuthentication
scaledjobs                          sj                       keda.sh/v1alpha1                  true         ScaledJob
scaledobjects                       so                       keda.sh/v1alpha1                  true         ScaledObject
triggerauthentications              ta,triggerauth           keda.sh/v1alpha1                  true         TriggerAuthentication
nodes                                                        metrics.k8s.io/v1beta1            false        NodeMetrics
pods                                                         metrics.k8s.io/v1beta1            true         PodMetrics
alertmanagerconfigs                 amcfg                    monitoring.coreos.com/v1alpha1    true         AlertmanagerConfig
alertmanagers                       am                       monitoring.coreos.com/v1          true         Alertmanager
podmonitors                         pmon                     monitoring.coreos.com/v1          true         PodMonitor
probes                              prb                      monitoring.coreos.com/v1          true         Probe
prometheusagents                    promagent                monitoring.coreos.com/v1alpha1    true         PrometheusAgent
prometheuses                        prom                     monitoring.coreos.com/v1          true         Prometheus
prometheusrules                     promrule                 monitoring.coreos.com/v1          true         PrometheusRule
scrapeconfigs                       scfg                     monitoring.coreos.com/v1alpha1    true         ScrapeConfig
servicemonitors                     smon                     monitoring.coreos.com/v1          true         ServiceMonitor
thanosrulers                        ruler                    monitoring.coreos.com/v1          true         ThanosRuler
ingressclasses                                               networking.k8s.io/v1              false        IngressClass
ingresses                           ing                      networking.k8s.io/v1              true         Ingress
networkpolicies                     netpol                   networking.k8s.io/v1              true         NetworkPolicy
runtimeclasses                                               node.k8s.io/v1                    false        RuntimeClass
poddisruptionbudgets                pdb                      policy/v1                         true         PodDisruptionBudget
clusterrolebindings                                          rbac.authorization.k8s.io/v1      false        ClusterRoleBinding
clusterroles                                                 rbac.authorization.k8s.io/v1      false        ClusterRole
rolebindings                                                 rbac.authorization.k8s.io/v1      true         RoleBinding
roles                                                        rbac.authorization.k8s.io/v1      true         Role
priorityclasses                     pc                       scheduling.k8s.io/v1              false        PriorityClass
csidrivers                                                   storage.k8s.io/v1                 false        CSIDriver
csinodes                                                     storage.k8s.io/v1                 false        CSINode
csistoragecapacities                                         storage.k8s.io/v1                 true         CSIStorageCapacity
storageclasses                      sc                       storage.k8s.io/v1                 false        StorageClass
volumeattachments                                            storage.k8s.io/v1                 false        VolumeAttachment
--------------------
kubectl get clusterrole admin -o yaml
aggregationRule:
  clusterRoleSelectors:
  - matchLabels:
      rbac.authorization.k8s.io/aggregate-to-admin: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  creationTimestamp: "2024-11-23T06:31:35Z"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: admin
  resourceVersion: "276"
  uid: 4483a54f-5b8d-4c0b-bcd7-a60269b82a78
rules:
- apiGroups:
  - ""
  resources:
  - pods/attach
  - pods/exec
  - pods/portforward
  - pods/proxy
  - secrets
  - services/proxy
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - serviceaccounts
  verbs:
  - impersonate
- apiGroups:
  - ""
  resources:
  - pods
  - pods/attach
  - pods/exec
  - pods/portforward
  - pods/proxy
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - ""
  resources:
  - pods/eviction
  verbs:
  - create
- apiGroups:
  - ""
  resources:
  - configmaps
  - events
  - persistentvolumeclaims
  - replicationcontrollers
  - replicationcontrollers/scale
  - secrets
  - serviceaccounts
  - services
  - services/proxy
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - ""
  resources:
  - serviceaccounts/token
  verbs:
  - create
- apiGroups:
  - apps
  resources:
  - daemonsets
  - deployments
  - deployments/rollback
  - deployments/scale
  - replicasets
  - replicasets/scale
  - statefulsets
  - statefulsets/scale
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - autoscaling
  resources:
  - horizontalpodautoscalers
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - batch
  resources:
  - cronjobs
  - jobs
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - extensions
  resources:
  - daemonsets
  - deployments
  - deployments/rollback
  - deployments/scale
  - ingresses
  - networkpolicies
  - replicasets
  - replicasets/scale
  - replicationcontrollers/scale
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  - networkpolicies
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - ""
  resources:
  - configmaps
  - endpoints
  - persistentvolumeclaims
  - persistentvolumeclaims/status
  - pods
  - replicationcontrollers
  - replicationcontrollers/scale
  - serviceaccounts
  - services
  - services/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - bindings
  - events
  - limitranges
  - namespaces/status
  - pods/log
  - pods/status
  - replicationcontrollers/status
  - resourcequotas
  - resourcequotas/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - discovery.k8s.io
  resources:
  - endpointslices
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - apps
  resources:
  - controllerrevisions
  - daemonsets
  - daemonsets/status
  - deployments
  - deployments/scale
  - deployments/status
  - replicasets
  - replicasets/scale
  - replicasets/status
  - statefulsets
  - statefulsets/scale
  - statefulsets/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - autoscaling
  resources:
  - horizontalpodautoscalers
  - horizontalpodautoscalers/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - batch
  resources:
  - cronjobs
  - cronjobs/status
  - jobs
  - jobs/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - extensions
  resources:
  - daemonsets
  - daemonsets/status
  - deployments
  - deployments/scale
  - deployments/status
  - ingresses
  - ingresses/status
  - networkpolicies
  - replicasets
  - replicasets/scale
  - replicasets/status
  - replicationcontrollers/scale
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets
  - poddisruptionbudgets/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  - ingresses/status
  - networkpolicies
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - authorization.k8s.io
  resources:
  - localsubjectaccessreviews
  verbs:
  - create
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  - rolebindings
  - roles
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch
--------------------
kubectl get clusterrole admin -o yaml | wc -l
     315
--------------------
cd ../k8s-authz
--------------------
./setup-tokens-on-cluster.sh
--------------------
./add-users-kubeconfig.sh
Context "docker-desktop-jane" created.
Context "docker-desktop-john" created.
--------------------
cat team1.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: "team1"
  labels:
    name: "team1"
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: admin
  namespace: team1
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["*"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: admin
  namespace: team1
subjects:
- kind: User
  name: jane
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: admin
  apiGroup: rbac.authorization.k8s.io--------------------
kubectl apply -f team1.yaml && kubectl apply -f team2.yaml
namespace/team1 created
role.rbac.authorization.k8s.io/admin created
rolebinding.rbac.authorization.k8s.io/admin created
namespace/team2 created
role.rbac.authorization.k8s.io/admin created
rolebinding.rbac.authorization.k8s.io/admin created
--------------------
kubectl config get-contexts
CURRENT   NAME                  CLUSTER          AUTHINFO         NAMESPACE
*         docker-desktop        docker-desktop   docker-desktop   
          docker-desktop-jane   docker-desktop   jane             team1
          docker-desktop-john   docker-desktop   john             team2
--------------------
kubectl config use-context docker-desktop-jane
Switched to context "docker-desktop-jane".
--------------------
kubectl get pods -A
Error from server (Forbidden): pods is forbidden: User "jane" cannot list resource "pods" in API group "" at the cluster scope
--------------------
kubectl get pods
No resources found in team1 namespace.
--------------------
kubectl config use-context docker-desktop-john
Switched to context "docker-desktop-john".
--------------------
kubectl get pods
No resources found in team2 namespace.
--------------------
kubectl get pods --namespace=team1
Error from server (Forbidden): pods is forbidden: User "john" cannot list resource "pods" in API group "" in the namespace "team1"
--------------------
kubectl config use-context docker-desktop
Switched to context "docker-desktop".
--------------------
Cleaning up Jane and John...
deleted user jane from /Users/jumiker/.kube/config
deleted user john from /Users/jumiker/.kube/config
deleted context docker-desktop-jane from /Users/jumiker/.kube/config
deleted context docker-desktop-john from /Users/jumiker/.kube/config
--------------------
cd ../ingress
--------------------
./install-nginx.sh
"ingress-nginx" already exists with the same configuration, skipping
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "kedacore" chart repository
...Successfully got an update from the "ingress-nginx" chart repository
...Successfully got an update from the "kiali" chart repository
...Successfully got an update from the "prometheus-community" chart repository
...Successfully got an update from the "istio" chart repository
Update Complete. ⎈Happy Helming!⎈
NAME: ingress
LAST DEPLOYED: Sat Nov 23 17:47:58 2024
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
The ingress-nginx controller has been installed.
It may take a few minutes for the load balancer IP to be available.
You can watch the status by running 'kubectl get service --namespace default ingress-ingress-nginx-controller --output wide --watch'

An example Ingress that makes use of the controller:
  apiVersion: networking.k8s.io/v1
  kind: Ingress
  metadata:
    name: example
    namespace: foo
  spec:
    ingressClassName: nginx
    rules:
      - host: www.example.com
        http:
          paths:
            - pathType: Prefix
              backend:
                service:
                  name: exampleService
                  port:
                    number: 80
              path: /
    # This section is only required if TLS is to be enabled for the Ingress
    tls:
      - hosts:
        - www.example.com
        secretName: example-tls

If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided:

  apiVersion: v1
  kind: Secret
  metadata:
    name: example-tls
    namespace: foo
  data:
    tls.crt: <base64 encoded cert>
    tls.key: <base64 encoded key>
  type: kubernetes.io/tls
--------------------
kubectl apply -f probe-test-app-ingress.yaml
ingress.networking.k8s.io/probe-test-app created
--------------------
curl http://localhost
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100   577  100   577    0     0   113k      0 --:--:-- --:--:-- --:--:--  140k
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>probetest</title>
</head>
<body>
    <p>This Flask app is served from probe-test-app-fc7776cb8-b4r56</p>
    <p>The readiness endpoint (/readyz) is Healthy</p>
    <p>The liveness endpoint is (/livez) Healthy</p>
    <form method="POST">
        <button type="submit" name="readyz">Toggle Readiness</button>
        <button type="submit" name="livez">Toggle Liveness</button>
    </form>
    <p>Version 1</p>
</body>
</html>--------------------
kubectl apply -f nyancat.yaml
deployment.apps/nyancat created
service/nyancat created
--------------------
kubectl rollout status deployment nyancat -n default
Waiting for deployment "nyancat" rollout to finish: 0 of 1 updated replicas are available...
deployment "nyancat" successfully rolled out
--------------------
kubectl apply -f nyancat-ingress.yaml
ingress.networking.k8s.io/probe-test-app configured
--------------------
curl http://localhost/nyancat/
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100   207  100   207    0     0  25580      0 --:--:-- --:--:-- --:--:-- 25875
<!doctype html>
<html lang=en>
<title>404 Not Found</title>
<h1>Not Found</h1>
<p>The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.</p>
--------------------
kubectl delete ingress probe-test-app
ingress.networking.k8s.io "probe-test-app" deleted
--------------------
helm uninstall ingress
release "ingress" uninstalled
--------------------
Cleaning up probe-test-app and nyancat...
horizontalpodautoscaler.autoscaling "probe-test-app" deleted
deployment.apps "probe-test-app" deleted
deployment.apps "nyancat" deleted
service "probe-test-app" deleted
service "nyancat" deleted
--------------------
cd ../istio
--------------------
./install-istio.sh
"istio" already exists with the same configuration, skipping
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "kedacore" chart repository
...Successfully got an update from the "ingress-nginx" chart repository
...Successfully got an update from the "kiali" chart repository
...Successfully got an update from the "prometheus-community" chart repository
...Successfully got an update from the "istio" chart repository
Update Complete. ⎈Happy Helming!⎈
namespace/istio-system created
NAME: istio-base
LAST DEPLOYED: Sat Nov 23 17:49:49 2024
NAMESPACE: istio-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Istio base successfully installed!

To learn more about the release, try:
  $ helm status istio-base -n istio-system
  $ helm get all istio-base -n istio-system
NAME: istiod
LAST DEPLOYED: Sat Nov 23 17:49:50 2024
NAMESPACE: istio-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
"istiod" successfully installed!

To learn more about the release, try:
  $ helm status istiod -n istio-system
  $ helm get all istiod -n istio-system

Next steps:
  * Deploy a Gateway: https://istio.io/latest/docs/setup/additional-setup/gateway/
  * Try out our tasks to get started on common configurations:
    * https://istio.io/latest/docs/tasks/traffic-management
    * https://istio.io/latest/docs/tasks/security/
    * https://istio.io/latest/docs/tasks/policy-enforcement/
  * Review the list of actively supported releases, CVE publications and our hardening guide:
    * https://istio.io/latest/docs/releases/supported-releases/
    * https://istio.io/latest/news/security/
    * https://istio.io/latest/docs/ops/best-practices/security/

For further documentation see https://istio.io website
"kiali" already exists with the same configuration, skipping
NAME: kiali-server
LAST DEPLOYED: Sat Nov 23 17:49:59 2024
NAMESPACE: istio-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Welcome to Kiali! For more details on Kiali, see: https://kiali.io

The Kiali Server [v2.1.0] has been installed in namespace [istio-system]. It will be ready soon.

When installing with "deployment.cluster_wide_access=false" using this Kiali Server Helm Chart,
it is your responsibility to manually create the proper Roles and RoleBindings for the Kiali Server
to have the correct permissions to access the service mesh namespaces.

(Helm: Chart=[kiali-server], Release=[kiali-server], Version=[2.1.0])
servicemonitor.monitoring.coreos.com/prometheus-oper-istio-controlplane created
servicemonitor.monitoring.coreos.com/prometheus-oper-istio-dataplane created
customresourcedefinition.apiextensions.k8s.io/gatewayclasses.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/gateways.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/grpcroutes.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/httproutes.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/referencegrants.gateway.networking.k8s.io created
--------------------
kubectl label namespace default istio-injection=enabled
namespace/default labeled
--------------------
kubectl apply -f bookinfo/platform/kube/bookinfo.yaml
service/details created
serviceaccount/bookinfo-details created
deployment.apps/details-v1 created
service/ratings created
serviceaccount/bookinfo-ratings created
deployment.apps/ratings-v1 created
service/reviews created
serviceaccount/bookinfo-reviews created
deployment.apps/reviews-v1 created
deployment.apps/reviews-v2 created
deployment.apps/reviews-v3 created
service/productpage created
serviceaccount/bookinfo-productpage created
deployment.apps/productpage-v1 created
--------------------
kubectl apply -f bookinfo/gateway-api/bookinfo-gateway.yaml
gateway.gateway.networking.k8s.io/bookinfo-gateway created
httproute.gateway.networking.k8s.io/bookinfo created
--------------------
kubectl apply -f bookinfo/platform/kube/bookinfo-versions.yaml
service/reviews-v1 created
service/reviews-v2 created
service/reviews-v3 created
service/productpage-v1 created
service/ratings-v1 created
service/details-v1 created
--------------------
kubectl apply -f bookinfo/gateway-api/route-all-v1.yaml
httproute.gateway.networking.k8s.io/reviews created
httproute.gateway.networking.k8s.io/productpage created
httproute.gateway.networking.k8s.io/ratings created
httproute.gateway.networking.k8s.io/details created
--------------------
kubectl apply -f bookinfo/gateway-api/route-reviews-90-10.yaml
httproute.gateway.networking.k8s.io/reviews configured
--------------------
kubectl apply -f bookinfo/gateway-api/route-jason-v2.yaml
httproute.gateway.networking.k8s.io/reviews configured
--------------------
bookinfo/platform/kube/cleanup.sh
using NAMESPACE=default
gateway.gateway.networking.k8s.io "bookinfo-gateway" deleted
httproute.gateway.networking.k8s.io "bookinfo" deleted
httproute.gateway.networking.k8s.io "details" deleted
httproute.gateway.networking.k8s.io "productpage" deleted
httproute.gateway.networking.k8s.io "ratings" deleted
httproute.gateway.networking.k8s.io "reviews" deleted
Application cleanup may take up to one minute
service "details" deleted
serviceaccount "bookinfo-details" deleted
deployment.apps "details-v1" deleted
service "ratings" deleted
serviceaccount "bookinfo-ratings" deleted
deployment.apps "ratings-v1" deleted
service "reviews" deleted
serviceaccount "bookinfo-reviews" deleted
deployment.apps "reviews-v1" deleted
deployment.apps "reviews-v2" deleted
deployment.apps "reviews-v3" deleted
service "productpage" deleted
serviceaccount "bookinfo-productpage" deleted
deployment.apps "productpage-v1" deleted
Application cleanup successful
--------------------
cd ../kustomise
--------------------kustomize build prod
apiVersion: v1
kind: Service
metadata:
  labels:
    run: my-nginx
  name: prod-my-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
  selector:
    run: my-nginx
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prod-my-nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      run: my-nginx
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - image: nginx
        name: my-nginx
--------------------
kubectl apply -k prod
service/prod-my-nginx created
deployment.apps/prod-my-nginx created
--------------------
kubectl get pods
NAME                             READY   STATUS            RESTARTS   AGE
prod-my-nginx-554c7b49dd-769vp   0/2     PodInitializing   0          1s
prod-my-nginx-554c7b49dd-k482m   0/2     PodInitializing   0          1s
rabbitmq-0                       1/1     Running           0          22m
rabbitmq-publish-2f22w           0/1     Completed         0          15m
--------------------
kubectl apply -k dev
service/dev-my-nginx created
deployment.apps/dev-my-nginx created
--------------------
kubectl get pods
NAME                             READY   STATUS            RESTARTS   AGE
dev-my-nginx-554c7b49dd-g7mz9    0/2     PodInitializing   0          1s
prod-my-nginx-554c7b49dd-769vp   0/2     PodInitializing   0          3s
prod-my-nginx-554c7b49dd-k482m   0/2     PodInitializing   0          3s
rabbitmq-0                       1/1     Running           0          22m
rabbitmq-publish-2f22w           0/1     Completed         0          15m
--------------------
Cleaning up Kustomization example...
service "prod-my-nginx" deleted
deployment.apps "prod-my-nginx" deleted
service "dev-my-nginx" deleted
deployment.apps "dev-my-nginx" deleted
--------------------
helm ls -A
NAME        	NAMESPACE   	REVISION	UPDATED                              	STATUS  	CHART                       	APP VERSION
adapter     	monitoring  	1       	2024-11-23 17:40:20.697313 +1100 AEDT	deployed	prometheus-adapter-4.11.0   	v0.12.0    
istio-base  	istio-system	1       	2024-11-23 17:49:49.81976 +1100 AEDT 	deployed	base-1.24.0                 	1.24.0     
istiod      	istio-system	1       	2024-11-23 17:49:50.954797 +1100 AEDT	deployed	istiod-1.24.0               	1.24.0     
keda        	keda        	1       	2024-11-23 17:44:28.623041 +1100 AEDT	deployed	keda-2.16.0                 	2.16.0     
kiali-server	istio-system	1       	2024-11-23 17:49:59.817847 +1100 AEDT	deployed	kiali-server-2.1.0          	v2.1.0     
prometheus  	monitoring  	1       	2024-11-23 17:39:52.429768 +1100 AEDT	deployed	kube-prometheus-stack-65.8.1	v0.77.2    
--------------------
Installing required CRD updates for prometheus chart upgrade from 65 to 66...
error: Apply failed with 2 conflicts: conflicts with "helm" using apiextensions.k8s.io/v1:
- .metadata.annotations.controller-gen.kubebuilder.io/version
- .metadata.annotations.operator.prometheus.io/version
Please review the fields above--they currently have other managers. Here
are the ways you can resolve this warning:
* If you intend to manage all of these fields, please re-run the apply
  command with the `--force-conflicts` flag.
* If you do not intend to manage all of the fields, please edit your
  manifest to remove references to the fields that should keep their
  current managers.
* You may co-own fields by updating your manifest to match the existing
  value; in this case, you'll become the manager if the other manager(s)
  stop managing the field (remove it from their configuration).
See https://kubernetes.io/docs/reference/using-api/server-side-apply/#conflicts
error: Apply failed with 3 conflicts: conflicts with "helm" using apiextensions.k8s.io/v1:
- .metadata.annotations.controller-gen.kubebuilder.io/version
- .metadata.annotations.operator.prometheus.io/version
- .spec.versions
Please review the fields above--they currently have other managers. Here
are the ways you can resolve this warning:
* If you intend to manage all of these fields, please re-run the apply
  command with the `--force-conflicts` flag.
* If you do not intend to manage all of the fields, please edit your
  manifest to remove references to the fields that should keep their
  current managers.
* You may co-own fields by updating your manifest to match the existing
  value; in this case, you'll become the manager if the other manager(s)
  stop managing the field (remove it from their configuration).
See https://kubernetes.io/docs/reference/using-api/server-side-apply/#conflicts
error: Apply failed with 3 conflicts: conflicts with "helm" using apiextensions.k8s.io/v1:
- .metadata.annotations.controller-gen.kubebuilder.io/version
- .metadata.annotations.operator.prometheus.io/version
- .spec.versions
Please review the fields above--they currently have other managers. Here
are the ways you can resolve this warning:
* If you intend to manage all of these fields, please re-run the apply
  command with the `--force-conflicts` flag.
* If you do not intend to manage all of the fields, please edit your
  manifest to remove references to the fields that should keep their
  current managers.
* You may co-own fields by updating your manifest to match the existing
  value; in this case, you'll become the manager if the other manager(s)
  stop managing the field (remove it from their configuration).
See https://kubernetes.io/docs/reference/using-api/server-side-apply/#conflicts
error: Apply failed with 3 conflicts: conflicts with "helm" using apiextensions.k8s.io/v1:
- .metadata.annotations.controller-gen.kubebuilder.io/version
- .metadata.annotations.operator.prometheus.io/version
- .spec.versions
Please review the fields above--they currently have other managers. Here
are the ways you can resolve this warning:
* If you intend to manage all of these fields, please re-run the apply
  command with the `--force-conflicts` flag.
* If you do not intend to manage all of the fields, please edit your
  manifest to remove references to the fields that should keep their
  current managers.
* You may co-own fields by updating your manifest to match the existing
  value; in this case, you'll become the manager if the other manager(s)
  stop managing the field (remove it from their configuration).
See https://kubernetes.io/docs/reference/using-api/server-side-apply/#conflicts
error: Apply failed with 3 conflicts: conflicts with "helm" using apiextensions.k8s.io/v1:
- .metadata.annotations.controller-gen.kubebuilder.io/version
- .metadata.annotations.operator.prometheus.io/version
- .spec.versions
Please review the fields above--they currently have other managers. Here
are the ways you can resolve this warning:
* If you intend to manage all of these fields, please re-run the apply
  command with the `--force-conflicts` flag.
* If you do not intend to manage all of the fields, please edit your
  manifest to remove references to the fields that should keep their
  current managers.
* You may co-own fields by updating your manifest to match the existing
  value; in this case, you'll become the manager if the other manager(s)
  stop managing the field (remove it from their configuration).
See https://kubernetes.io/docs/reference/using-api/server-side-apply/#conflicts
error: Apply failed with 3 conflicts: conflicts with "helm" using apiextensions.k8s.io/v1:
- .metadata.annotations.controller-gen.kubebuilder.io/version
- .metadata.annotations.operator.prometheus.io/version
- .spec.versions
Please review the fields above--they currently have other managers. Here
are the ways you can resolve this warning:
* If you intend to manage all of these fields, please re-run the apply
  command with the `--force-conflicts` flag.
* If you do not intend to manage all of the fields, please edit your
  manifest to remove references to the fields that should keep their
  current managers.
* You may co-own fields by updating your manifest to match the existing
  value; in this case, you'll become the manager if the other manager(s)
  stop managing the field (remove it from their configuration).
See https://kubernetes.io/docs/reference/using-api/server-side-apply/#conflicts
error: Apply failed with 3 conflicts: conflicts with "helm" using apiextensions.k8s.io/v1:
- .metadata.annotations.controller-gen.kubebuilder.io/version
- .metadata.annotations.operator.prometheus.io/version
- .spec.versions
Please review the fields above--they currently have other managers. Here
are the ways you can resolve this warning:
* If you intend to manage all of these fields, please re-run the apply
  command with the `--force-conflicts` flag.
* If you do not intend to manage all of the fields, please edit your
  manifest to remove references to the fields that should keep their
  current managers.
* You may co-own fields by updating your manifest to match the existing
  value; in this case, you'll become the manager if the other manager(s)
  stop managing the field (remove it from their configuration).
See https://kubernetes.io/docs/reference/using-api/server-side-apply/#conflicts
error: Apply failed with 3 conflicts: conflicts with "helm" using apiextensions.k8s.io/v1:
- .metadata.annotations.controller-gen.kubebuilder.io/version
- .metadata.annotations.operator.prometheus.io/version
- .spec.versions
Please review the fields above--they currently have other managers. Here
are the ways you can resolve this warning:
* If you intend to manage all of these fields, please re-run the apply
  command with the `--force-conflicts` flag.
* If you do not intend to manage all of the fields, please edit your
  manifest to remove references to the fields that should keep their
  current managers.
* You may co-own fields by updating your manifest to match the existing
  value; in this case, you'll become the manager if the other manager(s)
  stop managing the field (remove it from their configuration).
See https://kubernetes.io/docs/reference/using-api/server-side-apply/#conflicts
error: Apply failed with 3 conflicts: conflicts with "helm" using apiextensions.k8s.io/v1:
- .metadata.annotations.controller-gen.kubebuilder.io/version
- .metadata.annotations.operator.prometheus.io/version
- .spec.versions
Please review the fields above--they currently have other managers. Here
are the ways you can resolve this warning:
* If you intend to manage all of these fields, please re-run the apply
  command with the `--force-conflicts` flag.
* If you do not intend to manage all of the fields, please edit your
  manifest to remove references to the fields that should keep their
  current managers.
* You may co-own fields by updating your manifest to match the existing
  value; in this case, you'll become the manager if the other manager(s)
  stop managing the field (remove it from their configuration).
See https://kubernetes.io/docs/reference/using-api/server-side-apply/#conflicts
error: Apply failed with 3 conflicts: conflicts with "helm" using apiextensions.k8s.io/v1:
- .metadata.annotations.controller-gen.kubebuilder.io/version
- .metadata.annotations.operator.prometheus.io/version
- .spec.versions
Please review the fields above--they currently have other managers. Here
are the ways you can resolve this warning:
* If you intend to manage all of these fields, please re-run the apply
  command with the `--force-conflicts` flag.
* If you do not intend to manage all of the fields, please edit your
  manifest to remove references to the fields that should keep their
  current managers.
* You may co-own fields by updating your manifest to match the existing
  value; in this case, you'll become the manager if the other manager(s)
  stop managing the field (remove it from their configuration).
See https://kubernetes.io/docs/reference/using-api/server-side-apply/#conflicts
--------------------
helm upgrade prometheus prometheus-community/kube-prometheus-stack --version 66.2.1 -n monitoring
Release "prometheus" has been upgraded. Happy Helming!
NAME: prometheus
LAST DEPLOYED: Sat Nov 23 18:02:55 2024
NAMESPACE: monitoring
STATUS: deployed
REVISION: 2
NOTES:
kube-prometheus-stack has been installed. Check its status by running:
  kubectl --namespace monitoring get pods -l "release=prometheus"

Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create & configure Alertmanager and Prometheus instances using the Operator.
--------------------
helm get values prometheus -n monitoring
USER-SUPPLIED VALUES:
grafana:
  service:
    port: 3000
    type: LoadBalancer
kubelet:
  serviceMonitor:
    cAdvisorMetricRelabelings: null
prometheus:
  service:
    type: LoadBalancer
prometheus-node-exporter:
  hostRootFsMount:
    enabled: false
  prometheus:
    monitor:
      attachMetadata:
        node: true
      relabelings:
      - action: replace
        regex: (.+)
        replacement: ${1}
        sourceLabels:
        - __meta_kubernetes_endpoint_node_name
        targetLabel: node
