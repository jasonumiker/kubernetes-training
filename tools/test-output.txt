kubectl config get-contexts
CURRENT   NAME             CLUSTER          AUTHINFO         NAMESPACE
*         docker-desktop   docker-desktop   docker-desktop   
--------------------
kubectl get nodes
NAME             STATUS   ROLES           AGE   VERSION
docker-desktop   Ready    control-plane   81s   v1.30.2
--------------------
cd probe-test-app
--------------------
kubectl apply -f probe-test-app-pod.yaml
pod/probe-test-app created
pod/probe-test-app condition met
--------------------
kubectl get pods -o wide
NAME             READY   STATUS    RESTARTS   AGE   IP         NODE             NOMINATED NODE   READINESS GATES
probe-test-app   1/1     Running   0          17s   10.1.0.6   docker-desktop   <none>           <none>
--------------------
kubectl apply -f probe-test-app-service.yaml
service/probe-test-app created
--------------------
kubectl get services -o wide
NAME             TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE    SELECTOR
kubernetes       ClusterIP      10.96.0.1        <none>        443/TCP          101s   <none>
probe-test-app   LoadBalancer   10.108.128.222   localhost     8000:30912/TCP   1s     app.kubernetes.io/name=probe-test-app
--------------------
kubectl get endpoints
NAME             ENDPOINTS           AGE
kubernetes       192.168.65.3:6443   102s
probe-test-app   10.1.0.6:8080       2s
--------------------
kubectl apply -f probe-test-app-pod-2.yaml
pod/probe-test-app-2 created
pod/probe-test-app-2 condition met
--------------------
kubectl get endpoints
NAME             ENDPOINTS                     AGE
kubernetes       192.168.65.3:6443             106s
probe-test-app   10.1.0.6:8080,10.1.0.7:8080   6s
--------------------
kubectl delete pods --all
pod "probe-test-app" deleted
pod "probe-test-app-2" deleted
--------------------
kubectl apply -f probe-test-app-replicaset.yaml
replicaset.apps/probe-test-app created
pod/probe-test-app-86nv4 condition met
pod/probe-test-app-kqx8f condition met
pod/probe-test-app-zhcz8 condition met
--------------------
kubectl scale replicaset probe-test-app --replicas=2
replicaset.apps/probe-test-app scaled
--------------------
kubectl get pods
NAME                   READY   STATUS        RESTARTS   AGE
probe-test-app-86nv4   1/1     Running       0          4s
probe-test-app-kqx8f   1/1     Terminating   0          4s
probe-test-app-zhcz8   1/1     Running       0          4s
--------------------
kubectl delete replicaset probe-test-app
replicaset.apps "probe-test-app" deleted
--------------------
kubectl apply -f probe-test-app-deployment.yaml
deployment.apps/probe-test-app created
Waiting for deployment "probe-test-app" rollout to finish: 0 of 3 updated replicas are available...
Waiting for deployment "probe-test-app" rollout to finish: 1 of 3 updated replicas are available...
Waiting for deployment "probe-test-app" rollout to finish: 2 of 3 updated replicas are available...
deployment "probe-test-app" successfully rolled out
--------------------
kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
probe-test-app-fc7776cb8-6tvc9   1/1     Running   0          2s
probe-test-app-fc7776cb8-bdw27   1/1     Running   0          2s
probe-test-app-fc7776cb8-n9l4x   1/1     Running   0          2s
--------------------
kubectl get replicasets
NAME                       DESIRED   CURRENT   READY   AGE
probe-test-app-fc7776cb8   3         3         3       4s
--------------------
kubectl set image deployment/probe-test-app probe-test-app=jasonumiker/probe-test-app:v2
deployment.apps/probe-test-app image updated
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "probe-test-app" rollout to finish: 1 old replicas are pending termination...
deployment "probe-test-app" successfully rolled out
--------------------
kubectl events
LAST SEEN               TYPE      REASON                    OBJECT                                MESSAGE
2m12s                   Normal    Starting                  Node/docker-desktop                   Starting kubelet.
2m12s                   Warning   InvalidDiskCapacity       Node/docker-desktop                   invalid capacity 0 on image filesystem
2m12s (x8 over 2m12s)   Normal    NodeHasSufficientMemory   Node/docker-desktop                   Node docker-desktop status is now: NodeHasSufficientMemory
2m12s (x7 over 2m12s)   Normal    NodeHasNoDiskPressure     Node/docker-desktop                   Node docker-desktop status is now: NodeHasNoDiskPressure
2m12s (x7 over 2m12s)   Normal    NodeHasSufficientPID      Node/docker-desktop                   Node docker-desktop status is now: NodeHasSufficientPID
2m12s                   Normal    NodeAllocatableEnforced   Node/docker-desktop                   Updated Node Allocatable limit across pods
2m4s                    Normal    RegisteredNode            Node/docker-desktop                   Node docker-desktop event: Registered Node docker-desktop in Controller
2m2s                    Normal    Starting                  Node/docker-desktop                   
48s                     Normal    Scheduled                 Pod/probe-test-app                    Successfully assigned default/probe-test-app to docker-desktop
47s                     Normal    Pulling                   Pod/probe-test-app                    Pulling image "jasonumiker/probe-test-app:v1"
33s                     Normal    Started                   Pod/probe-test-app                    Started container probe-test-app
33s                     Normal    Pulled                    Pod/probe-test-app                    Successfully pulled image "jasonumiker/probe-test-app:v1" in 13.964s (13.964s including waiting). Image size: 1025410707 bytes.
33s                     Normal    Created                   Pod/probe-test-app                    Created container probe-test-app
26s                     Normal    Pulled                    Pod/probe-test-app-2                  Container image "jasonumiker/probe-test-app:v1" already present on machine
26s                     Normal    Scheduled                 Pod/probe-test-app-2                  Successfully assigned default/probe-test-app-2 to docker-desktop
26s                     Normal    Created                   Pod/probe-test-app-2                  Created container probe-test-app
26s                     Normal    Started                   Pod/probe-test-app-2                  Started container probe-test-app
26s                     Warning   Unhealthy                 Pod/probe-test-app-2                  Readiness probe failed: Get "http://10.1.0.7:8080/readyz": dial tcp 10.1.0.7:8080: connect: connection refused
23s                     Normal    Killing                   Pod/probe-test-app                    Stopping container probe-test-app
23s                     Normal    Killing                   Pod/probe-test-app-2                  Stopping container probe-test-app
21s                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app             Created pod: probe-test-app-86nv4
21s                     Normal    Scheduled                 Pod/probe-test-app-kqx8f              Successfully assigned default/probe-test-app-kqx8f to docker-desktop
21s                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app             Created pod: probe-test-app-kqx8f
21s                     Normal    Scheduled                 Pod/probe-test-app-86nv4              Successfully assigned default/probe-test-app-86nv4 to docker-desktop
21s                     Normal    Scheduled                 Pod/probe-test-app-zhcz8              Successfully assigned default/probe-test-app-zhcz8 to docker-desktop
21s                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app             Created pod: probe-test-app-zhcz8
20s                     Normal    Started                   Pod/probe-test-app-86nv4              Started container probe-test-app
20s                     Normal    Created                   Pod/probe-test-app-86nv4              Created container probe-test-app
20s                     Normal    Pulled                    Pod/probe-test-app-86nv4              Container image "jasonumiker/probe-test-app:v1" already present on machine
20s                     Normal    Pulled                    Pod/probe-test-app-kqx8f              Container image "jasonumiker/probe-test-app:v1" already present on machine
20s                     Normal    Started                   Pod/probe-test-app-zhcz8              Started container probe-test-app
20s                     Normal    Created                   Pod/probe-test-app-zhcz8              Created container probe-test-app
20s                     Normal    Pulled                    Pod/probe-test-app-zhcz8              Container image "jasonumiker/probe-test-app:v1" already present on machine
20s                     Normal    Started                   Pod/probe-test-app-kqx8f              Started container probe-test-app
20s                     Normal    Created                   Pod/probe-test-app-kqx8f              Created container probe-test-app
18s                     Normal    SuccessfulDelete          ReplicaSet/probe-test-app             Deleted pod: probe-test-app-kqx8f
17s                     Normal    Killing                   Pod/probe-test-app-kqx8f              Stopping container probe-test-app
16s                     Normal    Killing                   Pod/probe-test-app-86nv4              Stopping container probe-test-app
16s                     Normal    Killing                   Pod/probe-test-app-zhcz8              Stopping container probe-test-app
15s                     Normal    Pulled                    Pod/probe-test-app-fc7776cb8-bdw27    Container image "jasonumiker/probe-test-app:v1" already present on machine
15s                     Normal    Started                   Pod/probe-test-app-fc7776cb8-6tvc9    Started container probe-test-app
15s                     Normal    ScalingReplicaSet         Deployment/probe-test-app             Scaled up replica set probe-test-app-fc7776cb8 to 3
15s                     Normal    Created                   Pod/probe-test-app-fc7776cb8-bdw27    Created container probe-test-app
15s                     Normal    Started                   Pod/probe-test-app-fc7776cb8-bdw27    Started container probe-test-app
15s                     Normal    Scheduled                 Pod/probe-test-app-fc7776cb8-bdw27    Successfully assigned default/probe-test-app-fc7776cb8-bdw27 to docker-desktop
15s                     Normal    Scheduled                 Pod/probe-test-app-fc7776cb8-n9l4x    Successfully assigned default/probe-test-app-fc7776cb8-n9l4x to docker-desktop
15s                     Normal    Pulled                    Pod/probe-test-app-fc7776cb8-n9l4x    Container image "jasonumiker/probe-test-app:v1" already present on machine
15s                     Normal    Created                   Pod/probe-test-app-fc7776cb8-n9l4x    Created container probe-test-app
15s                     Normal    Started                   Pod/probe-test-app-fc7776cb8-n9l4x    Started container probe-test-app
15s                     Normal    Scheduled                 Pod/probe-test-app-fc7776cb8-6tvc9    Successfully assigned default/probe-test-app-fc7776cb8-6tvc9 to docker-desktop
15s                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8   Created pod: probe-test-app-fc7776cb8-n9l4x
15s                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8   Created pod: probe-test-app-fc7776cb8-bdw27
15s                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8   Created pod: probe-test-app-fc7776cb8-6tvc9
15s                     Normal    Pulled                    Pod/probe-test-app-fc7776cb8-6tvc9    Container image "jasonumiker/probe-test-app:v1" already present on machine
15s                     Normal    Created                   Pod/probe-test-app-fc7776cb8-6tvc9    Created container probe-test-app
10s                     Normal    Pulling                   Pod/probe-test-app-fb95466cc-64p2h    Pulling image "jasonumiker/probe-test-app:v2"
10s                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fb95466cc   Created pod: probe-test-app-fb95466cc-64p2h
10s                     Normal    ScalingReplicaSet         Deployment/probe-test-app             Scaled up replica set probe-test-app-fb95466cc to 1
10s                     Normal    Scheduled                 Pod/probe-test-app-fb95466cc-64p2h    Successfully assigned default/probe-test-app-fb95466cc-64p2h to docker-desktop
5s                      Normal    Created                   Pod/probe-test-app-fb95466cc-64p2h    Created container probe-test-app
5s                      Normal    Started                   Pod/probe-test-app-fb95466cc-64p2h    Started container probe-test-app
5s                      Normal    Pulled                    Pod/probe-test-app-fb95466cc-64p2h    Successfully pulled image "jasonumiker/probe-test-app:v2" in 4.537s (4.537s including waiting). Image size: 1025410707 bytes.
4s                      Normal    SuccessfulDelete          ReplicaSet/probe-test-app-fc7776cb8   Deleted pod: probe-test-app-fc7776cb8-6tvc9
4s                      Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fb95466cc   Created pod: probe-test-app-fb95466cc-cms54
4s                      Normal    ScalingReplicaSet         Deployment/probe-test-app             Scaled up replica set probe-test-app-fb95466cc to 2 from 1
4s                      Normal    Started                   Pod/probe-test-app-fb95466cc-cms54    Started container probe-test-app
4s                      Normal    Created                   Pod/probe-test-app-fb95466cc-cms54    Created container probe-test-app
4s                      Normal    Pulled                    Pod/probe-test-app-fb95466cc-cms54    Container image "jasonumiker/probe-test-app:v2" already present on machine
4s                      Normal    Scheduled                 Pod/probe-test-app-fb95466cc-cms54    Successfully assigned default/probe-test-app-fb95466cc-cms54 to docker-desktop
4s                      Normal    ScalingReplicaSet         Deployment/probe-test-app             Scaled down replica set probe-test-app-fc7776cb8 to 2 from 3
4s                      Normal    Killing                   Pod/probe-test-app-fc7776cb8-6tvc9    Stopping container probe-test-app
3s                      Normal    Scheduled                 Pod/probe-test-app-fb95466cc-24gzw    Successfully assigned default/probe-test-app-fb95466cc-24gzw to docker-desktop
3s                      Normal    Started                   Pod/probe-test-app-fb95466cc-24gzw    Started container probe-test-app
3s                      Normal    Created                   Pod/probe-test-app-fb95466cc-24gzw    Created container probe-test-app
3s                      Normal    Pulled                    Pod/probe-test-app-fb95466cc-24gzw    Container image "jasonumiker/probe-test-app:v2" already present on machine
3s                      Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fb95466cc   Created pod: probe-test-app-fb95466cc-24gzw
3s                      Normal    SuccessfulDelete          ReplicaSet/probe-test-app-fc7776cb8   Deleted pod: probe-test-app-fc7776cb8-n9l4x
3s                      Normal    Killing                   Pod/probe-test-app-fc7776cb8-n9l4x    Stopping container probe-test-app
3s                      Normal    ScalingReplicaSet         Deployment/probe-test-app             Scaled down replica set probe-test-app-fc7776cb8 to 1 from 2
3s                      Normal    ScalingReplicaSet         Deployment/probe-test-app             Scaled up replica set probe-test-app-fb95466cc to 3 from 2
2s                      Normal    Killing                   Pod/probe-test-app-fc7776cb8-bdw27    Stopping container probe-test-app
2s                      Normal    SuccessfulDelete          ReplicaSet/probe-test-app-fc7776cb8   Deleted pod: probe-test-app-fc7776cb8-bdw27
2s                      Normal    ScalingReplicaSet         Deployment/probe-test-app             Scaled down replica set probe-test-app-fc7776cb8 to 0 from 1
--------------------
kubectl rollout undo deployment/probe-test-app
deployment.apps/probe-test-app rolled back
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "probe-test-app" rollout to finish: 1 old replicas are pending termination...
deployment "probe-test-app" successfully rolled out
--------------------
kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
probe-test-app-fc7776cb8-7blsd   1/1     Running   0          3s
probe-test-app-fc7776cb8-p5nfv   1/1     Running   0          2s
probe-test-app-fc7776cb8-x2hwn   1/1     Running   0          4s
--------------------
kubectl describe replicaset probe-test-app
Name:           probe-test-app-fb95466cc
Namespace:      default
Selector:       app.kubernetes.io/name=probe-test-app,pod-template-hash=fb95466cc
Labels:         app.kubernetes.io/name=probe-test-app
                pod-template-hash=fb95466cc
Annotations:    deployment.kubernetes.io/desired-replicas: 3
                deployment.kubernetes.io/max-replicas: 4
                deployment.kubernetes.io/revision: 2
Controlled By:  Deployment/probe-test-app
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app.kubernetes.io/name=probe-test-app
           pod-template-hash=fb95466cc
  Containers:
   probe-test-app:
    Image:      jasonumiker/probe-test-app:v2
    Port:       8080/TCP
    Host Port:  0/TCP
    Requests:
      cpu:         50m
      memory:      52Mi
    Liveness:      http-get http://:8080/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:     http-get http://:8080/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  18s   replicaset-controller  Created pod: probe-test-app-fb95466cc-64p2h
  Normal  SuccessfulCreate  12s   replicaset-controller  Created pod: probe-test-app-fb95466cc-cms54
  Normal  SuccessfulCreate  11s   replicaset-controller  Created pod: probe-test-app-fb95466cc-24gzw
  Normal  SuccessfulDelete  6s    replicaset-controller  Deleted pod: probe-test-app-fb95466cc-24gzw
  Normal  SuccessfulDelete  5s    replicaset-controller  Deleted pod: probe-test-app-fb95466cc-64p2h
  Normal  SuccessfulDelete  4s    replicaset-controller  Deleted pod: probe-test-app-fb95466cc-cms54

Name:           probe-test-app-fc7776cb8
Namespace:      default
Selector:       app.kubernetes.io/name=probe-test-app,pod-template-hash=fc7776cb8
Labels:         app.kubernetes.io/name=probe-test-app
                pod-template-hash=fc7776cb8
Annotations:    deployment.kubernetes.io/desired-replicas: 3
                deployment.kubernetes.io/max-replicas: 4
                deployment.kubernetes.io/revision: 3
                deployment.kubernetes.io/revision-history: 1
Controlled By:  Deployment/probe-test-app
Replicas:       3 current / 3 desired
Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app.kubernetes.io/name=probe-test-app
           pod-template-hash=fc7776cb8
  Containers:
   probe-test-app:
    Image:      jasonumiker/probe-test-app:v1
    Port:       8080/TCP
    Host Port:  0/TCP
    Requests:
      cpu:         50m
      memory:      52Mi
    Liveness:      http-get http://:8080/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:     http-get http://:8080/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  23s   replicaset-controller  Created pod: probe-test-app-fc7776cb8-n9l4x
  Normal  SuccessfulCreate  23s   replicaset-controller  Created pod: probe-test-app-fc7776cb8-bdw27
  Normal  SuccessfulCreate  23s   replicaset-controller  Created pod: probe-test-app-fc7776cb8-6tvc9
  Normal  SuccessfulDelete  12s   replicaset-controller  Deleted pod: probe-test-app-fc7776cb8-6tvc9
  Normal  SuccessfulDelete  11s   replicaset-controller  Deleted pod: probe-test-app-fc7776cb8-n9l4x
  Normal  SuccessfulDelete  10s   replicaset-controller  Deleted pod: probe-test-app-fc7776cb8-bdw27
  Normal  SuccessfulCreate  7s    replicaset-controller  Created pod: probe-test-app-fc7776cb8-x2hwn
  Normal  SuccessfulCreate  6s    replicaset-controller  Created pod: probe-test-app-fc7776cb8-7blsd
  Normal  SuccessfulCreate  5s    replicaset-controller  Created pod: probe-test-app-fc7776cb8-p5nfv

--------------------
cd ../sidecar-and-init-containers
--------------------
kubectl apply -f sidecar.yaml
pod/pod-with-sidecar created
pod/pod-with-sidecar condition met
--------------------
kubectl apply -f init.yaml
pod/myapp-pod created
--------------------
kubectl get pod myapp-pod
NAME        READY   STATUS     RESTARTS   AGE
myapp-pod   0/1     Init:0/2   0          1s
--------------------
kubectl apply -f services-init-requires.yaml
service/myservice created
service/mydb created
--------------------
NAME        READY   STATUS    RESTARTS   AGE
myapp-pod   1/1     Running   0          14s
--------------------
pod "myapp-pod" deleted
pod "pod-with-sidecar" deleted
service "myservice" deleted
service "mydb" deleted
--------------------
cd ../pvs-and-statefulsets
--------------------
kubectl apply -f hostpath-provisioner.yaml
deployment.apps/hostpath-provisioner created
storageclass.storage.k8s.io/hostpath-provisioner created
serviceaccount/hostpath-provisioner created
clusterrole.rbac.authorization.k8s.io/hostpath-provisioner created
clusterrolebinding.rbac.authorization.k8s.io/hostpath-provisioner created
--------------------
kubectl get storageclass
NAME                   PROVISIONER            RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
hostpath (default)     docker.io/hostpath     Delete          Immediate              false                  3m46s
hostpath-provisioner   microk8s.io/hostpath   Delete          WaitForFirstConsumer   false                  1s
--------------------
kubectl apply -f pvc.yaml
persistentvolumeclaim/test-pvc created
--------------------
kubectl get pvc
NAME       STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS           VOLUMEATTRIBUTESCLASS   AGE
test-pvc   Pending                                      hostpath-provisioner   <unset>                 1s
--------------------
kubectl apply -f pod.yaml
pod/nginx created
pod/nginx condition met
--------------------
kubectl get pvc
NAME       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS           VOLUMEATTRIBUTESCLASS   AGE
test-pvc   Bound    pvc-12c9c46e-9493-48c5-bbb3-4593cb4e67c2   1Gi        RWO            hostpath-provisioner   <unset>                 12s
--------------------
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS           VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-12c9c46e-9493-48c5-bbb3-4593cb4e67c2   1Gi        RWO            Delete           Bound    default/test-pvc   hostpath-provisioner   <unset>                          6s
--------------------
kubectl apply -f service.yaml
service/nginx created
--------------------
curl http://localhost:8001
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100   153  100   153    0     0  37490      0 --:--:-- --:--:-- --:--:-- 38250
<html>
<head><title>403 Forbidden</title></head>
<body>
<center><h1>403 Forbidden</h1></center>
<hr><center>nginx/1.27.2</center>
</body>
</html>
--------------------
kubectl exec -it nginx  -- bash -c "echo 'Data on PV' > /usr/share/nginx/html/index.html"
--------------------
curl http://localhost:8001
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100    11  100    11    0     0   1990      0 --:--:-- --:--:-- --:--:--  2200
Data on PV
--------------------
kubectl delete pod nginx
pod "nginx" deleted
--------------------
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS           VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-12c9c46e-9493-48c5-bbb3-4593cb4e67c2   1Gi        RWO            Delete           Bound    default/test-pvc   hostpath-provisioner   <unset>                          23s
--------------------
kubectl apply -f pod.yaml
pod/nginx created
pod/nginx condition met
--------------------
curl http://localhost:8001
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100    11  100    11    0     0   3325      0 --:--:-- --:--:-- --:--:--  3666
Data on PV
--------------------
kubectl delete service nginx
service "nginx" deleted
--------------------
kubectl delete pod nginx
pod "nginx" deleted
--------------------
kubectl delete pvc test-pvc
persistentvolumeclaim "test-pvc" deleted
--------------------
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM              STORAGECLASS           VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-12c9c46e-9493-48c5-bbb3-4593cb4e67c2   1Gi        RWO            Delete           Released   default/test-pvc   hostpath-provisioner   <unset>                          31s
--------------------
--------------------
kubectl apply -k .
serviceaccount/rabbitmq created
role.rbac.authorization.k8s.io/rabbitmq created
rolebinding.rbac.authorization.k8s.io/rabbitmq created
configmap/rabbitmq-config created
secret/erlang-cookie created
secret/rabbitmq-admin created
service/rabbitmq-client created
service/rabbitmq-headless created
statefulset.apps/rabbitmq created
Waiting for 1 pods to be ready...
partitioned roll out complete: 1 new pods have been updated...
--------------------
kubectl describe statefulset rabbitmq
Name:               rabbitmq
Namespace:          default
CreationTimestamp:  Sun, 24 Nov 2024 10:35:37 +1100
Selector:           app=rabbitmq
Labels:             <none>
Annotations:        <none>
Replicas:           1 desired | 1 total
Update Strategy:    RollingUpdate
  Partition:        0
Pods Status:        1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=rabbitmq
  Service Account:  rabbitmq
  Init Containers:
   rabbitmq-config:
    Image:      busybox:1.37.0
    Port:       <none>
    Host Port:  <none>
    Command:
      sh
      -c
      cp /tmp/rabbitmq/rabbitmq.conf /etc/rabbitmq/rabbitmq.conf && echo '' >> /etc/rabbitmq/rabbitmq.conf; cp /tmp/rabbitmq/enabled_plugins /etc/rabbitmq/enabled_plugins
    Environment:  <none>
    Mounts:
      /etc/rabbitmq from rabbitmq-config-rw (rw)
      /tmp/rabbitmq from rabbitmq-config (rw)
  Containers:
   rabbitmq:
    Image:       rabbitmq:3.8.34
    Ports:       5672/TCP, 15672/TCP, 15692/TCP, 4369/TCP
    Host Ports:  0/TCP, 0/TCP, 0/TCP, 0/TCP
    Liveness:    exec [rabbitmq-diagnostics status] delay=60s timeout=15s period=60s #success=1 #failure=3
    Readiness:   exec [rabbitmq-diagnostics ping] delay=20s timeout=10s period=60s #success=1 #failure=3
    Environment:
      RABBITMQ_DEFAULT_PASS:   <set to the key 'pass' in secret 'rabbitmq-admin'>   Optional: false
      RABBITMQ_DEFAULT_USER:   <set to the key 'user' in secret 'rabbitmq-admin'>   Optional: false
      RABBITMQ_ERLANG_COOKIE:  <set to the key 'cookie' in secret 'erlang-cookie'>  Optional: false
    Mounts:
      /etc/rabbitmq from rabbitmq-config-rw (rw)
      /var/lib/rabbitmq/mnesia from rabbitmq-data (rw)
  Volumes:
   rabbitmq-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      rabbitmq-config
    Optional:  false
   rabbitmq-config-rw:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
   rabbitmq-data:
    Type:          PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:     rabbitmq-data
    ReadOnly:      false
  Node-Selectors:  <none>
  Tolerations:     <none>
Volume Claims:
  Name:          rabbitmq-data
  StorageClass:  hostpath-provisioner
  Labels:        <none>
  Annotations:   <none>
  Capacity:      3Gi
  Access Modes:  [ReadWriteOnce]
Events:
  Type    Reason            Age   From                    Message
  ----    ------            ----  ----                    -------
  Normal  SuccessfulCreate  68s   statefulset-controller  create Claim rabbitmq-data-rabbitmq-0 Pod rabbitmq-0 in StatefulSet rabbitmq success
  Normal  SuccessfulCreate  68s   statefulset-controller  create Pod rabbitmq-0 in StatefulSet rabbitmq successful
--------------------
kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
probe-test-app-fc7776cb8-7blsd   1/1     Running   0          3m33s
probe-test-app-fc7776cb8-p5nfv   1/1     Running   0          3m32s
probe-test-app-fc7776cb8-x2hwn   1/1     Running   0          3m34s
rabbitmq-0                       1/1     Running   0          69s
--------------------
kubectl get pvc
NAME                       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS           VOLUMEATTRIBUTESCLASS   AGE
rabbitmq-data-rabbitmq-0   Bound    pvc-97875aac-c534-4700-9b70-890e218c54d9   3Gi        RWO            hostpath-provisioner   <unset>                 70s
--------------------
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                              STORAGECLASS           VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-97875aac-c534-4700-9b70-890e218c54d9   3Gi        RWO            Delete           Bound    default/rabbitmq-data-rabbitmq-0   hostpath-provisioner   <unset>                          66s
--------------------
kubectl delete pod rabbitmq-0
pod "rabbitmq-0" deleted
--------------------
kubectl get pods
NAME                             READY   STATUS            RESTARTS   AGE
probe-test-app-fc7776cb8-7blsd   1/1     Running           0          3m39s
probe-test-app-fc7776cb8-p5nfv   1/1     Running           0          3m38s
probe-test-app-fc7776cb8-x2hwn   1/1     Running           0          3m40s
rabbitmq-0                       0/1     PodInitializing   0          1s
--------------------
cd ../../monitoring
--------------------
./install-prometheus.sh

Updating docker-desktop pods to expose metrics endpoints
This will involve several kube-system pod restarts

Fetching debian image to run nsenter on the docker-desktop host...
12.8: Pulling from library/debian
1a3f1864ec54: Pulling fs layer
1a3f1864ec54: Verifying Checksum
1a3f1864ec54: Download complete
1a3f1864ec54: Pull complete
Digest: sha256:10901ccd8d249047f9761845b4594f121edef079cfd8224edebd9ea726f0a7f6
Status: Downloaded newer image for debian:12.8
docker.io/library/debian:12.8
Host Node IP: 192.168.65.3
Updating kube-proxy configmap...
configmap "kube-proxy" deleted
configmap/kube-proxy created
Restarting the kube-proxy pod
pod "kube-proxy-58c9z" deleted
pod/kube-proxy-p6tts condition met
kube-proxy pod restarted.
Updating bind-address on kube-controller-manager...
Waiting for kube-controller-manager to restart, this can take some time...
pod/kube-controller-manager-docker-desktop condition met
pod/kube-controller-manager-docker-desktop condition met
kube-controller-manager pod restarted.
Updating bind-address on kube-scheduler
Waiting for kube-scheduler to restart, this can take some time...
pod/kube-scheduler-docker-desktop condition met
pod/kube-scheduler-docker-desktop condition met
kube-scheduler pod restarted.
Adding node ip to listen-metrics-urls on etcd
Waiting for etcd to restart, this can take some time...
Error from server (Timeout): the server was unable to return a response in the time allotted, but may still be processing the request (get pods)
Error from server (NotFound): pods "etcd-docker-desktop" not found
etcd pod did not restart in time - this may just be the api server still rebooting, give it a few minutes before panicking.

Done! You can now deploy the monitoring components.

"prometheus-community" already exists with the same configuration, skipping
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "gatekeeper" chart repository
...Successfully got an update from the "kedacore" chart repository
...Successfully got an update from the "ingress-nginx" chart repository
...Successfully got an update from the "kiali" chart repository
...Successfully got an update from the "istio" chart repository
...Successfully got an update from the "prometheus-community" chart repository
Update Complete. ⎈Happy Helming!⎈
namespace/monitoring created
NAME: prometheus
LAST DEPLOYED: Sun Nov 24 10:39:15 2024
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
NOTES:
kube-prometheus-stack has been installed. Check its status by running:
  kubectl --namespace monitoring get pods -l "release=prometheus"

Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create & configure Alertmanager and Prometheus instances using the Operator.
NAME: adapter
LAST DEPLOYED: Sun Nov 24 10:39:38 2024
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
adapter-prometheus-adapter has been deployed.
In a few minutes you should be able to list metrics using the following command(s):

  kubectl get --raw /apis/metrics.k8s.io/v1beta1
  kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1
Waiting for deployment "adapter-prometheus-adapter" rollout to finish: 0 of 1 updated replicas are available...
deployment "adapter-prometheus-adapter" successfully rolled out
--------------------
kubectl top nodes
error: metrics not available yet
--------------------
kubectl top pods
error: Metrics not available for pod default/probe-test-app-fc7776cb8-7blsd, age: 8m28.406853s
--------------------
kubectl top pods -n monitoring
error: Metrics not available for pod monitoring/adapter-prometheus-adapter-b84b78594-lrppj, age: 2m4.495731s
--------------------
cd ../probe-test-app
--------------------
kubectl apply -f probe-test-app-hpa.yaml
horizontalpodautoscaler.autoscaling/probe-test-app created
--------------------
kubectl apply -f generate-load-app-replicaset.yaml
replicaset.apps/generate-load-app created
--------------------
kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
generate-load-app-5x728          1/1     Running   0          31s
generate-load-app-cd9kd          1/1     Running   0          31s
generate-load-app-dq5xf          1/1     Running   0          31s
generate-load-app-jzrvs          1/1     Running   0          31s
generate-load-app-zhb4s          1/1     Running   0          31s
probe-test-app-fc7776cb8-7blsd   1/1     Running   0          9m3s
probe-test-app-fc7776cb8-p5nfv   1/1     Running   0          9m2s
probe-test-app-fc7776cb8-x2hwn   1/1     Running   0          9m4s
rabbitmq-0                       1/1     Running   0          5m25s
--------------------
kubectl delete replicaset generate-load-app
replicaset.apps "generate-load-app" deleted
--------------------
kubectl describe hpa probe-test-app
Name:                                                  probe-test-app
Namespace:                                             default
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Sun, 24 Nov 2024 10:41:44 +1100
Reference:                                             Deployment/probe-test-app
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  61% (30m) / 50%
Min replicas:                                          1
Max replicas:                                          5
Deployment pods:                                       3 current / 4 desired
Conditions:
  Type            Status  Reason              Message
  ----            ------  ------              -------
  AbleToScale     True    SucceededRescale    the HPA controller was able to update the target scale to 4
  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)
  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range
Events:
  Type    Reason             Age   From                       Message
  ----    ------             ----  ----                       -------
  Normal  SuccessfulRescale  5s    horizontal-pod-autoscaler  New size: 4; reason: cpu resource utilization (percentage of request) above target
--------------------
cd ../limit-examples
--------------------
kubectl apply -f cpu-stressor.yaml
deployment.apps/cpu-stressor created
Waiting for deployment "cpu-stressor" rollout to finish: 0 of 1 updated replicas are available...
deployment "cpu-stressor" successfully rolled out
--------------------
kubectl delete deployment cpu-stressor
deployment.apps "cpu-stressor" deleted
--------------------
kubectl apply -f memory-stressor.yaml
pod/memory-stressor created
--------------------
kubectl delete pod memory-stressor
pod "memory-stressor" deleted
--------------------
cd ../keda-example
--------------------
./install-keda.sh
"kedacore" already exists with the same configuration, skipping
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "kedacore" chart repository
...Successfully got an update from the "gatekeeper" chart repository
...Successfully got an update from the "ingress-nginx" chart repository
...Successfully got an update from the "kiali" chart repository
...Successfully got an update from the "prometheus-community" chart repository
...Successfully got an update from the "istio" chart repository
Update Complete. ⎈Happy Helming!⎈
NAME: keda
LAST DEPLOYED: Sun Nov 24 10:43:15 2024
NAMESPACE: keda
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
:::^.     .::::^:     :::::::::::::::    .:::::::::.                   .^.                  
7???~   .^7????~.     7??????????????.   :?????????77!^.              .7?7.                 
7???~  ^7???7~.       ~!!!!!!!!!!!!!!.   :????!!!!7????7~.           .7???7.                
7???~^7????~.                            :????:    :~7???7.         :7?????7.               
7???7????!.           ::::::::::::.      :????:      .7???!        :7??77???7.              
7????????7:           7???????????~      :????:       :????:      :???7?5????7.             
7????!~????^          !77777777777^      :????:       :????:     ^???7?#P7????7.            
7???~  ^????~                            :????:      :7???!     ^???7J#@J7?????7.           
7???~   :7???!.                          :????:   .:~7???!.    ~???7Y&@#7777????7.          
7???~    .7???7:      !!!!!!!!!!!!!!!    :????7!!77????7^     ~??775@@@GJJYJ?????7.         
7???~     .!????^     7?????????????7.   :?????????7!~:      !????G@@@@@@@@5??????7:        
::::.       :::::     :::::::::::::::    .::::::::..        .::::JGGGB@@@&7:::::::::        
                                                                      ?@@#~                  
                                                                      P@B^                   
                                                                    :&G:                    
                                                                    !5.                     
                                                                    .Kubernetes Event-driven Autoscaling (KEDA) - Application autoscaling made simple.

Get started by deploying Scaled Objects to your cluster:
    - Information about Scaled Objects : https://keda.sh/docs/latest/concepts/
    - Samples: https://github.com/kedacore/samples

Get information about the deployed ScaledObjects:
  kubectl get scaledobject [--namespace <namespace>]

Get details about a deployed ScaledObject:
  kubectl describe scaledobject <scaled-object-name> [--namespace <namespace>]

Get information about the deployed ScaledObjects:
  kubectl get triggerauthentication [--namespace <namespace>]

Get details about a deployed ScaledObject:
  kubectl describe triggerauthentication <trigger-authentication-name> [--namespace <namespace>]

Get an overview of the Horizontal Pod Autoscalers (HPA) that KEDA is using behind the scenes:
  kubectl get hpa [--all-namespaces] [--namespace <namespace>]

Learn more about KEDA:
- Documentation: https://keda.sh/
- Support: https://keda.sh/support/
- File an issue: https://github.com/kedacore/keda/issues/new/choose
--------------------
kubectl apply -f consumer.yaml
secret/rabbitmq-consumer-secret created
deployment.apps/rabbitmq-consumer created
--------------------
kubectl apply -f keda-scaled-object.yaml
scaledobject.keda.sh/rabbitmq-consumer created
triggerauthentication.keda.sh/rabbitmq-consumer-trigger created
--------------------
kubectl apply -f publisher.yaml
job.batch/rabbitmq-publish created
--------------------
kubectl get pods
NAME                                READY   STATUS              RESTARTS   AGE
probe-test-app-fc7776cb8-7blsd      1/1     Running             0          10m
probe-test-app-fc7776cb8-p5nfv      1/1     Running             0          10m
probe-test-app-fc7776cb8-wtzh7      1/1     Running             0          46s
probe-test-app-fc7776cb8-x2hwn      1/1     Running             0          10m
rabbitmq-0                          1/1     Running             0          6m39s
rabbitmq-consumer-dd9d7cfd4-n5ncq   0/1     ContainerCreating   0          3s
rabbitmq-publish-hn4h4              0/1     ContainerCreating   0          1s
--------------------
kubectl events
LAST SEEN               TYPE      REASON                    OBJECT                                           MESSAGE
12m (x8 over 12m)       Normal    NodeHasSufficientMemory   Node/docker-desktop                              Node docker-desktop status is now: NodeHasSufficientMemory
12m                     Normal    NodeAllocatableEnforced   Node/docker-desktop                              Updated Node Allocatable limit across pods
12m (x7 over 12m)       Normal    NodeHasSufficientPID      Node/docker-desktop                              Node docker-desktop status is now: NodeHasSufficientPID
12m (x7 over 12m)       Normal    NodeHasNoDiskPressure     Node/docker-desktop                              Node docker-desktop status is now: NodeHasNoDiskPressure
12m                     Normal    Starting                  Node/docker-desktop                              Starting kubelet.
12m                     Warning   InvalidDiskCapacity       Node/docker-desktop                              invalid capacity 0 on image filesystem
12m                     Normal    RegisteredNode            Node/docker-desktop                              Node docker-desktop event: Registered Node docker-desktop in Controller
12m                     Normal    Starting                  Node/docker-desktop                              
11m                     Normal    Scheduled                 Pod/probe-test-app                               Successfully assigned default/probe-test-app to docker-desktop
11m                     Normal    Pulling                   Pod/probe-test-app                               Pulling image "jasonumiker/probe-test-app:v1"
10m                     Normal    Started                   Pod/probe-test-app                               Started container probe-test-app
10m                     Normal    Created                   Pod/probe-test-app                               Created container probe-test-app
10m                     Normal    Pulled                    Pod/probe-test-app                               Successfully pulled image "jasonumiker/probe-test-app:v1" in 13.964s (13.964s including waiting). Image size: 1025410707 bytes.
10m                     Warning   Unhealthy                 Pod/probe-test-app-2                             Readiness probe failed: Get "http://10.1.0.7:8080/readyz": dial tcp 10.1.0.7:8080: connect: connection refused
10m                     Normal    Started                   Pod/probe-test-app-2                             Started container probe-test-app
10m                     Normal    Scheduled                 Pod/probe-test-app-2                             Successfully assigned default/probe-test-app-2 to docker-desktop
10m                     Normal    Created                   Pod/probe-test-app-2                             Created container probe-test-app
10m                     Normal    Pulled                    Pod/probe-test-app-2                             Container image "jasonumiker/probe-test-app:v1" already present on machine
10m                     Normal    Killing                   Pod/probe-test-app                               Stopping container probe-test-app
10m                     Normal    Killing                   Pod/probe-test-app-2                             Stopping container probe-test-app
10m                     Normal    Scheduled                 Pod/probe-test-app-zhcz8                         Successfully assigned default/probe-test-app-zhcz8 to docker-desktop
10m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app                        Created pod: probe-test-app-kqx8f
10m                     Normal    Scheduled                 Pod/probe-test-app-86nv4                         Successfully assigned default/probe-test-app-86nv4 to docker-desktop
10m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app                        Created pod: probe-test-app-86nv4
10m                     Normal    Scheduled                 Pod/probe-test-app-kqx8f                         Successfully assigned default/probe-test-app-kqx8f to docker-desktop
10m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app                        Created pod: probe-test-app-zhcz8
10m                     Normal    Created                   Pod/probe-test-app-zhcz8                         Created container probe-test-app
10m                     Normal    Pulled                    Pod/probe-test-app-zhcz8                         Container image "jasonumiker/probe-test-app:v1" already present on machine
10m                     Normal    Started                   Pod/probe-test-app-kqx8f                         Started container probe-test-app
10m                     Normal    Created                   Pod/probe-test-app-kqx8f                         Created container probe-test-app
10m                     Normal    Pulled                    Pod/probe-test-app-kqx8f                         Container image "jasonumiker/probe-test-app:v1" already present on machine
10m                     Normal    Started                   Pod/probe-test-app-zhcz8                         Started container probe-test-app
10m                     Normal    Pulled                    Pod/probe-test-app-86nv4                         Container image "jasonumiker/probe-test-app:v1" already present on machine
10m                     Normal    Created                   Pod/probe-test-app-86nv4                         Created container probe-test-app
10m                     Normal    Started                   Pod/probe-test-app-86nv4                         Started container probe-test-app
10m                     Normal    SuccessfulDelete          ReplicaSet/probe-test-app                        Deleted pod: probe-test-app-kqx8f
10m                     Normal    Killing                   Pod/probe-test-app-kqx8f                         Stopping container probe-test-app
10m                     Normal    Killing                   Pod/probe-test-app-zhcz8                         Stopping container probe-test-app
10m                     Normal    Killing                   Pod/probe-test-app-86nv4                         Stopping container probe-test-app
10m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8              Created pod: probe-test-app-fc7776cb8-bdw27
10m                     Normal    Pulled                    Pod/probe-test-app-fc7776cb8-n9l4x               Container image "jasonumiker/probe-test-app:v1" already present on machine
10m                     Normal    Started                   Pod/probe-test-app-fc7776cb8-bdw27               Started container probe-test-app
10m                     Normal    Created                   Pod/probe-test-app-fc7776cb8-bdw27               Created container probe-test-app
10m                     Normal    Pulled                    Pod/probe-test-app-fc7776cb8-bdw27               Container image "jasonumiker/probe-test-app:v1" already present on machine
10m                     Normal    Scheduled                 Pod/probe-test-app-fc7776cb8-bdw27               Successfully assigned default/probe-test-app-fc7776cb8-bdw27 to docker-desktop
10m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled up replica set probe-test-app-fc7776cb8 to 3
10m                     Normal    Started                   Pod/probe-test-app-fc7776cb8-6tvc9               Started container probe-test-app
10m                     Normal    Created                   Pod/probe-test-app-fc7776cb8-6tvc9               Created container probe-test-app
10m                     Normal    Pulled                    Pod/probe-test-app-fc7776cb8-6tvc9               Container image "jasonumiker/probe-test-app:v1" already present on machine
10m                     Normal    Scheduled                 Pod/probe-test-app-fc7776cb8-6tvc9               Successfully assigned default/probe-test-app-fc7776cb8-6tvc9 to docker-desktop
10m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8              Created pod: probe-test-app-fc7776cb8-6tvc9
10m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8              Created pod: probe-test-app-fc7776cb8-n9l4x
10m                     Normal    Started                   Pod/probe-test-app-fc7776cb8-n9l4x               Started container probe-test-app
10m                     Normal    Created                   Pod/probe-test-app-fc7776cb8-n9l4x               Created container probe-test-app
10m                     Normal    Scheduled                 Pod/probe-test-app-fc7776cb8-n9l4x               Successfully assigned default/probe-test-app-fc7776cb8-n9l4x to docker-desktop
10m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fb95466cc              Created pod: probe-test-app-fb95466cc-64p2h
10m                     Normal    Pulling                   Pod/probe-test-app-fb95466cc-64p2h               Pulling image "jasonumiker/probe-test-app:v2"
10m                     Normal    Scheduled                 Pod/probe-test-app-fb95466cc-64p2h               Successfully assigned default/probe-test-app-fb95466cc-64p2h to docker-desktop
10m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled up replica set probe-test-app-fb95466cc to 1
10m                     Normal    Started                   Pod/probe-test-app-fb95466cc-64p2h               Started container probe-test-app
10m                     Normal    Created                   Pod/probe-test-app-fb95466cc-64p2h               Created container probe-test-app
10m                     Normal    Pulled                    Pod/probe-test-app-fb95466cc-64p2h               Successfully pulled image "jasonumiker/probe-test-app:v2" in 4.537s (4.537s including waiting). Image size: 1025410707 bytes.
10m                     Normal    Started                   Pod/probe-test-app-fb95466cc-cms54               Started container probe-test-app
10m                     Normal    Scheduled                 Pod/probe-test-app-fb95466cc-cms54               Successfully assigned default/probe-test-app-fb95466cc-cms54 to docker-desktop
10m                     Normal    Created                   Pod/probe-test-app-fb95466cc-cms54               Created container probe-test-app
10m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fb95466cc              Created pod: probe-test-app-fb95466cc-cms54
10m                     Normal    SuccessfulDelete          ReplicaSet/probe-test-app-fc7776cb8              Deleted pod: probe-test-app-fc7776cb8-6tvc9
10m                     Normal    Killing                   Pod/probe-test-app-fc7776cb8-6tvc9               Stopping container probe-test-app
10m                     Normal    Pulled                    Pod/probe-test-app-fb95466cc-cms54               Container image "jasonumiker/probe-test-app:v2" already present on machine
10m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled down replica set probe-test-app-fc7776cb8 to 2 from 3
10m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled up replica set probe-test-app-fb95466cc to 2 from 1
10m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fb95466cc              Created pod: probe-test-app-fb95466cc-24gzw
10m                     Normal    Killing                   Pod/probe-test-app-fc7776cb8-n9l4x               Stopping container probe-test-app
10m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled down replica set probe-test-app-fc7776cb8 to 1 from 2
10m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled up replica set probe-test-app-fb95466cc to 3 from 2
10m                     Normal    Pulled                    Pod/probe-test-app-fb95466cc-24gzw               Container image "jasonumiker/probe-test-app:v2" already present on machine
10m                     Normal    Created                   Pod/probe-test-app-fb95466cc-24gzw               Created container probe-test-app
10m                     Normal    Scheduled                 Pod/probe-test-app-fb95466cc-24gzw               Successfully assigned default/probe-test-app-fb95466cc-24gzw to docker-desktop
10m                     Normal    Started                   Pod/probe-test-app-fb95466cc-24gzw               Started container probe-test-app
10m                     Normal    SuccessfulDelete          ReplicaSet/probe-test-app-fc7776cb8              Deleted pod: probe-test-app-fc7776cb8-n9l4x
10m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled down replica set probe-test-app-fc7776cb8 to 0 from 1
10m                     Normal    SuccessfulDelete          ReplicaSet/probe-test-app-fc7776cb8              Deleted pod: probe-test-app-fc7776cb8-bdw27
10m                     Normal    Killing                   Pod/probe-test-app-fc7776cb8-bdw27               Stopping container probe-test-app
10m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled up replica set probe-test-app-fc7776cb8 to 1 from 0
10m                     Normal    Started                   Pod/probe-test-app-fc7776cb8-x2hwn               Started container probe-test-app
10m                     Normal    Created                   Pod/probe-test-app-fc7776cb8-x2hwn               Created container probe-test-app
10m                     Normal    Pulled                    Pod/probe-test-app-fc7776cb8-x2hwn               Container image "jasonumiker/probe-test-app:v1" already present on machine
10m                     Normal    Scheduled                 Pod/probe-test-app-fc7776cb8-x2hwn               Successfully assigned default/probe-test-app-fc7776cb8-x2hwn to docker-desktop
10m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8              Created pod: probe-test-app-fc7776cb8-x2hwn
10m                     Normal    Scheduled                 Pod/probe-test-app-fc7776cb8-7blsd               Successfully assigned default/probe-test-app-fc7776cb8-7blsd to docker-desktop
10m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8              Created pod: probe-test-app-fc7776cb8-7blsd
10m                     Normal    Killing                   Pod/probe-test-app-fb95466cc-24gzw               Stopping container probe-test-app
10m                     Normal    SuccessfulDelete          ReplicaSet/probe-test-app-fb95466cc              Deleted pod: probe-test-app-fb95466cc-24gzw
10m                     Normal    Started                   Pod/probe-test-app-fc7776cb8-7blsd               Started container probe-test-app
10m                     Normal    Created                   Pod/probe-test-app-fc7776cb8-7blsd               Created container probe-test-app
10m                     Normal    Pulled                    Pod/probe-test-app-fc7776cb8-7blsd               Container image "jasonumiker/probe-test-app:v1" already present on machine
10m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled down replica set probe-test-app-fb95466cc to 2 from 3
10m                     Normal    Scheduled                 Pod/probe-test-app-fc7776cb8-p5nfv               Successfully assigned default/probe-test-app-fc7776cb8-p5nfv to docker-desktop
10m                     Normal    Killing                   Pod/probe-test-app-fb95466cc-64p2h               Stopping container probe-test-app
10m                     Normal    Pulled                    Pod/probe-test-app-fc7776cb8-p5nfv               Container image "jasonumiker/probe-test-app:v1" already present on machine
10m                     Normal    Started                   Pod/probe-test-app-fc7776cb8-p5nfv               Started container probe-test-app
10m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8              Created pod: probe-test-app-fc7776cb8-p5nfv
10m                     Normal    SuccessfulDelete          ReplicaSet/probe-test-app-fb95466cc              Deleted pod: probe-test-app-fb95466cc-64p2h
10m                     Normal    Created                   Pod/probe-test-app-fc7776cb8-p5nfv               Created container probe-test-app
10m (x4 over 10m)       Normal    ScalingReplicaSet         Deployment/probe-test-app                        (combined from similar events): Scaled down replica set probe-test-app-fb95466cc to 0 from 1
10m                     Normal    SuccessfulDelete          ReplicaSet/probe-test-app-fb95466cc              Deleted pod: probe-test-app-fb95466cc-cms54
10m                     Normal    Killing                   Pod/probe-test-app-fb95466cc-cms54               Stopping container probe-test-app
10m                     Normal    Scheduled                 Pod/pod-with-sidecar                             Successfully assigned default/pod-with-sidecar to docker-desktop
10m                     Normal    Pulling                   Pod/pod-with-sidecar                             Pulling image "alpine:3.20.3"
10m                     Normal    Created                   Pod/pod-with-sidecar                             Created container app-container
10m                     Normal    Pulled                    Pod/pod-with-sidecar                             Successfully pulled image "alpine:3.20.3" in 4.935s (4.935s including waiting). Image size: 8825163 bytes.
10m                     Normal    Started                   Pod/pod-with-sidecar                             Started container app-container
10m                     Normal    Pulling                   Pod/pod-with-sidecar                             Pulling image "nginx:1.27.2-bookworm"
9m58s                   Normal    Pulled                    Pod/pod-with-sidecar                             Successfully pulled image "nginx:1.27.2-bookworm" in 6.243s (6.243s including waiting). Image size: 196880043 bytes.
9m58s                   Normal    Created                   Pod/pod-with-sidecar                             Created container sidecar-container
9m58s                   Normal    Started                   Pod/pod-with-sidecar                             Started container sidecar-container
9m56s                   Normal    Scheduled                 Pod/myapp-pod                                    Successfully assigned default/myapp-pod to docker-desktop
9m55s                   Normal    Pulling                   Pod/myapp-pod                                    Pulling image "busybox:1.28"
9m51s                   Normal    Started                   Pod/myapp-pod                                    Started container init-mydb
9m51s                   Normal    Created                   Pod/myapp-pod                                    Created container init-mydb
9m51s                   Normal    Pulled                    Pod/myapp-pod                                    Container image "busybox:1.28" already present on machine
9m51s                   Normal    Started                   Pod/myapp-pod                                    Started container init-myservice
9m51s                   Normal    Created                   Pod/myapp-pod                                    Created container init-myservice
9m51s                   Normal    Pulled                    Pod/myapp-pod                                    Successfully pulled image "busybox:1.28" in 4.297s (4.297s including waiting). Image size: 1279385 bytes.
9m50s                   Normal    Started                   Pod/myapp-pod                                    Started container myapp-container
9m50s                   Normal    Created                   Pod/myapp-pod                                    Created container myapp-container
9m50s                   Normal    Pulled                    Pod/myapp-pod                                    Container image "busybox:1.28" already present on machine
9m41s                   Normal    Killing                   Pod/myapp-pod                                    Stopping container myapp-container
9m10s                   Normal    Killing                   Pod/pod-with-sidecar                             Stopping container app-container
9m10s                   Normal    Killing                   Pod/pod-with-sidecar                             Stopping container sidecar-container
8m35s                   Normal    WaitForFirstConsumer      PersistentVolumeClaim/test-pvc                   waiting for first consumer to be created before binding
8m33s                   Normal    ExternalProvisioning      PersistentVolumeClaim/test-pvc                   Waiting for a volume to be created either by the external provisioner 'microk8s.io/hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
8m32s                   Normal    Provisioning              PersistentVolumeClaim/test-pvc                   External provisioner is provisioning volume for claim "default/test-pvc"
8m27s                   Normal    Scheduled                 Pod/nginx                                        Successfully assigned default/nginx to docker-desktop
8m27s                   Normal    ProvisioningSucceeded     PersistentVolumeClaim/test-pvc                   Successfully provisioned volume pvc-12c9c46e-9493-48c5-bbb3-4593cb4e67c2
8m26s                   Normal    Pulling                   Pod/nginx                                        Pulling image "nginx:1.27.2"
8m24s                   Normal    Created                   Pod/nginx                                        Created container nginx
8m24s                   Normal    Started                   Pod/nginx                                        Started container nginx
8m24s                   Normal    Pulled                    Pod/nginx                                        Successfully pulled image "nginx:1.27.2" in 1.968s (1.968s including waiting). Image size: 196880043 bytes.
8m6s                    Normal    Killing                   Pod/nginx                                        Stopping container nginx
8m3s                    Normal    Pulled                    Pod/nginx                                        Container image "nginx:1.27.2" already present on machine
8m3s                    Normal    Created                   Pod/nginx                                        Created container nginx
8m3s                    Normal    Started                   Pod/nginx                                        Started container nginx
8m3s                    Normal    Scheduled                 Pod/nginx                                        Successfully assigned default/nginx to docker-desktop
7m59s                   Normal    Killing                   Pod/nginx                                        Stopping container nginx
7m54s                   Normal    Provisioning              PersistentVolumeClaim/rabbitmq-data-rabbitmq-0   External provisioner is provisioning volume for claim "default/rabbitmq-data-rabbitmq-0"
7m54s                   Normal    ExternalProvisioning      PersistentVolumeClaim/rabbitmq-data-rabbitmq-0   Waiting for a volume to be created either by the external provisioner 'microk8s.io/hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
7m54s                   Normal    WaitForFirstConsumer      PersistentVolumeClaim/rabbitmq-data-rabbitmq-0   waiting for first consumer to be created before binding
7m54s                   Normal    SuccessfulCreate          StatefulSet/rabbitmq                             create Claim rabbitmq-data-rabbitmq-0 Pod rabbitmq-0 in StatefulSet rabbitmq success
7m49s                   Normal    ProvisioningSucceeded     PersistentVolumeClaim/rabbitmq-data-rabbitmq-0   Successfully provisioned volume pvc-97875aac-c534-4700-9b70-890e218c54d9
7m48s                   Normal    Scheduled                 Pod/rabbitmq-0                                   Successfully assigned default/rabbitmq-0 to docker-desktop
7m47s                   Normal    Pulling                   Pod/rabbitmq-0                                   Pulling image "busybox:1.37.0"
7m43s                   Normal    Created                   Pod/rabbitmq-0                                   Created container rabbitmq-config
7m43s                   Normal    Pulled                    Pod/rabbitmq-0                                   Successfully pulled image "busybox:1.37.0" in 4.168s (4.168s including waiting). Image size: 4042190 bytes.
7m43s                   Normal    Pulling                   Pod/rabbitmq-0                                   Pulling image "rabbitmq:3.8.34"
7m43s                   Normal    Started                   Pod/rabbitmq-0                                   Started container rabbitmq-config
7m36s                   Normal    Started                   Pod/rabbitmq-0                                   Started container rabbitmq
7m36s                   Normal    Created                   Pod/rabbitmq-0                                   Created container rabbitmq
7m36s                   Normal    Pulled                    Pod/rabbitmq-0                                   Successfully pulled image "rabbitmq:3.8.34" in 6.581s (6.581s including waiting). Image size: 185945511 bytes.
<unknown>               Normal    Created                   RabbitMQ/pod/rabbitmq-0                          Node rabbit@rabbitmq-0 is registered
6m42s                   Normal    Killing                   Pod/rabbitmq-0                                   Stopping container rabbitmq
6m40s                   Normal    Scheduled                 Pod/rabbitmq-0                                   Successfully assigned default/rabbitmq-0 to docker-desktop
6m40s (x2 over 7m54s)   Normal    SuccessfulCreate          StatefulSet/rabbitmq                             create Pod rabbitmq-0 in StatefulSet rabbitmq successful
6m39s                   Normal    Pulled                    Pod/rabbitmq-0                                   Container image "busybox:1.37.0" already present on machine
6m39s                   Normal    Pulled                    Pod/rabbitmq-0                                   Container image "rabbitmq:3.8.34" already present on machine
6m39s                   Normal    Started                   Pod/rabbitmq-0                                   Started container rabbitmq-config
6m39s                   Normal    Created                   Pod/rabbitmq-0                                   Created container rabbitmq
6m39s                   Normal    Started                   Pod/rabbitmq-0                                   Started container rabbitmq
6m39s                   Normal    Created                   Pod/rabbitmq-0                                   Created container rabbitmq-config
<unknown>               Normal    Created                   RabbitMQ/pod/rabbitmq-0                          Node rabbit@rabbitmq-0 is registered
6m30s                   Normal    Starting                  Node/docker-desktop                              
6m13s                   Normal    RegisteredNode            Node/docker-desktop                              Node docker-desktop event: Registered Node docker-desktop in Controller
106s                    Normal    SuccessfulCreate          ReplicaSet/generate-load-app                     Created pod: generate-load-app-zhb4s
106s                    Normal    SuccessfulCreate          ReplicaSet/generate-load-app                     Created pod: generate-load-app-jzrvs
106s                    Normal    SuccessfulCreate          ReplicaSet/generate-load-app                     Created pod: generate-load-app-cd9kd
106s                    Normal    SuccessfulCreate          ReplicaSet/generate-load-app                     Created pod: generate-load-app-dq5xf
106s                    Normal    SuccessfulCreate          ReplicaSet/generate-load-app                     Created pod: generate-load-app-5x728
106s                    Normal    Scheduled                 Pod/generate-load-app-5x728                      Successfully assigned default/generate-load-app-5x728 to docker-desktop
106s                    Normal    Scheduled                 Pod/generate-load-app-zhb4s                      Successfully assigned default/generate-load-app-zhb4s to docker-desktop
106s                    Normal    Scheduled                 Pod/generate-load-app-dq5xf                      Successfully assigned default/generate-load-app-dq5xf to docker-desktop
106s                    Normal    Scheduled                 Pod/generate-load-app-cd9kd                      Successfully assigned default/generate-load-app-cd9kd to docker-desktop
106s                    Normal    Scheduled                 Pod/generate-load-app-jzrvs                      Successfully assigned default/generate-load-app-jzrvs to docker-desktop
105s                    Normal    Created                   Pod/generate-load-app-zhb4s                      Created container generate-load-app
105s                    Normal    Pulled                    Pod/generate-load-app-dq5xf                      Container image "busybox:1.37.0" already present on machine
105s                    Normal    Created                   Pod/generate-load-app-5x728                      Created container generate-load-app
105s                    Normal    Started                   Pod/generate-load-app-5x728                      Started container generate-load-app
105s                    Normal    Started                   Pod/generate-load-app-zhb4s                      Started container generate-load-app
105s                    Normal    Pulled                    Pod/generate-load-app-cd9kd                      Container image "busybox:1.37.0" already present on machine
105s                    Normal    Created                   Pod/generate-load-app-cd9kd                      Created container generate-load-app
105s                    Normal    Started                   Pod/generate-load-app-cd9kd                      Started container generate-load-app
105s                    Normal    Pulled                    Pod/generate-load-app-zhb4s                      Container image "busybox:1.37.0" already present on machine
105s                    Normal    Pulled                    Pod/generate-load-app-5x728                      Container image "busybox:1.37.0" already present on machine
105s                    Normal    Created                   Pod/generate-load-app-dq5xf                      Created container generate-load-app
105s                    Normal    Started                   Pod/generate-load-app-dq5xf                      Started container generate-load-app
105s                    Normal    Started                   Pod/generate-load-app-jzrvs                      Started container generate-load-app
105s                    Normal    Pulled                    Pod/generate-load-app-jzrvs                      Container image "busybox:1.37.0" already present on machine
105s                    Normal    Created                   Pod/generate-load-app-jzrvs                      Created container generate-load-app
74s                     Normal    Killing                   Pod/generate-load-app-dq5xf                      Stopping container generate-load-app
74s                     Normal    Killing                   Pod/generate-load-app-jzrvs                      Stopping container generate-load-app
74s                     Normal    Killing                   Pod/generate-load-app-cd9kd                      Stopping container generate-load-app
74s                     Normal    Killing                   Pod/generate-load-app-5x728                      Stopping container generate-load-app
74s                     Normal    Killing                   Pod/generate-load-app-zhb4s                      Stopping container generate-load-app
47s                     Normal    SuccessfulRescale         HorizontalPodAutoscaler/probe-test-app           New size: 4; reason: cpu resource utilization (percentage of request) above target
47s                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8              Created pod: probe-test-app-fc7776cb8-wtzh7
47s                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled up replica set probe-test-app-fc7776cb8 to 4 from 3
47s                     Normal    Scheduled                 Pod/probe-test-app-fc7776cb8-wtzh7               Successfully assigned default/probe-test-app-fc7776cb8-wtzh7 to docker-desktop
46s                     Normal    Pulled                    Pod/probe-test-app-fc7776cb8-wtzh7               Container image "jasonumiker/probe-test-app:v1" already present on machine
46s                     Normal    Created                   Pod/probe-test-app-fc7776cb8-wtzh7               Created container probe-test-app
46s                     Normal    Started                   Pod/probe-test-app-fc7776cb8-wtzh7               Started container probe-test-app
46s                     Warning   Unhealthy                 Pod/probe-test-app-fc7776cb8-wtzh7               Readiness probe failed: Get "http://10.1.0.43:8080/readyz": dial tcp 10.1.0.43:8080: connect: connection refused
40s                     Normal    SuccessfulCreate          ReplicaSet/cpu-stressor-8556c54f68               Created pod: cpu-stressor-8556c54f68-249mz
40s                     Normal    ScalingReplicaSet         Deployment/cpu-stressor                          Scaled up replica set cpu-stressor-8556c54f68 to 1
40s                     Normal    Pulling                   Pod/cpu-stressor-8556c54f68-249mz                Pulling image "narmidm/k8s-pod-cpu-stressor:v1.2.0"
40s                     Normal    Scheduled                 Pod/cpu-stressor-8556c54f68-249mz                Successfully assigned default/cpu-stressor-8556c54f68-249mz to docker-desktop
36s                     Normal    Started                   Pod/cpu-stressor-8556c54f68-249mz                Started container cpu-stressor
36s                     Normal    Created                   Pod/cpu-stressor-8556c54f68-249mz                Created container cpu-stressor
36s                     Normal    Pulled                    Pod/cpu-stressor-8556c54f68-249mz                Successfully pulled image "narmidm/k8s-pod-cpu-stressor:v1.2.0" in 4.285s (4.285s including waiting). Image size: 9842826 bytes.
33s                     Normal    Killing                   Pod/cpu-stressor-8556c54f68-249mz                Stopping container cpu-stressor
32s                     Normal    Scheduled                 Pod/memory-stressor                              Successfully assigned default/memory-stressor to docker-desktop
31s                     Normal    Pulling                   Pod/memory-stressor                              Pulling image "polinux/stress:1.0.4"
27s                     Normal    Pulled                    Pod/memory-stressor                              Container image "polinux/stress:1.0.4" already present on machine
27s (x2 over 27s)       Normal    Started                   Pod/memory-stressor                              Started container memory-stressor
27s (x2 over 27s)       Normal    Created                   Pod/memory-stressor                              Created container memory-stressor
27s                     Normal    Pulled                    Pod/memory-stressor                              Successfully pulled image "polinux/stress:1.0.4" in 4.15s (4.15s including waiting). Image size: 9744175 bytes.
25s (x2 over 26s)       Warning   BackOff                   Pod/memory-stressor                              Back-off restarting failed container memory-stressor in pod memory-stressor_default(f479f681-4057-4cdd-89c5-0acf2603797f)
4s                      Normal    SuccessfulCreate          ReplicaSet/rabbitmq-consumer-dd9d7cfd4           Created pod: rabbitmq-consumer-dd9d7cfd4-n5ncq
4s                      Normal    ScalingReplicaSet         Deployment/rabbitmq-consumer                     Scaled up replica set rabbitmq-consumer-dd9d7cfd4 to 1
4s                      Normal    Pulling                   Pod/rabbitmq-consumer-dd9d7cfd4-n5ncq            Pulling image "ghcr.io/kedacore/rabbitmq-client:v1.0"
4s                      Normal    Scheduled                 Pod/rabbitmq-consumer-dd9d7cfd4-n5ncq            Successfully assigned default/rabbitmq-consumer-dd9d7cfd4-n5ncq to docker-desktop
2s                      Normal    SuccessfulCreate          Job/rabbitmq-publish                             Created pod: rabbitmq-publish-hn4h4
2s                      Normal    Scheduled                 Pod/rabbitmq-publish-hn4h4                       Successfully assigned default/rabbitmq-publish-hn4h4 to docker-desktop
1s                      Normal    Started                   Pod/rabbitmq-consumer-dd9d7cfd4-n5ncq            Started container rabbitmq-consumer
1s                      Normal    Pulling                   Pod/rabbitmq-publish-hn4h4                       Pulling image "ghcr.io/kedacore/rabbitmq-client:v1.0"
1s                      Normal    Created                   Pod/rabbitmq-consumer-dd9d7cfd4-n5ncq            Created container rabbitmq-consumer
1s                      Normal    Pulled                    Pod/rabbitmq-consumer-dd9d7cfd4-n5ncq            Successfully pulled image "ghcr.io/kedacore/rabbitmq-client:v1.0" in 2.402s (2.402s including waiting). Image size: 10241515 bytes.
0s                      Normal    Started                   Pod/rabbitmq-publish-hn4h4                       Started container rabbitmq-client
0s                      Normal    Created                   Pod/rabbitmq-publish-hn4h4                       Created container rabbitmq-client
0s                      Normal    Pulled                    Pod/rabbitmq-publish-hn4h4                       Successfully pulled image "ghcr.io/kedacore/rabbitmq-client:v1.0" in 821ms (995ms including waiting). Image size: 10241515 bytes.
--------------------
kubectl describe job rabbitmq-publish
Name:             rabbitmq-publish
Namespace:        default
Selector:         batch.kubernetes.io/controller-uid=33c44809-348c-407d-b2b6-5c5b7f192b32
Labels:           batch.kubernetes.io/controller-uid=33c44809-348c-407d-b2b6-5c5b7f192b32
                  batch.kubernetes.io/job-name=rabbitmq-publish
                  controller-uid=33c44809-348c-407d-b2b6-5c5b7f192b32
                  job-name=rabbitmq-publish
Annotations:      <none>
Parallelism:      1
Completions:      1
Completion Mode:  NonIndexed
Suspend:          false
Backoff Limit:    4
Start Time:       Sun, 24 Nov 2024 10:43:29 +1100
Pods Statuses:    1 Active (0 Ready) / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  batch.kubernetes.io/controller-uid=33c44809-348c-407d-b2b6-5c5b7f192b32
           batch.kubernetes.io/job-name=rabbitmq-publish
           controller-uid=33c44809-348c-407d-b2b6-5c5b7f192b32
           job-name=rabbitmq-publish
  Containers:
   rabbitmq-client:
    Image:      ghcr.io/kedacore/rabbitmq-client:v1.0
    Port:       <none>
    Host Port:  <none>
    Command:
      send
      $(rabbitmq_host)
      300
    Environment:
      rabbitmq_host:  <set to the key 'host' in secret 'rabbitmq-consumer-secret'>  Optional: false
    Mounts:           <none>
  Volumes:            <none>
  Node-Selectors:     <none>
  Tolerations:        <none>
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  4s    job-controller  Created pod: rabbitmq-publish-hn4h4
--------------------
cd ../cronjob
--------------------
kubectl apply -f cronjob.yaml
cronjob.batch/hello created
--------------------
kubectl get pods
NAME                                READY   STATUS      RESTARTS   AGE
hello-28873424-4sth4                0/1     Completed   0          101s
hello-28873425-gtbxp                0/1     Completed   0          41s
probe-test-app-fc7776cb8-7blsd      1/1     Running     0          12m
probe-test-app-fc7776cb8-nd8v6      1/1     Running     0          117s
probe-test-app-fc7776cb8-p5nfv      1/1     Running     0          12m
probe-test-app-fc7776cb8-wtzh7      1/1     Running     0          2m57s
probe-test-app-fc7776cb8-x2hwn      1/1     Running     0          12m
rabbitmq-0                          1/1     Running     0          8m50s
rabbitmq-consumer-dd9d7cfd4-5qsrw   1/1     Running     0          56s
rabbitmq-consumer-dd9d7cfd4-n5ncq   1/1     Running     0          2m14s
rabbitmq-consumer-dd9d7cfd4-tnckp   1/1     Running     0          56s
rabbitmq-consumer-dd9d7cfd4-zmb7v   1/1     Running     0          56s
rabbitmq-publish-hn4h4              0/1     Completed   0          2m12s
--------------------
kubectl get cronjob
NAME    SCHEDULE    TIMEZONE   SUSPEND   ACTIVE   LAST SCHEDULE   AGE
hello   * * * * *   <none>     False     0        42s             2m7s
--------------------
kubectl delete cronjob hello
cronjob.batch "hello" deleted
--------------------
kubectl get pods -A
NAMESPACE     NAME                                                     READY   STATUS      RESTARTS        AGE
default       probe-test-app-fc7776cb8-7blsd                           1/1     Running     0               12m
default       probe-test-app-fc7776cb8-nd8v6                           1/1     Running     0               2m
default       probe-test-app-fc7776cb8-p5nfv                           1/1     Running     0               12m
default       probe-test-app-fc7776cb8-wtzh7                           1/1     Running     0               3m
default       probe-test-app-fc7776cb8-x2hwn                           1/1     Running     0               12m
default       rabbitmq-0                                               1/1     Running     0               8m53s
default       rabbitmq-consumer-dd9d7cfd4-5qsrw                        1/1     Running     0               59s
default       rabbitmq-consumer-dd9d7cfd4-n5ncq                        1/1     Running     0               2m17s
default       rabbitmq-consumer-dd9d7cfd4-tnckp                        1/1     Running     0               59s
default       rabbitmq-consumer-dd9d7cfd4-zmb7v                        1/1     Running     0               59s
default       rabbitmq-publish-hn4h4                                   0/1     Completed   0               2m15s
keda          keda-admission-webhooks-6b7b75c487-pfkxz                 1/1     Running     0               2m28s
keda          keda-operator-86846bb678-n8t8n                           1/1     Running     1 (2m20s ago)   2m28s
keda          keda-operator-metrics-apiserver-5b677c7769-8l9jd         1/1     Running     0               2m28s
kube-system   coredns-7db6d8ff4d-cx79v                                 1/1     Running     0               14m
kube-system   coredns-7db6d8ff4d-ps8h5                                 1/1     Running     0               14m
kube-system   etcd-docker-desktop                                      1/1     Running     0               6m42s
kube-system   hostpath-provisioner-6bb9769b5f-xgxpw                    1/1     Running     1 (7m43s ago)   10m
kube-system   kube-apiserver-docker-desktop                            1/1     Running     0               14m
kube-system   kube-controller-manager-docker-desktop                   1/1     Running     0               8m28s
kube-system   kube-proxy-p6tts                                         1/1     Running     0               8m43s
kube-system   kube-scheduler-docker-desktop                            1/1     Running     1 (7m38s ago)   8m6s
kube-system   storage-provisioner                                      1/1     Running     1 (7m45s ago)   14m
kube-system   vpnkit-controller                                        1/1     Running     0               14m
monitoring    adapter-prometheus-adapter-b84b78594-lrppj               1/1     Running     0               6m6s
monitoring    alertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running     0               6m
monitoring    prometheus-grafana-6b758d7b46-lj9tj                      3/3     Running     0               6m13s
monitoring    prometheus-kube-prometheus-operator-c5f7c5b6-j8j8f       1/1     Running     0               6m13s
monitoring    prometheus-kube-state-metrics-677845d566-98775           1/1     Running     0               6m13s
monitoring    prometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running     0               6m
monitoring    prometheus-prometheus-node-exporter-vr5np                1/1     Running     0               6m13s
--------------------
kubectl api-resources
NAME                                SHORTNAMES               APIVERSION                        NAMESPACED   KIND
bindings                                                     v1                                true         Binding
componentstatuses                   cs                       v1                                false        ComponentStatus
configmaps                          cm                       v1                                true         ConfigMap
endpoints                           ep                       v1                                true         Endpoints
events                              ev                       v1                                true         Event
limitranges                         limits                   v1                                true         LimitRange
namespaces                          ns                       v1                                false        Namespace
nodes                               no                       v1                                false        Node
persistentvolumeclaims              pvc                      v1                                true         PersistentVolumeClaim
persistentvolumes                   pv                       v1                                false        PersistentVolume
pods                                po                       v1                                true         Pod
podtemplates                                                 v1                                true         PodTemplate
replicationcontrollers              rc                       v1                                true         ReplicationController
resourcequotas                      quota                    v1                                true         ResourceQuota
secrets                                                      v1                                true         Secret
serviceaccounts                     sa                       v1                                true         ServiceAccount
services                            svc                      v1                                true         Service
mutatingwebhookconfigurations                                admissionregistration.k8s.io/v1   false        MutatingWebhookConfiguration
validatingadmissionpolicies                                  admissionregistration.k8s.io/v1   false        ValidatingAdmissionPolicy
validatingadmissionpolicybindings                            admissionregistration.k8s.io/v1   false        ValidatingAdmissionPolicyBinding
validatingwebhookconfigurations                              admissionregistration.k8s.io/v1   false        ValidatingWebhookConfiguration
customresourcedefinitions           crd,crds                 apiextensions.k8s.io/v1           false        CustomResourceDefinition
apiservices                                                  apiregistration.k8s.io/v1         false        APIService
controllerrevisions                                          apps/v1                           true         ControllerRevision
daemonsets                          ds                       apps/v1                           true         DaemonSet
deployments                         deploy                   apps/v1                           true         Deployment
replicasets                         rs                       apps/v1                           true         ReplicaSet
statefulsets                        sts                      apps/v1                           true         StatefulSet
selfsubjectreviews                                           authentication.k8s.io/v1          false        SelfSubjectReview
tokenreviews                                                 authentication.k8s.io/v1          false        TokenReview
localsubjectaccessreviews                                    authorization.k8s.io/v1           true         LocalSubjectAccessReview
selfsubjectaccessreviews                                     authorization.k8s.io/v1           false        SelfSubjectAccessReview
selfsubjectrulesreviews                                      authorization.k8s.io/v1           false        SelfSubjectRulesReview
subjectaccessreviews                                         authorization.k8s.io/v1           false        SubjectAccessReview
horizontalpodautoscalers            hpa                      autoscaling/v2                    true         HorizontalPodAutoscaler
cronjobs                            cj                       batch/v1                          true         CronJob
jobs                                                         batch/v1                          true         Job
certificatesigningrequests          csr                      certificates.k8s.io/v1            false        CertificateSigningRequest
leases                                                       coordination.k8s.io/v1            true         Lease
endpointslices                                               discovery.k8s.io/v1               true         EndpointSlice
cloudeventsources                                            eventing.keda.sh/v1alpha1         true         CloudEventSource
clustercloudeventsources                                     eventing.keda.sh/v1alpha1         false        ClusterCloudEventSource
events                              ev                       events.k8s.io/v1                  true         Event
flowschemas                                                  flowcontrol.apiserver.k8s.io/v1   false        FlowSchema
prioritylevelconfigurations                                  flowcontrol.apiserver.k8s.io/v1   false        PriorityLevelConfiguration
clustertriggerauthentications       cta,clustertriggerauth   keda.sh/v1alpha1                  false        ClusterTriggerAuthentication
scaledjobs                          sj                       keda.sh/v1alpha1                  true         ScaledJob
scaledobjects                       so                       keda.sh/v1alpha1                  true         ScaledObject
triggerauthentications              ta,triggerauth           keda.sh/v1alpha1                  true         TriggerAuthentication
nodes                                                        metrics.k8s.io/v1beta1            false        NodeMetrics
pods                                                         metrics.k8s.io/v1beta1            true         PodMetrics
alertmanagerconfigs                 amcfg                    monitoring.coreos.com/v1alpha1    true         AlertmanagerConfig
alertmanagers                       am                       monitoring.coreos.com/v1          true         Alertmanager
podmonitors                         pmon                     monitoring.coreos.com/v1          true         PodMonitor
probes                              prb                      monitoring.coreos.com/v1          true         Probe
prometheusagents                    promagent                monitoring.coreos.com/v1alpha1    true         PrometheusAgent
prometheuses                        prom                     monitoring.coreos.com/v1          true         Prometheus
prometheusrules                     promrule                 monitoring.coreos.com/v1          true         PrometheusRule
scrapeconfigs                       scfg                     monitoring.coreos.com/v1alpha1    true         ScrapeConfig
servicemonitors                     smon                     monitoring.coreos.com/v1          true         ServiceMonitor
thanosrulers                        ruler                    monitoring.coreos.com/v1          true         ThanosRuler
ingressclasses                                               networking.k8s.io/v1              false        IngressClass
ingresses                           ing                      networking.k8s.io/v1              true         Ingress
networkpolicies                     netpol                   networking.k8s.io/v1              true         NetworkPolicy
runtimeclasses                                               node.k8s.io/v1                    false        RuntimeClass
poddisruptionbudgets                pdb                      policy/v1                         true         PodDisruptionBudget
clusterrolebindings                                          rbac.authorization.k8s.io/v1      false        ClusterRoleBinding
clusterroles                                                 rbac.authorization.k8s.io/v1      false        ClusterRole
rolebindings                                                 rbac.authorization.k8s.io/v1      true         RoleBinding
roles                                                        rbac.authorization.k8s.io/v1      true         Role
priorityclasses                     pc                       scheduling.k8s.io/v1              false        PriorityClass
csidrivers                                                   storage.k8s.io/v1                 false        CSIDriver
csinodes                                                     storage.k8s.io/v1                 false        CSINode
csistoragecapacities                                         storage.k8s.io/v1                 true         CSIStorageCapacity
storageclasses                      sc                       storage.k8s.io/v1                 false        StorageClass
volumeattachments                                            storage.k8s.io/v1                 false        VolumeAttachment
--------------------
kubectl get clusterrole admin -o yaml
aggregationRule:
  clusterRoleSelectors:
  - matchLabels:
      rbac.authorization.k8s.io/aggregate-to-admin: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  creationTimestamp: "2024-11-23T23:31:01Z"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: admin
  resourceVersion: "313"
  uid: ad2f8b1e-2cb4-42f6-9ddf-db67468c57e1
rules:
- apiGroups:
  - ""
  resources:
  - pods/attach
  - pods/exec
  - pods/portforward
  - pods/proxy
  - secrets
  - services/proxy
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - serviceaccounts
  verbs:
  - impersonate
- apiGroups:
  - ""
  resources:
  - pods
  - pods/attach
  - pods/exec
  - pods/portforward
  - pods/proxy
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - ""
  resources:
  - pods/eviction
  verbs:
  - create
- apiGroups:
  - ""
  resources:
  - configmaps
  - events
  - persistentvolumeclaims
  - replicationcontrollers
  - replicationcontrollers/scale
  - secrets
  - serviceaccounts
  - services
  - services/proxy
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - ""
  resources:
  - serviceaccounts/token
  verbs:
  - create
- apiGroups:
  - apps
  resources:
  - daemonsets
  - deployments
  - deployments/rollback
  - deployments/scale
  - replicasets
  - replicasets/scale
  - statefulsets
  - statefulsets/scale
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - autoscaling
  resources:
  - horizontalpodautoscalers
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - batch
  resources:
  - cronjobs
  - jobs
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - extensions
  resources:
  - daemonsets
  - deployments
  - deployments/rollback
  - deployments/scale
  - ingresses
  - networkpolicies
  - replicasets
  - replicasets/scale
  - replicationcontrollers/scale
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  - networkpolicies
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - ""
  resources:
  - configmaps
  - endpoints
  - persistentvolumeclaims
  - persistentvolumeclaims/status
  - pods
  - replicationcontrollers
  - replicationcontrollers/scale
  - serviceaccounts
  - services
  - services/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - bindings
  - events
  - limitranges
  - namespaces/status
  - pods/log
  - pods/status
  - replicationcontrollers/status
  - resourcequotas
  - resourcequotas/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - discovery.k8s.io
  resources:
  - endpointslices
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - apps
  resources:
  - controllerrevisions
  - daemonsets
  - daemonsets/status
  - deployments
  - deployments/scale
  - deployments/status
  - replicasets
  - replicasets/scale
  - replicasets/status
  - statefulsets
  - statefulsets/scale
  - statefulsets/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - autoscaling
  resources:
  - horizontalpodautoscalers
  - horizontalpodautoscalers/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - batch
  resources:
  - cronjobs
  - cronjobs/status
  - jobs
  - jobs/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - extensions
  resources:
  - daemonsets
  - daemonsets/status
  - deployments
  - deployments/scale
  - deployments/status
  - ingresses
  - ingresses/status
  - networkpolicies
  - replicasets
  - replicasets/scale
  - replicasets/status
  - replicationcontrollers/scale
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets
  - poddisruptionbudgets/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  - ingresses/status
  - networkpolicies
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - authorization.k8s.io
  resources:
  - localsubjectaccessreviews
  verbs:
  - create
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  - rolebindings
  - roles
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch
--------------------
kubectl get clusterrole admin -o yaml | wc -l
     315
--------------------
cd ../k8s-authz
--------------------
./setup-tokens-on-cluster.sh
--------------------
./add-users-kubeconfig.sh
Context "docker-desktop-jane" created.
Context "docker-desktop-john" created.
--------------------
cat team1.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: "team1"
  labels:
    name: "team1"
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: admin
  namespace: team1
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["*"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: admin
  namespace: team1
subjects:
- kind: User
  name: jane
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: admin
  apiGroup: rbac.authorization.k8s.io--------------------
kubectl apply -f team1.yaml && kubectl apply -f team2.yaml
namespace/team1 created
role.rbac.authorization.k8s.io/admin created
rolebinding.rbac.authorization.k8s.io/admin created
namespace/team2 created
role.rbac.authorization.k8s.io/admin created
rolebinding.rbac.authorization.k8s.io/admin created
--------------------
kubectl config get-contexts
CURRENT   NAME                  CLUSTER          AUTHINFO         NAMESPACE
*         docker-desktop        docker-desktop   docker-desktop   
          docker-desktop-jane   docker-desktop   jane             team1
          docker-desktop-john   docker-desktop   john             team2
--------------------
kubectl config use-context docker-desktop-jane
Switched to context "docker-desktop-jane".
--------------------
kubectl get pods -A
Error from server (Forbidden): pods is forbidden: User "jane" cannot list resource "pods" in API group "" at the cluster scope
--------------------
kubectl get pods
No resources found in team1 namespace.
--------------------
kubectl config use-context docker-desktop-john
Switched to context "docker-desktop-john".
--------------------
kubectl get pods
No resources found in team2 namespace.
--------------------
kubectl get pods --namespace=team1
Error from server (Forbidden): pods is forbidden: User "john" cannot list resource "pods" in API group "" in the namespace "team1"
--------------------
kubectl config use-context docker-desktop
Switched to context "docker-desktop".
--------------------
Cleaning up Jane and John...
deleted user jane from /Users/jumiker/.kube/config
deleted user john from /Users/jumiker/.kube/config
deleted context docker-desktop-jane from /Users/jumiker/.kube/config
deleted context docker-desktop-john from /Users/jumiker/.kube/config
--------------------
cd ../ingress
--------------------
./install-nginx.sh
"ingress-nginx" already exists with the same configuration, skipping
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "gatekeeper" chart repository
...Successfully got an update from the "kedacore" chart repository
...Successfully got an update from the "ingress-nginx" chart repository
...Successfully got an update from the "kiali" chart repository
...Successfully got an update from the "prometheus-community" chart repository
...Successfully got an update from the "istio" chart repository
Update Complete. ⎈Happy Helming!⎈
NAME: ingress
LAST DEPLOYED: Sun Nov 24 10:46:47 2024
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
The ingress-nginx controller has been installed.
It may take a few minutes for the load balancer IP to be available.
You can watch the status by running 'kubectl get service --namespace default ingress-ingress-nginx-controller --output wide --watch'

An example Ingress that makes use of the controller:
  apiVersion: networking.k8s.io/v1
  kind: Ingress
  metadata:
    name: example
    namespace: foo
  spec:
    ingressClassName: nginx
    rules:
      - host: www.example.com
        http:
          paths:
            - pathType: Prefix
              backend:
                service:
                  name: exampleService
                  port:
                    number: 80
              path: /
    # This section is only required if TLS is to be enabled for the Ingress
    tls:
      - hosts:
        - www.example.com
        secretName: example-tls

If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided:

  apiVersion: v1
  kind: Secret
  metadata:
    name: example-tls
    namespace: foo
  data:
    tls.crt: <base64 encoded cert>
    tls.key: <base64 encoded key>
  type: kubernetes.io/tls
--------------------
kubectl apply -f probe-test-app-ingress.yaml
ingress.networking.k8s.io/probe-test-app created
--------------------
curl http://localhost
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100   577  100   577    0     0  83720      0 --:--:-- --:--:-- --:--:-- 96166
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>probetest</title>
</head>
<body>
    <p>This Flask app is served from probe-test-app-fc7776cb8-7blsd</p>
    <p>The readiness endpoint (/readyz) is Healthy</p>
    <p>The liveness endpoint is (/livez) Healthy</p>
    <form method="POST">
        <button type="submit" name="readyz">Toggle Readiness</button>
        <button type="submit" name="livez">Toggle Liveness</button>
    </form>
    <p>Version 1</p>
</body>
</html>--------------------
kubectl apply -f nyancat.yaml
deployment.apps/nyancat created
service/nyancat created
--------------------
kubectl rollout status deployment nyancat -n default
Waiting for deployment "nyancat" rollout to finish: 0 of 1 updated replicas are available...
deployment "nyancat" successfully rolled out
--------------------
kubectl apply -f nyancat-ingress.yaml
ingress.networking.k8s.io/probe-test-app configured
--------------------
curl http://localhost/nyancat/
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100   207  100   207    0     0  31155      0 --:--:-- --:--:-- --:--:-- 34500
<!doctype html>
<html lang=en>
<title>404 Not Found</title>
<h1>Not Found</h1>
<p>The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.</p>
--------------------
kubectl delete ingress probe-test-app
ingress.networking.k8s.io "probe-test-app" deleted
--------------------
helm uninstall ingress
release "ingress" uninstalled
--------------------
Cleaning up probe-test-app and nyancat...
horizontalpodautoscaler.autoscaling "probe-test-app" deleted
deployment.apps "probe-test-app" deleted
deployment.apps "nyancat" deleted
service "probe-test-app" deleted
service "nyancat" deleted
--------------------
cd ../istio
--------------------
./install-istio.sh
"istio" already exists with the same configuration, skipping
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "gatekeeper" chart repository
...Successfully got an update from the "kedacore" chart repository
...Successfully got an update from the "ingress-nginx" chart repository
...Successfully got an update from the "kiali" chart repository
...Successfully got an update from the "prometheus-community" chart repository
...Successfully got an update from the "istio" chart repository
Update Complete. ⎈Happy Helming!⎈
namespace/istio-system created
NAME: istio-base
LAST DEPLOYED: Sun Nov 24 10:48:29 2024
NAMESPACE: istio-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Istio base successfully installed!

To learn more about the release, try:
  $ helm status istio-base -n istio-system
  $ helm get all istio-base -n istio-system
NAME: istiod
LAST DEPLOYED: Sun Nov 24 10:48:31 2024
NAMESPACE: istio-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
"istiod" successfully installed!

To learn more about the release, try:
  $ helm status istiod -n istio-system
  $ helm get all istiod -n istio-system

Next steps:
  * Deploy a Gateway: https://istio.io/latest/docs/setup/additional-setup/gateway/
  * Try out our tasks to get started on common configurations:
    * https://istio.io/latest/docs/tasks/traffic-management
    * https://istio.io/latest/docs/tasks/security/
    * https://istio.io/latest/docs/tasks/policy-enforcement/
  * Review the list of actively supported releases, CVE publications and our hardening guide:
    * https://istio.io/latest/docs/releases/supported-releases/
    * https://istio.io/latest/news/security/
    * https://istio.io/latest/docs/ops/best-practices/security/

For further documentation see https://istio.io website
"kiali" already exists with the same configuration, skipping
NAME: kiali-server
LAST DEPLOYED: Sun Nov 24 10:48:39 2024
NAMESPACE: istio-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Welcome to Kiali! For more details on Kiali, see: https://kiali.io

The Kiali Server [v2.1.0] has been installed in namespace [istio-system]. It will be ready soon.

When installing with "deployment.cluster_wide_access=false" using this Kiali Server Helm Chart,
it is your responsibility to manually create the proper Roles and RoleBindings for the Kiali Server
to have the correct permissions to access the service mesh namespaces.

(Helm: Chart=[kiali-server], Release=[kiali-server], Version=[2.1.0])
servicemonitor.monitoring.coreos.com/prometheus-oper-istio-controlplane created
servicemonitor.monitoring.coreos.com/prometheus-oper-istio-dataplane created
customresourcedefinition.apiextensions.k8s.io/gatewayclasses.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/gateways.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/grpcroutes.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/httproutes.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/referencegrants.gateway.networking.k8s.io created
--------------------
kubectl label namespace default istio-injection=enabled
namespace/default labeled
--------------------
kubectl apply -f bookinfo/platform/kube/bookinfo.yaml
service/details created
serviceaccount/bookinfo-details created
deployment.apps/details-v1 created
service/ratings created
serviceaccount/bookinfo-ratings created
deployment.apps/ratings-v1 created
service/reviews created
serviceaccount/bookinfo-reviews created
deployment.apps/reviews-v1 created
deployment.apps/reviews-v2 created
deployment.apps/reviews-v3 created
service/productpage created
serviceaccount/bookinfo-productpage created
deployment.apps/productpage-v1 created
--------------------
kubectl apply -f bookinfo/gateway-api/bookinfo-gateway.yaml
gateway.gateway.networking.k8s.io/bookinfo-gateway created
httproute.gateway.networking.k8s.io/bookinfo created
--------------------
kubectl apply -f bookinfo/platform/kube/bookinfo-versions.yaml
service/reviews-v1 created
service/reviews-v2 created
service/reviews-v3 created
service/productpage-v1 created
service/ratings-v1 created
service/details-v1 created
--------------------
kubectl apply -f bookinfo/gateway-api/route-all-v1.yaml
httproute.gateway.networking.k8s.io/reviews created
httproute.gateway.networking.k8s.io/productpage created
httproute.gateway.networking.k8s.io/ratings created
httproute.gateway.networking.k8s.io/details created
--------------------
kubectl apply -f bookinfo/gateway-api/route-reviews-90-10.yaml
httproute.gateway.networking.k8s.io/reviews configured
--------------------
kubectl apply -f bookinfo/gateway-api/route-jason-v2.yaml
httproute.gateway.networking.k8s.io/reviews configured
--------------------
bookinfo/platform/kube/cleanup.sh
using NAMESPACE=default
gateway.gateway.networking.k8s.io "bookinfo-gateway" deleted
httproute.gateway.networking.k8s.io "bookinfo" deleted
httproute.gateway.networking.k8s.io "details" deleted
httproute.gateway.networking.k8s.io "productpage" deleted
httproute.gateway.networking.k8s.io "ratings" deleted
httproute.gateway.networking.k8s.io "reviews" deleted
Application cleanup may take up to one minute
service "details" deleted
serviceaccount "bookinfo-details" deleted
deployment.apps "details-v1" deleted
service "ratings" deleted
serviceaccount "bookinfo-ratings" deleted
deployment.apps "ratings-v1" deleted
service "reviews" deleted
serviceaccount "bookinfo-reviews" deleted
deployment.apps "reviews-v1" deleted
deployment.apps "reviews-v2" deleted
deployment.apps "reviews-v3" deleted
service "productpage" deleted
serviceaccount "bookinfo-productpage" deleted
deployment.apps "productpage-v1" deleted
Application cleanup successful
--------------------
cd ../kustomize
--------------------
kustomize build prod
apiVersion: v1
kind: Service
metadata:
  labels:
    run: my-nginx
  name: prod-my-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
  selector:
    run: my-nginx
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prod-my-nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      run: my-nginx
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - image: nginx
        name: my-nginx
--------------------
kubectl apply -k prod
service/prod-my-nginx created
deployment.apps/prod-my-nginx created
--------------------
kubectl get pods
NAME                             READY   STATUS      RESTARTS   AGE
prod-my-nginx-554c7b49dd-9dkv9   0/2     Init:0/1    0          1s
prod-my-nginx-554c7b49dd-lsk6p   0/2     Init:0/1    0          1s
rabbitmq-0                       1/1     Running     0          13m
rabbitmq-publish-hn4h4           0/1     Completed   0          6m27s
--------------------
kubectl apply -k dev
service/dev-my-nginx created
deployment.apps/dev-my-nginx created
--------------------
kubectl get pods
NAME                             READY   STATUS            RESTARTS   AGE
dev-my-nginx-554c7b49dd-76wxv    0/2     Init:0/1          0          1s
prod-my-nginx-554c7b49dd-9dkv9   0/2     PodInitializing   0          3s
prod-my-nginx-554c7b49dd-lsk6p   0/2     PodInitializing   0          3s
rabbitmq-0                       1/1     Running           0          13m
rabbitmq-publish-hn4h4           0/1     Completed         0          6m29s
--------------------
Cleaning up Kustomization example...
service "prod-my-nginx" deleted
deployment.apps "prod-my-nginx" deleted
service "dev-my-nginx" deleted
deployment.apps "dev-my-nginx" deleted
--------------------
helm ls -A
NAME        	NAMESPACE   	REVISION	UPDATED                              	STATUS  	CHART                       	APP VERSION
adapter     	monitoring  	1       	2024-11-24 10:39:38.377617 +1100 AEDT	deployed	prometheus-adapter-4.11.0   	v0.12.0    
istio-base  	istio-system	1       	2024-11-24 10:48:29.833618 +1100 AEDT	deployed	base-1.24.0                 	1.24.0     
istiod      	istio-system	1       	2024-11-24 10:48:31.01309 +1100 AEDT 	deployed	istiod-1.24.0               	1.24.0     
keda        	keda        	1       	2024-11-24 10:43:15.614363 +1100 AEDT	deployed	keda-2.16.0                 	2.16.0     
kiali-server	istio-system	1       	2024-11-24 10:48:39.821786 +1100 AEDT	deployed	kiali-server-2.1.0          	v2.1.0     
prometheus  	monitoring  	1       	2024-11-24 10:39:15.687673 +1100 AEDT	deployed	kube-prometheus-stack-65.8.1	v0.77.2    
--------------------
Installing required CRD updates for prometheus chart upgrade from 65 to 66...
error: Apply failed with 2 conflicts: conflicts with "helm" using apiextensions.k8s.io/v1:
- .metadata.annotations.controller-gen.kubebuilder.io/version
- .metadata.annotations.operator.prometheus.io/version
Please review the fields above--they currently have other managers. Here
are the ways you can resolve this warning:
* If you intend to manage all of these fields, please re-run the apply
  command with the `--force-conflicts` flag.
* If you do not intend to manage all of the fields, please edit your
  manifest to remove references to the fields that should keep their
  current managers.
* You may co-own fields by updating your manifest to match the existing
  value; in this case, you'll become the manager if the other manager(s)
  stop managing the field (remove it from their configuration).
See https://kubernetes.io/docs/reference/using-api/server-side-apply/#conflicts
error: Apply failed with 3 conflicts: conflicts with "helm" using apiextensions.k8s.io/v1:
- .metadata.annotations.controller-gen.kubebuilder.io/version
- .metadata.annotations.operator.prometheus.io/version
- .spec.versions
Please review the fields above--they currently have other managers. Here
are the ways you can resolve this warning:
* If you intend to manage all of these fields, please re-run the apply
  command with the `--force-conflicts` flag.
* If you do not intend to manage all of the fields, please edit your
  manifest to remove references to the fields that should keep their
  current managers.
* You may co-own fields by updating your manifest to match the existing
  value; in this case, you'll become the manager if the other manager(s)
  stop managing the field (remove it from their configuration).
See https://kubernetes.io/docs/reference/using-api/server-side-apply/#conflicts
error: Apply failed with 3 conflicts: conflicts with "helm" using apiextensions.k8s.io/v1:
- .metadata.annotations.controller-gen.kubebuilder.io/version
- .metadata.annotations.operator.prometheus.io/version
- .spec.versions
Please review the fields above--they currently have other managers. Here
are the ways you can resolve this warning:
* If you intend to manage all of these fields, please re-run the apply
  command with the `--force-conflicts` flag.
* If you do not intend to manage all of the fields, please edit your
  manifest to remove references to the fields that should keep their
  current managers.
* You may co-own fields by updating your manifest to match the existing
  value; in this case, you'll become the manager if the other manager(s)
  stop managing the field (remove it from their configuration).
See https://kubernetes.io/docs/reference/using-api/server-side-apply/#conflicts
error: Apply failed with 3 conflicts: conflicts with "helm" using apiextensions.k8s.io/v1:
- .metadata.annotations.controller-gen.kubebuilder.io/version
- .metadata.annotations.operator.prometheus.io/version
- .spec.versions
Please review the fields above--they currently have other managers. Here
are the ways you can resolve this warning:
* If you intend to manage all of these fields, please re-run the apply
  command with the `--force-conflicts` flag.
* If you do not intend to manage all of the fields, please edit your
  manifest to remove references to the fields that should keep their
  current managers.
* You may co-own fields by updating your manifest to match the existing
  value; in this case, you'll become the manager if the other manager(s)
  stop managing the field (remove it from their configuration).
See https://kubernetes.io/docs/reference/using-api/server-side-apply/#conflicts
error: Apply failed with 3 conflicts: conflicts with "helm" using apiextensions.k8s.io/v1:
- .metadata.annotations.controller-gen.kubebuilder.io/version
- .metadata.annotations.operator.prometheus.io/version
- .spec.versions
Please review the fields above--they currently have other managers. Here
are the ways you can resolve this warning:
* If you intend to manage all of these fields, please re-run the apply
  command with the `--force-conflicts` flag.
* If you do not intend to manage all of the fields, please edit your
  manifest to remove references to the fields that should keep their
  current managers.
* You may co-own fields by updating your manifest to match the existing
  value; in this case, you'll become the manager if the other manager(s)
  stop managing the field (remove it from their configuration).
See https://kubernetes.io/docs/reference/using-api/server-side-apply/#conflicts
error: Apply failed with 3 conflicts: conflicts with "helm" using apiextensions.k8s.io/v1:
- .metadata.annotations.controller-gen.kubebuilder.io/version
- .metadata.annotations.operator.prometheus.io/version
- .spec.versions
Please review the fields above--they currently have other managers. Here
are the ways you can resolve this warning:
* If you intend to manage all of these fields, please re-run the apply
  command with the `--force-conflicts` flag.
* If you do not intend to manage all of the fields, please edit your
  manifest to remove references to the fields that should keep their
  current managers.
* You may co-own fields by updating your manifest to match the existing
  value; in this case, you'll become the manager if the other manager(s)
  stop managing the field (remove it from their configuration).
See https://kubernetes.io/docs/reference/using-api/server-side-apply/#conflicts
error: Apply failed with 3 conflicts: conflicts with "helm" using apiextensions.k8s.io/v1:
- .metadata.annotations.controller-gen.kubebuilder.io/version
- .metadata.annotations.operator.prometheus.io/version
- .spec.versions
Please review the fields above--they currently have other managers. Here
are the ways you can resolve this warning:
* If you intend to manage all of these fields, please re-run the apply
  command with the `--force-conflicts` flag.
* If you do not intend to manage all of the fields, please edit your
  manifest to remove references to the fields that should keep their
  current managers.
* You may co-own fields by updating your manifest to match the existing
  value; in this case, you'll become the manager if the other manager(s)
  stop managing the field (remove it from their configuration).
See https://kubernetes.io/docs/reference/using-api/server-side-apply/#conflicts
error: Apply failed with 3 conflicts: conflicts with "helm" using apiextensions.k8s.io/v1:
- .metadata.annotations.controller-gen.kubebuilder.io/version
- .metadata.annotations.operator.prometheus.io/version
- .spec.versions
Please review the fields above--they currently have other managers. Here
are the ways you can resolve this warning:
* If you intend to manage all of these fields, please re-run the apply
  command with the `--force-conflicts` flag.
* If you do not intend to manage all of the fields, please edit your
  manifest to remove references to the fields that should keep their
  current managers.
* You may co-own fields by updating your manifest to match the existing
  value; in this case, you'll become the manager if the other manager(s)
  stop managing the field (remove it from their configuration).
See https://kubernetes.io/docs/reference/using-api/server-side-apply/#conflicts
error: Apply failed with 3 conflicts: conflicts with "helm" using apiextensions.k8s.io/v1:
- .metadata.annotations.controller-gen.kubebuilder.io/version
- .metadata.annotations.operator.prometheus.io/version
- .spec.versions
Please review the fields above--they currently have other managers. Here
are the ways you can resolve this warning:
* If you intend to manage all of these fields, please re-run the apply
  command with the `--force-conflicts` flag.
* If you do not intend to manage all of the fields, please edit your
  manifest to remove references to the fields that should keep their
  current managers.
* You may co-own fields by updating your manifest to match the existing
  value; in this case, you'll become the manager if the other manager(s)
  stop managing the field (remove it from their configuration).
See https://kubernetes.io/docs/reference/using-api/server-side-apply/#conflicts
error: Apply failed with 3 conflicts: conflicts with "helm" using apiextensions.k8s.io/v1:
- .metadata.annotations.controller-gen.kubebuilder.io/version
- .metadata.annotations.operator.prometheus.io/version
- .spec.versions
Please review the fields above--they currently have other managers. Here
are the ways you can resolve this warning:
* If you intend to manage all of these fields, please re-run the apply
  command with the `--force-conflicts` flag.
* If you do not intend to manage all of the fields, please edit your
  manifest to remove references to the fields that should keep their
  current managers.
* You may co-own fields by updating your manifest to match the existing
  value; in this case, you'll become the manager if the other manager(s)
  stop managing the field (remove it from their configuration).
See https://kubernetes.io/docs/reference/using-api/server-side-apply/#conflicts
--------------------
helm upgrade prometheus prometheus-community/kube-prometheus-stack --version 66.2.1 -n monitoring
Release "prometheus" has been upgraded. Happy Helming!
NAME: prometheus
LAST DEPLOYED: Sun Nov 24 10:50:18 2024
NAMESPACE: monitoring
STATUS: deployed
REVISION: 2
NOTES:
kube-prometheus-stack has been installed. Check its status by running:
  kubectl --namespace monitoring get pods -l "release=prometheus"

Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create & configure Alertmanager and Prometheus instances using the Operator.
--------------------
helm get values prometheus -n monitoring
USER-SUPPLIED VALUES:
grafana:
  service:
    port: 3000
    type: LoadBalancer
kubelet:
  serviceMonitor:
    cAdvisorMetricRelabelings: null
prometheus:
  service:
    type: LoadBalancer
prometheus-node-exporter:
  hostRootFsMount:
    enabled: false
  prometheus:
    monitor:
      attachMetadata:
        node: true
      relabelings:
      - action: replace
        regex: (.+)
        replacement: ${1}
        sourceLabels:
        - __meta_kubernetes_endpoint_node_name
        targetLabel: node
--------------------
helm repo add gatekeeper https://open-policy-agent.github.io/gatekeeper/charts
"gatekeeper" already exists with the same configuration, skipping
--------------------
helm install gatekeeper/gatekeeper --name-template=gatekeeper --namespace gatekeeper-system --create-namespace --version 3.17.1
NAME: gatekeeper
LAST DEPLOYED: Sun Nov 24 11:23:35 2024
NAMESPACE: gatekeeper-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
--------------------
cd ../opa-gatekeeper
--------------------
kubectl apply -f k8srequiredlabels-constraint-template.yaml
constrainttemplate.templates.gatekeeper.sh/k8srequiredlabels unchanged
--------------------
kubectl apply -f pods-in-default-must-have-owner.yaml
k8srequiredlabels.constraints.gatekeeper.sh/pods-in-default-must-have-owner created
--------------------
kubectl apply -f ../probe-test-app/probe-test-app-pod.yaml
Error from server (Forbidden): error when creating "../probe-test-app/probe-test-app-pod.yaml": admission webhook "validation.gatekeeper.sh" denied the request: [pods-in-default-must-have-owner] missing required label, requires all of: owner
[pods-in-default-must-have-owner] regex mismatch
--------------------
kubectl apply -f probe-test-app-pod.yaml
pod/probe-test-app created
--------------------
kubectl delete constraint pods-in-default-must-have-owner
k8srequiredlabels.constraints.gatekeeper.sh "pods-in-default-must-have-owner" deleted
--------------------
kubectl delete pod probe-test-app
pod "probe-test-app" deleted
--------------------
helm repo add argo-helm https://argoproj.github.io/argo-helm
"argo-helm" already exists with the same configuration, skipping
--------------------
helm install argo-cd argo-helm/argo-cd --namespace argocd --create-namespace --version 7.6.1
NAME: argo-cd
LAST DEPLOYED: Sun Nov 24 11:24:24 2024
NAMESPACE: argocd
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
In order to access the server UI you have the following options:

1. kubectl port-forward service/argo-cd-argocd-server -n argocd 8080:443

    and then open the browser on http://localhost:8080 and accept the certificate

2. enable ingress in the values file `server.ingress.enabled` and either
      - Add the annotation for ssl passthrough: https://argo-cd.readthedocs.io/en/stable/operator-manual/ingress/#option-1-ssl-passthrough
      - Set the `configs.params."server.insecure"` in the values file and terminate SSL at your ingress: https://argo-cd.readthedocs.io/en/stable/operator-manual/ingress/#option-2-multiple-ingress-objects-and-hosts


After reaching the UI the first time you can login with username: admin and the random password generated during the installation. You can find the password by running:

kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d

(You should delete the initial secret afterwards as suggested by the Getting Started Guide: https://argo-cd.readthedocs.io/en/stable/getting_started/#4-login-using-the-cli)
--------------------
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath={.data.password} | base64 -d
L8bV86ZRyookd9qR--------------------
cd ../argocd
--------------------
kubectl apply -f probe-test-app.yaml -n argocd
application.argoproj.io/probe-test-app created
--------------------
kubectl apply -f argo-rollouts-app.yaml -n argocd
application.argoproj.io/argo-rollouts created
--------------------
kubectl delete deployment probe-test-app
deployment.apps "probe-test-app" deleted
--------------------
kubectl get pods
NAME                             READY   STATUS        RESTARTS   AGE
probe-test-app-fc7776cb8-bjjfd   0/2     Terminating   0          1s
probe-test-app-fc7776cb8-gk7f6   0/2     Terminating   0          1s
probe-test-app-fc7776cb8-r4nlr   0/2     Terminating   0          1s
rabbitmq-0                       1/1     Running       0          48m
rabbitmq-publish-hn4h4           0/1     Completed     0          41m
--------------------
kubectl delete application probe-test-app -n argocd
application.argoproj.io "probe-test-app" deleted
