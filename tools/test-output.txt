kubectl config get-contexts
CURRENT   NAME             CLUSTER          AUTHINFO         NAMESPACE
*         docker-desktop   docker-desktop   docker-desktop   
--------------------
kubectl get nodes
NAME             STATUS   ROLES           AGE   VERSION
docker-desktop   Ready    control-plane   75s   v1.30.5
--------------------
cd probe-test-app
--------------------
kubectl apply -f probe-test-app-pod.yaml
pod/probe-test-app created
pod/probe-test-app condition met
--------------------
kubectl get pods -o wide
NAME             READY   STATUS    RESTARTS   AGE   IP         NODE             NOMINATED NODE   READINESS GATES
probe-test-app   1/1     Running   0          18s   10.1.0.6   docker-desktop   <none>           <none>
--------------------
kubectl apply -f probe-test-app-service.yaml
service/probe-test-app created
--------------------
kubectl get services -o wide
NAME             TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE   SELECTOR
kubernetes       ClusterIP      10.96.0.1       <none>        443/TCP          95s   <none>
probe-test-app   LoadBalancer   10.99.195.153   localhost     8000:31781/TCP   1s    app.kubernetes.io/name=probe-test-app
--------------------
kubectl get endpoints
NAME             ENDPOINTS           AGE
kubernetes       192.168.65.3:6443   96s
probe-test-app   10.1.0.6:8080       2s
--------------------
kubectl apply -f probe-test-app-pod-2.yaml
pod/probe-test-app-2 created
pod/probe-test-app-2 condition met
--------------------
kubectl get endpoints
NAME             ENDPOINTS                     AGE
kubernetes       192.168.65.3:6443             99s
probe-test-app   10.1.0.6:8080,10.1.0.7:8080   5s
--------------------
kubectl delete pods --all
pod "probe-test-app" deleted
pod "probe-test-app-2" deleted
--------------------
kubectl apply -f probe-test-app-replicaset.yaml
replicaset.apps/probe-test-app created
pod/probe-test-app-bn8zn condition met
pod/probe-test-app-bnwd8 condition met
pod/probe-test-app-zz6tr condition met
--------------------
kubectl scale replicaset probe-test-app --replicas=2
replicaset.apps/probe-test-app scaled
--------------------
kubectl get pods
NAME                   READY   STATUS        RESTARTS   AGE
probe-test-app-bn8zn   1/1     Terminating   0          5s
probe-test-app-bnwd8   1/1     Running       0          5s
probe-test-app-zz6tr   1/1     Running       0          5s
--------------------
kubectl delete replicaset probe-test-app
replicaset.apps "probe-test-app" deleted
--------------------
kubectl apply -f probe-test-app-deployment.yaml
deployment.apps/probe-test-app created
Waiting for deployment "probe-test-app" rollout to finish: 0 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 0 of 3 updated replicas are available...
Waiting for deployment "probe-test-app" rollout to finish: 1 of 3 updated replicas are available...
Waiting for deployment "probe-test-app" rollout to finish: 2 of 3 updated replicas are available...
deployment "probe-test-app" successfully rolled out
--------------------
kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
probe-test-app-fc7776cb8-cw66x   1/1     Running   0          3s
probe-test-app-fc7776cb8-rbxhc   1/1     Running   0          3s
probe-test-app-fc7776cb8-v9mwm   1/1     Running   0          3s
--------------------
kubectl get replicasets
NAME                       DESIRED   CURRENT   READY   AGE
probe-test-app-fc7776cb8   3         3         3       4s
--------------------
kubectl set image deployment/probe-test-app probe-test-app=jasonumiker/probe-test-app:v2
deployment.apps/probe-test-app image updated
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "probe-test-app" rollout to finish: 1 old replicas are pending termination...
deployment "probe-test-app" successfully rolled out
--------------------
kubectl events
LAST SEEN               TYPE     REASON                    OBJECT                                MESSAGE
2m20s                   Normal   Starting                  Node/docker-desktop                   Starting kubelet.
2m20s (x8 over 2m20s)   Normal   NodeHasSufficientMemory   Node/docker-desktop                   Node docker-desktop status is now: NodeHasSufficientMemory
2m20s (x8 over 2m20s)   Normal   NodeHasNoDiskPressure     Node/docker-desktop                   Node docker-desktop status is now: NodeHasNoDiskPressure
2m20s (x7 over 2m20s)   Normal   NodeHasSufficientPID      Node/docker-desktop                   Node docker-desktop status is now: NodeHasSufficientPID
2m20s                   Normal   NodeAllocatableEnforced   Node/docker-desktop                   Updated Node Allocatable limit across pods
2m10s                   Normal   RegisteredNode            Node/docker-desktop                   Node docker-desktop event: Registered Node docker-desktop in Controller
2m8s                    Normal   Starting                  Node/docker-desktop                   
60s                     Normal   Pulling                   Pod/probe-test-app                    Pulling image "jasonumiker/probe-test-app:v1"
60s                     Normal   Scheduled                 Pod/probe-test-app                    Successfully assigned default/probe-test-app to docker-desktop
46s                     Normal   Pulled                    Pod/probe-test-app                    Successfully pulled image "jasonumiker/probe-test-app:v1" in 13.948s (13.948s including waiting). Image size: 383887660 bytes.
44s                     Normal   Created                   Pod/probe-test-app                    Created container probe-test-app
44s                     Normal   Started                   Pod/probe-test-app                    Started container probe-test-app
38s                     Normal   Pulled                    Pod/probe-test-app-2                  Container image "jasonumiker/probe-test-app:v1" already present on machine
38s                     Normal   Scheduled                 Pod/probe-test-app-2                  Successfully assigned default/probe-test-app-2 to docker-desktop
37s                     Normal   Created                   Pod/probe-test-app-2                  Created container probe-test-app
37s                     Normal   Started                   Pod/probe-test-app-2                  Started container probe-test-app
35s                     Normal   Killing                   Pod/probe-test-app                    Stopping container probe-test-app
34s                     Normal   Killing                   Pod/probe-test-app-2                  Stopping container probe-test-app
32s                     Normal   Scheduled                 Pod/probe-test-app-bnwd8              Successfully assigned default/probe-test-app-bnwd8 to docker-desktop
32s                     Normal   SuccessfulCreate          ReplicaSet/probe-test-app             Created pod: probe-test-app-bn8zn
32s                     Normal   SuccessfulCreate          ReplicaSet/probe-test-app             Created pod: probe-test-app-zz6tr
32s                     Normal   Scheduled                 Pod/probe-test-app-zz6tr              Successfully assigned default/probe-test-app-zz6tr to docker-desktop
32s                     Normal   SuccessfulCreate          ReplicaSet/probe-test-app             Created pod: probe-test-app-bnwd8
32s                     Normal   Scheduled                 Pod/probe-test-app-bn8zn              Successfully assigned default/probe-test-app-bn8zn to docker-desktop
31s                     Normal   Pulled                    Pod/probe-test-app-bnwd8              Container image "jasonumiker/probe-test-app:v1" already present on machine
31s                     Normal   Created                   Pod/probe-test-app-bn8zn              Created container probe-test-app
31s                     Normal   Pulled                    Pod/probe-test-app-bn8zn              Container image "jasonumiker/probe-test-app:v1" already present on machine
31s                     Normal   Pulled                    Pod/probe-test-app-zz6tr              Container image "jasonumiker/probe-test-app:v1" already present on machine
30s                     Normal   Started                   Pod/probe-test-app-bnwd8              Started container probe-test-app
30s                     Normal   Created                   Pod/probe-test-app-bnwd8              Created container probe-test-app
30s                     Normal   Created                   Pod/probe-test-app-zz6tr              Created container probe-test-app
30s                     Normal   Started                   Pod/probe-test-app-bn8zn              Started container probe-test-app
30s                     Normal   Started                   Pod/probe-test-app-zz6tr              Started container probe-test-app
28s                     Normal   SuccessfulDelete          ReplicaSet/probe-test-app             Deleted pod: probe-test-app-bn8zn
26s                     Normal   Killing                   Pod/probe-test-app-bnwd8              Stopping container probe-test-app
26s                     Normal   Killing                   Pod/probe-test-app-zz6tr              Stopping container probe-test-app
26s                     Normal   Killing                   Pod/probe-test-app-bn8zn              Stopping container probe-test-app
25s                     Normal   Scheduled                 Pod/probe-test-app-fc7776cb8-v9mwm    Successfully assigned default/probe-test-app-fc7776cb8-v9mwm to docker-desktop
25s                     Normal   Scheduled                 Pod/probe-test-app-fc7776cb8-cw66x    Successfully assigned default/probe-test-app-fc7776cb8-cw66x to docker-desktop
25s                     Normal   ScalingReplicaSet         Deployment/probe-test-app             Scaled up replica set probe-test-app-fc7776cb8 to 3
25s                     Normal   SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8   Created pod: probe-test-app-fc7776cb8-rbxhc
25s                     Normal   SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8   Created pod: probe-test-app-fc7776cb8-cw66x
25s                     Normal   SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8   Created pod: probe-test-app-fc7776cb8-v9mwm
25s                     Normal   Scheduled                 Pod/probe-test-app-fc7776cb8-rbxhc    Successfully assigned default/probe-test-app-fc7776cb8-rbxhc to docker-desktop
24s                     Normal   Started                   Pod/probe-test-app-fc7776cb8-cw66x    Started container probe-test-app
24s                     Normal   Created                   Pod/probe-test-app-fc7776cb8-cw66x    Created container probe-test-app
24s                     Normal   Started                   Pod/probe-test-app-fc7776cb8-rbxhc    Started container probe-test-app
24s                     Normal   Pulled                    Pod/probe-test-app-fc7776cb8-cw66x    Container image "jasonumiker/probe-test-app:v1" already present on machine
24s                     Normal   Pulled                    Pod/probe-test-app-fc7776cb8-rbxhc    Container image "jasonumiker/probe-test-app:v1" already present on machine
24s                     Normal   Pulled                    Pod/probe-test-app-fc7776cb8-v9mwm    Container image "jasonumiker/probe-test-app:v1" already present on machine
24s                     Normal   Created                   Pod/probe-test-app-fc7776cb8-v9mwm    Created container probe-test-app
24s                     Normal   Started                   Pod/probe-test-app-fc7776cb8-v9mwm    Started container probe-test-app
24s                     Normal   Created                   Pod/probe-test-app-fc7776cb8-rbxhc    Created container probe-test-app
19s                     Normal   Pulling                   Pod/probe-test-app-fb95466cc-2j6mn    Pulling image "jasonumiker/probe-test-app:v2"
19s                     Normal   ScalingReplicaSet         Deployment/probe-test-app             Scaled up replica set probe-test-app-fb95466cc to 1
19s                     Normal   SuccessfulCreate          ReplicaSet/probe-test-app-fb95466cc   Created pod: probe-test-app-fb95466cc-2j6mn
19s                     Normal   Scheduled                 Pod/probe-test-app-fb95466cc-2j6mn    Successfully assigned default/probe-test-app-fb95466cc-2j6mn to docker-desktop
15s                     Normal   Pulled                    Pod/probe-test-app-fb95466cc-2j6mn    Successfully pulled image "jasonumiker/probe-test-app:v2" in 3.433s (3.433s including waiting). Image size: 383887659 bytes.
15s                     Normal   Started                   Pod/probe-test-app-fb95466cc-2j6mn    Started container probe-test-app
15s                     Normal   Created                   Pod/probe-test-app-fb95466cc-2j6mn    Created container probe-test-app
14s                     Normal   SuccessfulDelete          ReplicaSet/probe-test-app-fc7776cb8   Deleted pod: probe-test-app-fc7776cb8-v9mwm
14s                     Normal   Scheduled                 Pod/probe-test-app-fb95466cc-ltsqq    Successfully assigned default/probe-test-app-fb95466cc-ltsqq to docker-desktop
14s                     Normal   Killing                   Pod/probe-test-app-fc7776cb8-v9mwm    Stopping container probe-test-app
14s                     Normal   ScalingReplicaSet         Deployment/probe-test-app             Scaled up replica set probe-test-app-fb95466cc to 2 from 1
14s                     Normal   ScalingReplicaSet         Deployment/probe-test-app             Scaled down replica set probe-test-app-fc7776cb8 to 2 from 3
14s                     Normal   SuccessfulCreate          ReplicaSet/probe-test-app-fb95466cc   Created pod: probe-test-app-fb95466cc-ltsqq
13s                     Normal   Pulled                    Pod/probe-test-app-fb95466cc-ltsqq    Container image "jasonumiker/probe-test-app:v2" already present on machine
13s                     Normal   Started                   Pod/probe-test-app-fb95466cc-ltsqq    Started container probe-test-app
13s                     Normal   Created                   Pod/probe-test-app-fb95466cc-ltsqq    Created container probe-test-app
4s                      Normal   SuccessfulDelete          ReplicaSet/probe-test-app-fc7776cb8   Deleted pod: probe-test-app-fc7776cb8-cw66x
4s                      Normal   Scheduled                 Pod/probe-test-app-fb95466cc-5pvpg    Successfully assigned default/probe-test-app-fb95466cc-5pvpg to docker-desktop
4s                      Normal   SuccessfulCreate          ReplicaSet/probe-test-app-fb95466cc   Created pod: probe-test-app-fb95466cc-5pvpg
4s                      Normal   Killing                   Pod/probe-test-app-fc7776cb8-cw66x    Stopping container probe-test-app
4s                      Normal   ScalingReplicaSet         Deployment/probe-test-app             Scaled down replica set probe-test-app-fc7776cb8 to 1 from 2
4s                      Normal   ScalingReplicaSet         Deployment/probe-test-app             Scaled up replica set probe-test-app-fb95466cc to 3 from 2
3s                      Normal   Pulled                    Pod/probe-test-app-fb95466cc-5pvpg    Container image "jasonumiker/probe-test-app:v2" already present on machine
3s                      Normal   Started                   Pod/probe-test-app-fb95466cc-5pvpg    Started container probe-test-app
3s                      Normal   Created                   Pod/probe-test-app-fb95466cc-5pvpg    Created container probe-test-app
2s                      Normal   SuccessfulDelete          ReplicaSet/probe-test-app-fc7776cb8   Deleted pod: probe-test-app-fc7776cb8-rbxhc
2s                      Normal   Killing                   Pod/probe-test-app-fc7776cb8-rbxhc    Stopping container probe-test-app
2s                      Normal   ScalingReplicaSet         Deployment/probe-test-app             Scaled down replica set probe-test-app-fc7776cb8 to 0 from 1
--------------------
kubectl rollout undo deployment/probe-test-app
deployment.apps/probe-test-app rolled back
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "probe-test-app" rollout to finish: 1 old replicas are pending termination...
deployment "probe-test-app" successfully rolled out
--------------------
kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
probe-test-app-fc7776cb8-54jqv   1/1     Running   0          11s
probe-test-app-fc7776cb8-57xvm   1/1     Running   0          16s
probe-test-app-fc7776cb8-lqkks   1/1     Running   0          13s
--------------------
kubectl describe replicaset probe-test-app
Name:           probe-test-app-fb95466cc
Namespace:      default
Selector:       app.kubernetes.io/name=probe-test-app,pod-template-hash=fb95466cc
Labels:         app.kubernetes.io/name=probe-test-app
                pod-template-hash=fb95466cc
Annotations:    deployment.kubernetes.io/desired-replicas: 3
                deployment.kubernetes.io/max-replicas: 4
                deployment.kubernetes.io/revision: 2
Controlled By:  Deployment/probe-test-app
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app.kubernetes.io/name=probe-test-app
           pod-template-hash=fb95466cc
  Containers:
   probe-test-app:
    Image:      jasonumiker/probe-test-app:v2
    Port:       8080/TCP
    Host Port:  0/TCP
    Requests:
      cpu:         50m
      memory:      52Mi
    Liveness:      http-get http://:8080/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:     http-get http://:8080/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  39s   replicaset-controller  Created pod: probe-test-app-fb95466cc-2j6mn
  Normal  SuccessfulCreate  34s   replicaset-controller  Created pod: probe-test-app-fb95466cc-ltsqq
  Normal  SuccessfulCreate  24s   replicaset-controller  Created pod: probe-test-app-fb95466cc-5pvpg
  Normal  SuccessfulDelete  16s   replicaset-controller  Deleted pod: probe-test-app-fb95466cc-5pvpg
  Normal  SuccessfulDelete  14s   replicaset-controller  Deleted pod: probe-test-app-fb95466cc-ltsqq
  Normal  SuccessfulDelete  4s    replicaset-controller  Deleted pod: probe-test-app-fb95466cc-2j6mn

Name:           probe-test-app-fc7776cb8
Namespace:      default
Selector:       app.kubernetes.io/name=probe-test-app,pod-template-hash=fc7776cb8
Labels:         app.kubernetes.io/name=probe-test-app
                pod-template-hash=fc7776cb8
Annotations:    deployment.kubernetes.io/desired-replicas: 3
                deployment.kubernetes.io/max-replicas: 4
                deployment.kubernetes.io/revision: 3
                deployment.kubernetes.io/revision-history: 1
Controlled By:  Deployment/probe-test-app
Replicas:       3 current / 3 desired
Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app.kubernetes.io/name=probe-test-app
           pod-template-hash=fc7776cb8
  Containers:
   probe-test-app:
    Image:      jasonumiker/probe-test-app:v1
    Port:       8080/TCP
    Host Port:  0/TCP
    Requests:
      cpu:         50m
      memory:      52Mi
    Liveness:      http-get http://:8080/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:     http-get http://:8080/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  45s   replicaset-controller  Created pod: probe-test-app-fc7776cb8-v9mwm
  Normal  SuccessfulCreate  45s   replicaset-controller  Created pod: probe-test-app-fc7776cb8-cw66x
  Normal  SuccessfulCreate  45s   replicaset-controller  Created pod: probe-test-app-fc7776cb8-rbxhc
  Normal  SuccessfulDelete  34s   replicaset-controller  Deleted pod: probe-test-app-fc7776cb8-v9mwm
  Normal  SuccessfulDelete  24s   replicaset-controller  Deleted pod: probe-test-app-fc7776cb8-cw66x
  Normal  SuccessfulDelete  22s   replicaset-controller  Deleted pod: probe-test-app-fc7776cb8-rbxhc
  Normal  SuccessfulCreate  19s   replicaset-controller  Created pod: probe-test-app-fc7776cb8-57xvm
  Normal  SuccessfulCreate  16s   replicaset-controller  Created pod: probe-test-app-fc7776cb8-lqkks
  Normal  SuccessfulCreate  14s   replicaset-controller  Created pod: probe-test-app-fc7776cb8-54jqv

--------------------
cd ../sidecar-and-init-containers
--------------------
kubectl apply -f sidecar.yaml
pod/pod-with-sidecar created
pod/pod-with-sidecar condition met
--------------------
kubectl apply -f init.yaml
pod/myapp-pod created
--------------------
kubectl get pod myapp-pod
NAME        READY   STATUS     RESTARTS   AGE
myapp-pod   0/1     Init:0/2   0          1s
--------------------
kubectl apply -f services-init-requires.yaml
service/myservice created
service/mydb created
--------------------
NAME        READY   STATUS    RESTARTS   AGE
myapp-pod   1/1     Running   0          13s
--------------------
pod "myapp-pod" deleted
pod "pod-with-sidecar" deleted
service "myservice" deleted
service "mydb" deleted
--------------------
cd ../pvs-and-statefulsets
--------------------
kubectl apply -f hostpath-provisioner.yaml
deployment.apps/hostpath-provisioner created
storageclass.storage.k8s.io/hostpath-provisioner created
serviceaccount/hostpath-provisioner created
clusterrole.rbac.authorization.k8s.io/hostpath-provisioner created
clusterrolebinding.rbac.authorization.k8s.io/hostpath-provisioner created
--------------------
kubectl get storageclass
NAME                   PROVISIONER            RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
hostpath (default)     docker.io/hostpath     Delete          Immediate              false                  4m3s
hostpath-provisioner   microk8s.io/hostpath   Delete          WaitForFirstConsumer   false                  1s
--------------------
kubectl apply -f pvc.yaml
persistentvolumeclaim/test-pvc created
--------------------
kubectl get pvc
NAME       STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS           VOLUMEATTRIBUTESCLASS   AGE
test-pvc   Pending                                      hostpath-provisioner   <unset>                 1s
--------------------
kubectl apply -f pod.yaml
pod/nginx created
pod/nginx condition met
--------------------
kubectl get pvc
NAME       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS           VOLUMEATTRIBUTESCLASS   AGE
test-pvc   Bound    pvc-968e65f6-16a5-4cdb-904d-06f8232284ef   1Gi        RWO            hostpath-provisioner   <unset>                 13s
--------------------
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS           VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-968e65f6-16a5-4cdb-904d-06f8232284ef   1Gi        RWO            Delete           Bound    default/test-pvc   hostpath-provisioner   <unset>                          6s
--------------------
kubectl apply -f service.yaml
service/nginx created
--------------------
curl http://localhost:8001
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   153  100   153    0     0   155k      0 --:--:-- --:--:-- --:--:--  149k
<html>
<head><title>403 Forbidden</title></head>
<body>
<center><h1>403 Forbidden</h1></center>
<hr><center>nginx/1.27.2</center>
</body>
</html>
--------------------
kubectl exec -it nginx  -- bash -c "echo 'Data on PV' > /usr/share/nginx/html/index.html"
--------------------
curl http://localhost:8001
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100    11  100    11    0     0   3310      0 --:--:-- --:--:-- --:--:--  3666
Data on PV
--------------------
kubectl delete pod nginx
pod "nginx" deleted
--------------------
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS           VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-968e65f6-16a5-4cdb-904d-06f8232284ef   1Gi        RWO            Delete           Bound    default/test-pvc   hostpath-provisioner   <unset>                          24s
--------------------
kubectl apply -f pod.yaml
pod/nginx created
pod/nginx condition met
--------------------
curl http://localhost:8001
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100    11  100    11    0     0  11789      0 --:--:-- --:--:-- --:--:-- 11000
Data on PV
--------------------
kubectl delete service nginx
service "nginx" deleted
--------------------
kubectl delete pod nginx
pod "nginx" deleted
--------------------
kubectl delete pvc test-pvc
persistentvolumeclaim "test-pvc" deleted
--------------------
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM              STORAGECLASS           VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-968e65f6-16a5-4cdb-904d-06f8232284ef   1Gi        RWO            Delete           Released   default/test-pvc   hostpath-provisioner   <unset>                          33s
--------------------
--------------------
kubectl apply -k .
serviceaccount/rabbitmq created
role.rbac.authorization.k8s.io/rabbitmq created
rolebinding.rbac.authorization.k8s.io/rabbitmq created
configmap/rabbitmq-config created
secret/erlang-cookie created
secret/rabbitmq-admin created
service/rabbitmq-client created
service/rabbitmq-headless created
statefulset.apps/rabbitmq created
Waiting for 1 pods to be ready...
partitioned roll out complete: 1 new pods have been updated...
--------------------
kubectl describe statefulset rabbitmq
Name:               rabbitmq
Namespace:          default
CreationTimestamp:  Sat, 23 Nov 2024 12:50:41 +1100
Selector:           app=rabbitmq
Labels:             <none>
Annotations:        <none>
Replicas:           1 desired | 1 total
Update Strategy:    RollingUpdate
  Partition:        0
Pods Status:        1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=rabbitmq
  Service Account:  rabbitmq
  Init Containers:
   rabbitmq-config:
    Image:      busybox:1.37.0
    Port:       <none>
    Host Port:  <none>
    Command:
      sh
      -c
      cp /tmp/rabbitmq/rabbitmq.conf /etc/rabbitmq/rabbitmq.conf && echo '' >> /etc/rabbitmq/rabbitmq.conf; cp /tmp/rabbitmq/enabled_plugins /etc/rabbitmq/enabled_plugins
    Environment:  <none>
    Mounts:
      /etc/rabbitmq from rabbitmq-config-rw (rw)
      /tmp/rabbitmq from rabbitmq-config (rw)
  Containers:
   rabbitmq:
    Image:       rabbitmq:3.8.34
    Ports:       5672/TCP, 15672/TCP, 15692/TCP, 4369/TCP
    Host Ports:  0/TCP, 0/TCP, 0/TCP, 0/TCP
    Liveness:    exec [rabbitmq-diagnostics status] delay=60s timeout=15s period=60s #success=1 #failure=3
    Readiness:   exec [rabbitmq-diagnostics ping] delay=20s timeout=10s period=60s #success=1 #failure=3
    Environment:
      RABBITMQ_DEFAULT_PASS:   <set to the key 'pass' in secret 'rabbitmq-admin'>   Optional: false
      RABBITMQ_DEFAULT_USER:   <set to the key 'user' in secret 'rabbitmq-admin'>   Optional: false
      RABBITMQ_ERLANG_COOKIE:  <set to the key 'cookie' in secret 'erlang-cookie'>  Optional: false
    Mounts:
      /etc/rabbitmq from rabbitmq-config-rw (rw)
      /var/lib/rabbitmq/mnesia from rabbitmq-data (rw)
  Volumes:
   rabbitmq-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      rabbitmq-config
    Optional:  false
   rabbitmq-config-rw:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
   rabbitmq-data:
    Type:          PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:     rabbitmq-data
    ReadOnly:      false
  Node-Selectors:  <none>
  Tolerations:     <none>
Volume Claims:
  Name:          rabbitmq-data
  StorageClass:  hostpath-provisioner
  Labels:        <none>
  Annotations:   <none>
  Capacity:      3Gi
  Access Modes:  [ReadWriteOnce]
Events:
  Type    Reason            Age   From                    Message
  ----    ------            ----  ----                    -------
  Normal  SuccessfulCreate  68s   statefulset-controller  create Claim rabbitmq-data-rabbitmq-0 Pod rabbitmq-0 in StatefulSet rabbitmq success
  Normal  SuccessfulCreate  68s   statefulset-controller  create Pod rabbitmq-0 in StatefulSet rabbitmq successful
--------------------
kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
probe-test-app-fc7776cb8-54jqv   1/1     Running   0          3m43s
probe-test-app-fc7776cb8-57xvm   1/1     Running   0          3m48s
probe-test-app-fc7776cb8-lqkks   1/1     Running   0          3m45s
rabbitmq-0                       1/1     Running   0          69s
--------------------
kubectl get pvc
NAME                       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS           VOLUMEATTRIBUTESCLASS   AGE
rabbitmq-data-rabbitmq-0   Bound    pvc-420722c4-7fd5-4519-b2ba-fcd5d315679d   3Gi        RWO            hostpath-provisioner   <unset>                 70s
--------------------
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                              STORAGECLASS           VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-420722c4-7fd5-4519-b2ba-fcd5d315679d   3Gi        RWO            Delete           Bound    default/rabbitmq-data-rabbitmq-0   hostpath-provisioner   <unset>                          66s
--------------------
kubectl delete pod rabbitmq-0
pod "rabbitmq-0" deleted
--------------------
kubectl get pods
NAME                             READY   STATUS     RESTARTS   AGE
probe-test-app-fc7776cb8-54jqv   1/1     Running    0          3m49s
probe-test-app-fc7776cb8-57xvm   1/1     Running    0          3m54s
probe-test-app-fc7776cb8-lqkks   1/1     Running    0          3m51s
rabbitmq-0                       0/1     Init:0/1   0          1s
--------------------
cd ../../monitoring
--------------------
./install-prometheus.sh

Updating docker-desktop pods to expose metrics endpoints
This will involve several kube-system pod restarts

Fetching debian image to run nsenter on the docker-desktop host...
12.8: Pulling from library/debian
b2b31b28ee3c: Pulling fs layer
b2b31b28ee3c: Download complete
Digest: sha256:10901ccd8d249047f9761845b4594f121edef079cfd8224edebd9ea726f0a7f6
Status: Downloaded newer image for debian:12.8
docker.io/library/debian:12.8
Host Node IP: 192.168.65.3
Updating kube-proxy configmap...
sed: can't read s/metricsBindAddress: 127.0.0.1:10249/metricsBindAddress: 0.0.0.0:10249/g: No such file or directory
configmap "kube-proxy" deleted
configmap/kube-proxy created
Restarting the kube-proxy pod
pod "kube-proxy-z9xdk" deleted
pod/kube-proxy-qfz8s condition met
kube-proxy pod restarted.
Updating bind-address on kube-controller-manager...
Waiting for kube-controller-manager to restart, this can take some time...
pod/kube-controller-manager-docker-desktop condition met
pod/kube-controller-manager-docker-desktop condition met
kube-controller-manager pod restarted.
Updating bind-address on kube-scheduler
Waiting for kube-scheduler to restart, this can take some time...
pod/kube-scheduler-docker-desktop condition met
pod/kube-scheduler-docker-desktop condition met
kube-scheduler pod restarted.
Adding node ip to listen-metrics-urls on etcd
Waiting for etcd to restart, this can take some time...
Error from server (Timeout): the server was unable to return a response in the time allotted, but may still be processing the request (get pods)
pod/etcd-docker-desktop condition met

Done! You can now deploy the monitoring components.

WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/jumiker/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/jumiker/.kube/config
"prometheus-community" already exists with the same configuration, skipping
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/jumiker/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/jumiker/.kube/config
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "ingress-nginx" chart repository
...Successfully got an update from the "kiali" chart repository
...Successfully got an update from the "prometheus-community" chart repository
...Successfully got an update from the "istio" chart repository
...Successfully got an update from the "kedacore" chart repository
Update Complete. ⎈Happy Helming!⎈
namespace/monitoring created
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/jumiker/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/jumiker/.kube/config
NAME: prometheus
LAST DEPLOYED: Sat Nov 23 12:54:28 2024
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
NOTES:
kube-prometheus-stack has been installed. Check its status by running:
  kubectl --namespace monitoring get pods -l "release=prometheus"

Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create & configure Alertmanager and Prometheus instances using the Operator.
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/jumiker/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/jumiker/.kube/config
NAME: adapter
LAST DEPLOYED: Sat Nov 23 12:54:46 2024
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
adapter-prometheus-adapter has been deployed.
In a few minutes you should be able to list metrics using the following command(s):

  kubectl get --raw /apis/metrics.k8s.io/v1beta1
  kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1
Waiting for deployment "adapter-prometheus-adapter" rollout to finish: 0 of 1 updated replicas are available...
deployment "adapter-prometheus-adapter" successfully rolled out
--------------------
kubectl top nodes
NAME             CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
docker-desktop   263m         1%     4299Mi          13%       
--------------------
kubectl top pods
NAME                             CPU(cores)   MEMORY(bytes)   
probe-test-app-fc7776cb8-54jqv   0m           38Mi            
probe-test-app-fc7776cb8-57xvm   0m           38Mi            
probe-test-app-fc7776cb8-lqkks   0m           38Mi            
rabbitmq-0                       0m           142Mi           
--------------------
kubectl top pods -n monitoring
NAME                                                     CPU(cores)   MEMORY(bytes)   
adapter-prometheus-adapter-b84b78594-kbjwh               15m          57Mi            
alertmanager-prometheus-kube-prometheus-alertmanager-0   0m           21Mi            
prometheus-grafana-6b758d7b46-vmjr5                      0m           251Mi           
prometheus-kube-prometheus-operator-c5f7c5b6-kgqs8       0m           31Mi            
prometheus-kube-state-metrics-677845d566-48wcs           0m           22Mi            
prometheus-prometheus-kube-prometheus-prometheus-0       5m           149Mi           
prometheus-prometheus-node-exporter-x5gq7                0m           8Mi             
--------------------
cd ../probe-test-app
--------------------
kubectl apply -f probe-test-app-hpa.yaml
horizontalpodautoscaler.autoscaling/probe-test-app created
--------------------
kubectl apply -f generate-load-app-replicaset.yaml
replicaset.apps/generate-load-app created
--------------------
kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
generate-load-app-7mdqz          1/1     Running   0          32s
generate-load-app-9wzkg          1/1     Running   0          32s
generate-load-app-kkznw          1/1     Running   0          32s
generate-load-app-qwd9l          1/1     Running   0          32s
generate-load-app-tpjgc          1/1     Running   0          32s
probe-test-app-fc7776cb8-54jqv   1/1     Running   0          9m8s
probe-test-app-fc7776cb8-57xvm   1/1     Running   0          9m13s
probe-test-app-fc7776cb8-lqkks   1/1     Running   0          9m10s
rabbitmq-0                       1/1     Running   0          5m20s
--------------------
kubectl delete replicaset generate-load-app
replicaset.apps "generate-load-app" deleted
--------------------
kubectl describe hpa probe-test-app
Name:                                                  probe-test-app
Namespace:                                             default
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Sat, 23 Nov 2024 12:56:42 +1100
Reference:                                             Deployment/probe-test-app
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  52% (26m) / 50%
Min replicas:                                          1
Max replicas:                                          5
Deployment pods:                                       3 current / 3 desired
Conditions:
  Type            Status  Reason              Message
  ----            ------  ------              -------
  AbleToScale     True    ReadyForNewScale    recommended size matches current size
  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)
  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range
Events:           <none>
--------------------
cd ../limit-examples
--------------------
kubectl apply -f cpu-stressor.yaml
deployment.apps/cpu-stressor created
Waiting for deployment "cpu-stressor" rollout to finish: 0 of 1 updated replicas are available...
deployment "cpu-stressor" successfully rolled out
--------------------
kubectl delete deployment cpu-stressor
deployment.apps "cpu-stressor" deleted
--------------------
kubectl apply -f memory-stressor.yaml
pod/memory-stressor created
--------------------
kubectl delete pod memory-stressor
pod "memory-stressor" deleted
--------------------
cd ../keda-example
--------------------
./install-keda.sh
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/jumiker/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/jumiker/.kube/config
"kedacore" already exists with the same configuration, skipping
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/jumiker/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/jumiker/.kube/config
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "kedacore" chart repository
...Successfully got an update from the "ingress-nginx" chart repository
...Successfully got an update from the "kiali" chart repository
...Successfully got an update from the "prometheus-community" chart repository
...Successfully got an update from the "istio" chart repository
Update Complete. ⎈Happy Helming!⎈
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/jumiker/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/jumiker/.kube/config
NAME: keda
LAST DEPLOYED: Sat Nov 23 12:58:14 2024
NAMESPACE: keda
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
:::^.     .::::^:     :::::::::::::::    .:::::::::.                   .^.                  
7???~   .^7????~.     7??????????????.   :?????????77!^.              .7?7.                 
7???~  ^7???7~.       ~!!!!!!!!!!!!!!.   :????!!!!7????7~.           .7???7.                
7???~^7????~.                            :????:    :~7???7.         :7?????7.               
7???7????!.           ::::::::::::.      :????:      .7???!        :7??77???7.              
7????????7:           7???????????~      :????:       :????:      :???7?5????7.             
7????!~????^          !77777777777^      :????:       :????:     ^???7?#P7????7.            
7???~  ^????~                            :????:      :7???!     ^???7J#@J7?????7.           
7???~   :7???!.                          :????:   .:~7???!.    ~???7Y&@#7777????7.          
7???~    .7???7:      !!!!!!!!!!!!!!!    :????7!!77????7^     ~??775@@@GJJYJ?????7.         
7???~     .!????^     7?????????????7.   :?????????7!~:      !????G@@@@@@@@5??????7:        
::::.       :::::     :::::::::::::::    .::::::::..        .::::JGGGB@@@&7:::::::::        
                                                                      ?@@#~                  
                                                                      P@B^                   
                                                                    :&G:                    
                                                                    !5.                     
                                                                    .Kubernetes Event-driven Autoscaling (KEDA) - Application autoscaling made simple.

Get started by deploying Scaled Objects to your cluster:
    - Information about Scaled Objects : https://keda.sh/docs/latest/concepts/
    - Samples: https://github.com/kedacore/samples

Get information about the deployed ScaledObjects:
  kubectl get scaledobject [--namespace <namespace>]

Get details about a deployed ScaledObject:
  kubectl describe scaledobject <scaled-object-name> [--namespace <namespace>]

Get information about the deployed ScaledObjects:
  kubectl get triggerauthentication [--namespace <namespace>]

Get details about a deployed ScaledObject:
  kubectl describe triggerauthentication <trigger-authentication-name> [--namespace <namespace>]

Get an overview of the Horizontal Pod Autoscalers (HPA) that KEDA is using behind the scenes:
  kubectl get hpa [--all-namespaces] [--namespace <namespace>]

Learn more about KEDA:
- Documentation: https://keda.sh/
- Support: https://keda.sh/support/
- File an issue: https://github.com/kedacore/keda/issues/new/choose
--------------------
kubectl apply -f consumer.yaml
secret/rabbitmq-consumer-secret created
deployment.apps/rabbitmq-consumer created
--------------------
kubectl apply -f keda-scaled-object.yaml
scaledobject.keda.sh/rabbitmq-consumer created
triggerauthentication.keda.sh/rabbitmq-consumer-trigger created
--------------------
kubectl apply -f publisher.yaml
job.batch/rabbitmq-publish created
--------------------
kubectl get pods
NAME                                READY   STATUS              RESTARTS   AGE
probe-test-app-fc7776cb8-54jqv      1/1     Running             0          10m
probe-test-app-fc7776cb8-57xvm      1/1     Running             0          10m
probe-test-app-fc7776cb8-lqkks      1/1     Running             0          10m
rabbitmq-0                          1/1     Running             0          6m34s
rabbitmq-consumer-dd9d7cfd4-7zgn2   0/1     ContainerCreating   0          3s
rabbitmq-publish-vbv5z              0/1     ContainerCreating   0          1s
--------------------
kubectl events
LAST SEEN               TYPE      REASON                    OBJECT                                           MESSAGE
12m                     Normal    Starting                  Node/docker-desktop                              Starting kubelet.
12m                     Normal    NodeAllocatableEnforced   Node/docker-desktop                              Updated Node Allocatable limit across pods
12m (x7 over 12m)       Normal    NodeHasSufficientPID      Node/docker-desktop                              Node docker-desktop status is now: NodeHasSufficientPID
12m (x8 over 12m)       Normal    NodeHasNoDiskPressure     Node/docker-desktop                              Node docker-desktop status is now: NodeHasNoDiskPressure
12m (x8 over 12m)       Normal    NodeHasSufficientMemory   Node/docker-desktop                              Node docker-desktop status is now: NodeHasSufficientMemory
12m                     Normal    RegisteredNode            Node/docker-desktop                              Node docker-desktop event: Registered Node docker-desktop in Controller
12m                     Normal    Starting                  Node/docker-desktop                              
11m                     Normal    Pulling                   Pod/probe-test-app                               Pulling image "jasonumiker/probe-test-app:v1"
11m                     Normal    Scheduled                 Pod/probe-test-app                               Successfully assigned default/probe-test-app to docker-desktop
11m                     Normal    Pulled                    Pod/probe-test-app                               Successfully pulled image "jasonumiker/probe-test-app:v1" in 13.948s (13.948s including waiting). Image size: 383887660 bytes.
11m                     Normal    Created                   Pod/probe-test-app                               Created container probe-test-app
11m                     Normal    Started                   Pod/probe-test-app                               Started container probe-test-app
11m                     Normal    Pulled                    Pod/probe-test-app-2                             Container image "jasonumiker/probe-test-app:v1" already present on machine
11m                     Normal    Scheduled                 Pod/probe-test-app-2                             Successfully assigned default/probe-test-app-2 to docker-desktop
11m                     Normal    Created                   Pod/probe-test-app-2                             Created container probe-test-app
11m                     Normal    Started                   Pod/probe-test-app-2                             Started container probe-test-app
11m                     Normal    Killing                   Pod/probe-test-app                               Stopping container probe-test-app
11m                     Normal    Killing                   Pod/probe-test-app-2                             Stopping container probe-test-app
11m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app                        Created pod: probe-test-app-bn8zn
11m                     Normal    Scheduled                 Pod/probe-test-app-zz6tr                         Successfully assigned default/probe-test-app-zz6tr to docker-desktop
11m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app                        Created pod: probe-test-app-zz6tr
11m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app                        Created pod: probe-test-app-bnwd8
11m                     Normal    Scheduled                 Pod/probe-test-app-bnwd8                         Successfully assigned default/probe-test-app-bnwd8 to docker-desktop
11m                     Normal    Scheduled                 Pod/probe-test-app-bn8zn                         Successfully assigned default/probe-test-app-bn8zn to docker-desktop
11m                     Normal    Pulled                    Pod/probe-test-app-bnwd8                         Container image "jasonumiker/probe-test-app:v1" already present on machine
11m                     Normal    Created                   Pod/probe-test-app-bn8zn                         Created container probe-test-app
11m                     Normal    Pulled                    Pod/probe-test-app-bn8zn                         Container image "jasonumiker/probe-test-app:v1" already present on machine
11m                     Normal    Pulled                    Pod/probe-test-app-zz6tr                         Container image "jasonumiker/probe-test-app:v1" already present on machine
10m                     Normal    Created                   Pod/probe-test-app-zz6tr                         Created container probe-test-app
10m                     Normal    Created                   Pod/probe-test-app-bnwd8                         Created container probe-test-app
10m                     Normal    Started                   Pod/probe-test-app-zz6tr                         Started container probe-test-app
10m                     Normal    Started                   Pod/probe-test-app-bnwd8                         Started container probe-test-app
10m                     Normal    Started                   Pod/probe-test-app-bn8zn                         Started container probe-test-app
10m                     Normal    SuccessfulDelete          ReplicaSet/probe-test-app                        Deleted pod: probe-test-app-bn8zn
10m                     Normal    Killing                   Pod/probe-test-app-bnwd8                         Stopping container probe-test-app
10m                     Normal    Killing                   Pod/probe-test-app-bn8zn                         Stopping container probe-test-app
10m                     Normal    Killing                   Pod/probe-test-app-zz6tr                         Stopping container probe-test-app
10m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8              Created pod: probe-test-app-fc7776cb8-cw66x
10m                     Normal    Scheduled                 Pod/probe-test-app-fc7776cb8-v9mwm               Successfully assigned default/probe-test-app-fc7776cb8-v9mwm to docker-desktop
10m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled up replica set probe-test-app-fc7776cb8 to 3
10m                     Normal    Scheduled                 Pod/probe-test-app-fc7776cb8-cw66x               Successfully assigned default/probe-test-app-fc7776cb8-cw66x to docker-desktop
10m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8              Created pod: probe-test-app-fc7776cb8-v9mwm
10m                     Normal    Scheduled                 Pod/probe-test-app-fc7776cb8-rbxhc               Successfully assigned default/probe-test-app-fc7776cb8-rbxhc to docker-desktop
10m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8              Created pod: probe-test-app-fc7776cb8-rbxhc
10m                     Normal    Created                   Pod/probe-test-app-fc7776cb8-v9mwm               Created container probe-test-app
10m                     Normal    Created                   Pod/probe-test-app-fc7776cb8-cw66x               Created container probe-test-app
10m                     Normal    Pulled                    Pod/probe-test-app-fc7776cb8-cw66x               Container image "jasonumiker/probe-test-app:v1" already present on machine
10m                     Normal    Started                   Pod/probe-test-app-fc7776cb8-cw66x               Started container probe-test-app
10m                     Normal    Started                   Pod/probe-test-app-fc7776cb8-v9mwm               Started container probe-test-app
10m                     Normal    Pulled                    Pod/probe-test-app-fc7776cb8-v9mwm               Container image "jasonumiker/probe-test-app:v1" already present on machine
10m                     Normal    Started                   Pod/probe-test-app-fc7776cb8-rbxhc               Started container probe-test-app
10m                     Normal    Created                   Pod/probe-test-app-fc7776cb8-rbxhc               Created container probe-test-app
10m                     Normal    Pulled                    Pod/probe-test-app-fc7776cb8-rbxhc               Container image "jasonumiker/probe-test-app:v1" already present on machine
10m                     Normal    Pulling                   Pod/probe-test-app-fb95466cc-2j6mn               Pulling image "jasonumiker/probe-test-app:v2"
10m                     Normal    Scheduled                 Pod/probe-test-app-fb95466cc-2j6mn               Successfully assigned default/probe-test-app-fb95466cc-2j6mn to docker-desktop
10m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled up replica set probe-test-app-fb95466cc to 1
10m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fb95466cc              Created pod: probe-test-app-fb95466cc-2j6mn
10m                     Normal    Started                   Pod/probe-test-app-fb95466cc-2j6mn               Started container probe-test-app
10m                     Normal    Created                   Pod/probe-test-app-fb95466cc-2j6mn               Created container probe-test-app
10m                     Normal    Pulled                    Pod/probe-test-app-fb95466cc-2j6mn               Successfully pulled image "jasonumiker/probe-test-app:v2" in 3.433s (3.433s including waiting). Image size: 383887659 bytes.
10m                     Normal    Killing                   Pod/probe-test-app-fc7776cb8-v9mwm               Stopping container probe-test-app
10m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled up replica set probe-test-app-fb95466cc to 2 from 1
10m                     Normal    Scheduled                 Pod/probe-test-app-fb95466cc-ltsqq               Successfully assigned default/probe-test-app-fb95466cc-ltsqq to docker-desktop
10m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled down replica set probe-test-app-fc7776cb8 to 2 from 3
10m                     Normal    SuccessfulDelete          ReplicaSet/probe-test-app-fc7776cb8              Deleted pod: probe-test-app-fc7776cb8-v9mwm
10m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fb95466cc              Created pod: probe-test-app-fb95466cc-ltsqq
10m                     Normal    Pulled                    Pod/probe-test-app-fb95466cc-ltsqq               Container image "jasonumiker/probe-test-app:v2" already present on machine
10m                     Normal    Started                   Pod/probe-test-app-fb95466cc-ltsqq               Started container probe-test-app
10m                     Normal    Created                   Pod/probe-test-app-fb95466cc-ltsqq               Created container probe-test-app
10m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fb95466cc              Created pod: probe-test-app-fb95466cc-5pvpg
10m                     Normal    SuccessfulDelete          ReplicaSet/probe-test-app-fc7776cb8              Deleted pod: probe-test-app-fc7776cb8-cw66x
10m                     Normal    Killing                   Pod/probe-test-app-fc7776cb8-cw66x               Stopping container probe-test-app
10m                     Normal    Scheduled                 Pod/probe-test-app-fb95466cc-5pvpg               Successfully assigned default/probe-test-app-fb95466cc-5pvpg to docker-desktop
10m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled down replica set probe-test-app-fc7776cb8 to 1 from 2
10m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled up replica set probe-test-app-fb95466cc to 3 from 2
10m                     Normal    Started                   Pod/probe-test-app-fb95466cc-5pvpg               Started container probe-test-app
10m                     Normal    Pulled                    Pod/probe-test-app-fb95466cc-5pvpg               Container image "jasonumiker/probe-test-app:v2" already present on machine
10m                     Normal    Created                   Pod/probe-test-app-fb95466cc-5pvpg               Created container probe-test-app
10m                     Normal    Killing                   Pod/probe-test-app-fc7776cb8-rbxhc               Stopping container probe-test-app
10m                     Normal    SuccessfulDelete          ReplicaSet/probe-test-app-fc7776cb8              Deleted pod: probe-test-app-fc7776cb8-rbxhc
10m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled down replica set probe-test-app-fc7776cb8 to 0 from 1
10m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled up replica set probe-test-app-fc7776cb8 to 1 from 0
10m                     Normal    Scheduled                 Pod/probe-test-app-fc7776cb8-57xvm               Successfully assigned default/probe-test-app-fc7776cb8-57xvm to docker-desktop
10m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8              Created pod: probe-test-app-fc7776cb8-57xvm
10m                     Normal    Created                   Pod/probe-test-app-fc7776cb8-57xvm               Created container probe-test-app
10m                     Normal    Started                   Pod/probe-test-app-fc7776cb8-57xvm               Started container probe-test-app
10m                     Warning   Unhealthy                 Pod/probe-test-app-fc7776cb8-57xvm               Readiness probe failed: Get "http://10.1.0.17:8080/readyz": dial tcp 10.1.0.17:8080: connect: connection refused
10m                     Normal    Pulled                    Pod/probe-test-app-fc7776cb8-57xvm               Container image "jasonumiker/probe-test-app:v1" already present on machine
10m                     Normal    ScalingReplicaSet         Deployment/probe-test-app                        Scaled down replica set probe-test-app-fb95466cc to 2 from 3
10m                     Normal    Killing                   Pod/probe-test-app-fb95466cc-5pvpg               Stopping container probe-test-app
10m                     Normal    Scheduled                 Pod/probe-test-app-fc7776cb8-lqkks               Successfully assigned default/probe-test-app-fc7776cb8-lqkks to docker-desktop
10m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8              Created pod: probe-test-app-fc7776cb8-lqkks
10m                     Normal    Pulled                    Pod/probe-test-app-fc7776cb8-lqkks               Container image "jasonumiker/probe-test-app:v1" already present on machine
10m                     Normal    Created                   Pod/probe-test-app-fc7776cb8-lqkks               Created container probe-test-app
10m                     Normal    SuccessfulDelete          ReplicaSet/probe-test-app-fb95466cc              Deleted pod: probe-test-app-fb95466cc-5pvpg
10m                     Warning   Unhealthy                 Pod/probe-test-app-fc7776cb8-lqkks               Readiness probe failed: Get "http://10.1.0.18:8080/readyz": dial tcp 10.1.0.18:8080: connect: connection refused
10m                     Normal    Started                   Pod/probe-test-app-fc7776cb8-lqkks               Started container probe-test-app
10m                     Normal    SuccessfulDelete          ReplicaSet/probe-test-app-fb95466cc              Deleted pod: probe-test-app-fb95466cc-ltsqq
10m                     Normal    Scheduled                 Pod/probe-test-app-fc7776cb8-54jqv               Successfully assigned default/probe-test-app-fc7776cb8-54jqv to docker-desktop
10m                     Normal    SuccessfulCreate          ReplicaSet/probe-test-app-fc7776cb8              Created pod: probe-test-app-fc7776cb8-54jqv
10m                     Normal    Killing                   Pod/probe-test-app-fb95466cc-ltsqq               Stopping container probe-test-app
10m                     Normal    Pulled                    Pod/probe-test-app-fc7776cb8-54jqv               Container image "jasonumiker/probe-test-app:v1" already present on machine
10m                     Normal    Created                   Pod/probe-test-app-fc7776cb8-54jqv               Created container probe-test-app
10m                     Normal    Started                   Pod/probe-test-app-fc7776cb8-54jqv               Started container probe-test-app
10m (x4 over 10m)       Normal    ScalingReplicaSet         Deployment/probe-test-app                        (combined from similar events): Scaled down replica set probe-test-app-fb95466cc to 0 from 1
10m                     Normal    SuccessfulDelete          ReplicaSet/probe-test-app-fb95466cc              Deleted pod: probe-test-app-fb95466cc-2j6mn
10m                     Normal    Killing                   Pod/probe-test-app-fb95466cc-2j6mn               Stopping container probe-test-app
10m                     Normal    Scheduled                 Pod/pod-with-sidecar                             Successfully assigned default/pod-with-sidecar to docker-desktop
10m                     Normal    Pulling                   Pod/pod-with-sidecar                             Pulling image "alpine:3.20.3"
10m                     Normal    Pulling                   Pod/pod-with-sidecar                             Pulling image "nginx:1.27.2-bookworm"
10m                     Normal    Started                   Pod/pod-with-sidecar                             Started container app-container
10m                     Normal    Created                   Pod/pod-with-sidecar                             Created container app-container
10m                     Normal    Pulled                    Pod/pod-with-sidecar                             Successfully pulled image "alpine:3.20.3" in 3.776s (3.776s including waiting). Image size: 3634744 bytes.
9m56s                   Normal    Pulled                    Pod/pod-with-sidecar                             Successfully pulled image "nginx:1.27.2-bookworm" in 5.732s (5.732s including waiting). Image size: 72955450 bytes.
9m55s                   Normal    Created                   Pod/pod-with-sidecar                             Created container sidecar-container
9m55s                   Normal    Started                   Pod/pod-with-sidecar                             Started container sidecar-container
9m52s                   Normal    Scheduled                 Pod/myapp-pod                                    Successfully assigned default/myapp-pod to docker-desktop
9m52s                   Normal    Pulling                   Pod/myapp-pod                                    Pulling image "busybox:1.28"
9m48s                   Normal    Started                   Pod/myapp-pod                                    Started container init-mydb
9m48s                   Normal    Created                   Pod/myapp-pod                                    Created container init-mydb
9m48s                   Normal    Pulled                    Pod/myapp-pod                                    Container image "busybox:1.28" already present on machine
9m48s                   Normal    Started                   Pod/myapp-pod                                    Started container init-myservice
9m48s                   Normal    Created                   Pod/myapp-pod                                    Created container init-myservice
9m48s                   Normal    Pulled                    Pod/myapp-pod                                    Successfully pulled image "busybox:1.28" in 3.313s (3.313s including waiting). Image size: 727869 bytes.
9m47s                   Normal    Pulled                    Pod/myapp-pod                                    Container image "busybox:1.28" already present on machine
9m47s                   Normal    Created                   Pod/myapp-pod                                    Created container myapp-container
9m46s                   Normal    Started                   Pod/myapp-pod                                    Started container myapp-container
9m38s                   Normal    Killing                   Pod/myapp-pod                                    Stopping container myapp-container
9m7s                    Normal    Killing                   Pod/pod-with-sidecar                             Stopping container sidecar-container
9m7s                    Normal    Killing                   Pod/pod-with-sidecar                             Stopping container app-container
8m32s                   Normal    WaitForFirstConsumer      PersistentVolumeClaim/test-pvc                   waiting for first consumer to be created before binding
8m30s                   Normal    ExternalProvisioning      PersistentVolumeClaim/test-pvc                   Waiting for a volume to be created either by the external provisioner 'microk8s.io/hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
8m29s                   Normal    Provisioning              PersistentVolumeClaim/test-pvc                   External provisioner is provisioning volume for claim "default/test-pvc"
8m24s                   Normal    ProvisioningSucceeded     PersistentVolumeClaim/test-pvc                   Successfully provisioned volume pvc-968e65f6-16a5-4cdb-904d-06f8232284ef
8m24s                   Normal    Scheduled                 Pod/nginx                                        Successfully assigned default/nginx to docker-desktop
8m23s                   Normal    Pulling                   Pod/nginx                                        Pulling image "nginx:1.27.2"
8m21s                   Normal    Pulled                    Pod/nginx                                        Successfully pulled image "nginx:1.27.2" in 1.662s (1.662s including waiting). Image size: 72955450 bytes.
8m21s                   Normal    Started                   Pod/nginx                                        Started container nginx
8m21s                   Normal    Created                   Pod/nginx                                        Created container nginx
8m3s                    Normal    Killing                   Pod/nginx                                        Stopping container nginx
7m59s                   Normal    Scheduled                 Pod/nginx                                        Successfully assigned default/nginx to docker-desktop
7m59s                   Normal    Pulled                    Pod/nginx                                        Container image "nginx:1.27.2" already present on machine
7m58s                   Normal    Started                   Pod/nginx                                        Started container nginx
7m58s                   Normal    Created                   Pod/nginx                                        Created container nginx
7m55s                   Normal    Killing                   Pod/nginx                                        Stopping container nginx
7m49s (x2 over 7m49s)   Normal    ExternalProvisioning      PersistentVolumeClaim/rabbitmq-data-rabbitmq-0   Waiting for a volume to be created either by the external provisioner 'microk8s.io/hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
7m49s                   Normal    WaitForFirstConsumer      PersistentVolumeClaim/rabbitmq-data-rabbitmq-0   waiting for first consumer to be created before binding
7m49s                   Normal    Provisioning              PersistentVolumeClaim/rabbitmq-data-rabbitmq-0   External provisioner is provisioning volume for claim "default/rabbitmq-data-rabbitmq-0"
7m49s                   Normal    SuccessfulCreate          StatefulSet/rabbitmq                             create Claim rabbitmq-data-rabbitmq-0 Pod rabbitmq-0 in StatefulSet rabbitmq success
7m44s                   Normal    ProvisioningSucceeded     PersistentVolumeClaim/rabbitmq-data-rabbitmq-0   Successfully provisioned volume pvc-420722c4-7fd5-4519-b2ba-fcd5d315679d
7m43s                   Normal    Scheduled                 Pod/rabbitmq-0                                   Successfully assigned default/rabbitmq-0 to docker-desktop
7m42s                   Normal    Pulling                   Pod/rabbitmq-0                                   Pulling image "busybox:1.37.0"
7m39s                   Normal    Pulled                    Pod/rabbitmq-0                                   Successfully pulled image "busybox:1.37.0" in 3.506s (3.506s including waiting). Image size: 2167126 bytes.
7m38s                   Normal    Pulling                   Pod/rabbitmq-0                                   Pulling image "rabbitmq:3.8.34"
7m38s                   Normal    Created                   Pod/rabbitmq-0                                   Created container rabbitmq-config
7m38s                   Normal    Started                   Pod/rabbitmq-0                                   Started container rabbitmq-config
7m32s                   Normal    Pulled                    Pod/rabbitmq-0                                   Successfully pulled image "rabbitmq:3.8.34" in 6.241s (6.241s including waiting). Image size: 99409513 bytes.
7m32s                   Normal    Created                   Pod/rabbitmq-0                                   Created container rabbitmq
7m31s                   Normal    Started                   Pod/rabbitmq-0                                   Started container rabbitmq
<unknown>               Normal    Created                   RabbitMQ/pod/rabbitmq-0                          Node rabbit@rabbitmq-0 is registered
6m37s                   Normal    Killing                   Pod/rabbitmq-0                                   Stopping container rabbitmq
6m35s (x2 over 7m49s)   Normal    SuccessfulCreate          StatefulSet/rabbitmq                             create Pod rabbitmq-0 in StatefulSet rabbitmq successful
6m35s                   Normal    Scheduled                 Pod/rabbitmq-0                                   Successfully assigned default/rabbitmq-0 to docker-desktop
6m34s                   Normal    Created                   Pod/rabbitmq-0                                   Created container rabbitmq
6m34s                   Normal    Pulled                    Pod/rabbitmq-0                                   Container image "busybox:1.37.0" already present on machine
6m34s                   Normal    Created                   Pod/rabbitmq-0                                   Created container rabbitmq-config
6m34s                   Normal    Started                   Pod/rabbitmq-0                                   Started container rabbitmq-config
6m34s                   Normal    Pulled                    Pod/rabbitmq-0                                   Container image "rabbitmq:3.8.34" already present on machine
6m34s                   Normal    Started                   Pod/rabbitmq-0                                   Started container rabbitmq
<unknown>               Normal    Created                   RabbitMQ/pod/rabbitmq-0                          Node rabbit@rabbitmq-0 is registered
6m25s                   Normal    Starting                  Node/docker-desktop                              
6m7s                    Normal    RegisteredNode            Node/docker-desktop                              Node docker-desktop event: Registered Node docker-desktop in Controller
107s                    Normal    SuccessfulCreate          ReplicaSet/generate-load-app                     Created pod: generate-load-app-tpjgc
107s                    Normal    SuccessfulCreate          ReplicaSet/generate-load-app                     Created pod: generate-load-app-kkznw
107s                    Normal    SuccessfulCreate          ReplicaSet/generate-load-app                     Created pod: generate-load-app-7mdqz
107s                    Normal    SuccessfulCreate          ReplicaSet/generate-load-app                     Created pod: generate-load-app-qwd9l
107s                    Normal    SuccessfulCreate          ReplicaSet/generate-load-app                     Created pod: generate-load-app-9wzkg
106s                    Normal    Scheduled                 Pod/generate-load-app-9wzkg                      Successfully assigned default/generate-load-app-9wzkg to docker-desktop
106s                    Normal    Scheduled                 Pod/generate-load-app-7mdqz                      Successfully assigned default/generate-load-app-7mdqz to docker-desktop
106s                    Normal    Scheduled                 Pod/generate-load-app-qwd9l                      Successfully assigned default/generate-load-app-qwd9l to docker-desktop
106s                    Normal    Scheduled                 Pod/generate-load-app-kkznw                      Successfully assigned default/generate-load-app-kkznw to docker-desktop
106s                    Normal    Scheduled                 Pod/generate-load-app-tpjgc                      Successfully assigned default/generate-load-app-tpjgc to docker-desktop
106s                    Normal    Pulled                    Pod/generate-load-app-9wzkg                      Container image "busybox:1.37.0" already present on machine
105s                    Normal    Started                   Pod/generate-load-app-tpjgc                      Started container generate-load-app
105s                    Normal    Created                   Pod/generate-load-app-kkznw                      Created container generate-load-app
105s                    Normal    Created                   Pod/generate-load-app-tpjgc                      Created container generate-load-app
105s                    Normal    Started                   Pod/generate-load-app-qwd9l                      Started container generate-load-app
105s                    Normal    Started                   Pod/generate-load-app-7mdqz                      Started container generate-load-app
105s                    Normal    Pulled                    Pod/generate-load-app-tpjgc                      Container image "busybox:1.37.0" already present on machine
105s                    Normal    Pulled                    Pod/generate-load-app-qwd9l                      Container image "busybox:1.37.0" already present on machine
105s                    Normal    Created                   Pod/generate-load-app-7mdqz                      Created container generate-load-app
105s                    Normal    Started                   Pod/generate-load-app-kkznw                      Started container generate-load-app
105s                    Normal    Pulled                    Pod/generate-load-app-kkznw                      Container image "busybox:1.37.0" already present on machine
105s                    Normal    Pulled                    Pod/generate-load-app-7mdqz                      Container image "busybox:1.37.0" already present on machine
105s                    Normal    Created                   Pod/generate-load-app-9wzkg                      Created container generate-load-app
105s                    Normal    Created                   Pod/generate-load-app-qwd9l                      Created container generate-load-app
105s                    Normal    Started                   Pod/generate-load-app-9wzkg                      Started container generate-load-app
74s                     Normal    Killing                   Pod/generate-load-app-7mdqz                      Stopping container generate-load-app
74s                     Normal    Killing                   Pod/generate-load-app-9wzkg                      Stopping container generate-load-app
74s                     Normal    Killing                   Pod/generate-load-app-kkznw                      Stopping container generate-load-app
74s                     Normal    Killing                   Pod/generate-load-app-tpjgc                      Stopping container generate-load-app
74s                     Normal    Killing                   Pod/generate-load-app-qwd9l                      Stopping container generate-load-app
41s                     Normal    Pulling                   Pod/cpu-stressor-8556c54f68-gmkwr                Pulling image "narmidm/k8s-pod-cpu-stressor:v1.2.0"
41s                     Normal    SuccessfulCreate          ReplicaSet/cpu-stressor-8556c54f68               Created pod: cpu-stressor-8556c54f68-gmkwr
41s                     Normal    ScalingReplicaSet         Deployment/cpu-stressor                          Scaled up replica set cpu-stressor-8556c54f68 to 1
41s                     Normal    Scheduled                 Pod/cpu-stressor-8556c54f68-gmkwr                Successfully assigned default/cpu-stressor-8556c54f68-gmkwr to docker-desktop
37s                     Normal    Started                   Pod/cpu-stressor-8556c54f68-gmkwr                Started container cpu-stressor
37s                     Normal    Created                   Pod/cpu-stressor-8556c54f68-gmkwr                Created container cpu-stressor
37s                     Normal    Pulled                    Pod/cpu-stressor-8556c54f68-gmkwr                Successfully pulled image "narmidm/k8s-pod-cpu-stressor:v1.2.0" in 3.204s (3.204s including waiting). Image size: 4851746 bytes.
34s                     Normal    Killing                   Pod/cpu-stressor-8556c54f68-gmkwr                Stopping container cpu-stressor
33s                     Normal    Pulling                   Pod/memory-stressor                              Pulling image "polinux/stress:1.0.4"
33s                     Normal    Scheduled                 Pod/memory-stressor                              Successfully assigned default/memory-stressor to docker-desktop
29s                     Normal    Pulled                    Pod/memory-stressor                              Successfully pulled image "polinux/stress:1.0.4" in 3.397s (3.397s including waiting). Image size: 4041495 bytes.
28s                     Normal    Pulled                    Pod/memory-stressor                              Container image "polinux/stress:1.0.4" already present on machine
28s (x2 over 29s)       Normal    Created                   Pod/memory-stressor                              Created container memory-stressor
28s (x2 over 29s)       Normal    Started                   Pod/memory-stressor                              Started container memory-stressor
26s (x2 over 27s)       Warning   BackOff                   Pod/memory-stressor                              Back-off restarting failed container memory-stressor in pod memory-stressor_default(c456b799-06c3-4dc9-997d-9bbefbc3d15f)
4s                      Normal    ScalingReplicaSet         Deployment/rabbitmq-consumer                     Scaled up replica set rabbitmq-consumer-dd9d7cfd4 to 1
4s                      Normal    SuccessfulCreate          ReplicaSet/rabbitmq-consumer-dd9d7cfd4           Created pod: rabbitmq-consumer-dd9d7cfd4-7zgn2
4s                      Normal    Pulling                   Pod/rabbitmq-consumer-dd9d7cfd4-7zgn2            Pulling image "ghcr.io/kedacore/rabbitmq-client:v1.0"
4s                      Normal    Scheduled                 Pod/rabbitmq-consumer-dd9d7cfd4-7zgn2            Successfully assigned default/rabbitmq-consumer-dd9d7cfd4-7zgn2 to docker-desktop
2s                      Normal    SuccessfulCreate          Job/rabbitmq-publish                             Created pod: rabbitmq-publish-vbv5z
2s                      Normal    Pulling                   Pod/rabbitmq-publish-vbv5z                       Pulling image "ghcr.io/kedacore/rabbitmq-client:v1.0"
2s                      Normal    Scheduled                 Pod/rabbitmq-publish-vbv5z                       Successfully assigned default/rabbitmq-publish-vbv5z to docker-desktop
--------------------
kubectl describe job rabbitmq-publish
Name:             rabbitmq-publish
Namespace:        default
Selector:         batch.kubernetes.io/controller-uid=da94fb64-b773-4210-86e5-049cf0c85bcc
Labels:           batch.kubernetes.io/controller-uid=da94fb64-b773-4210-86e5-049cf0c85bcc
                  batch.kubernetes.io/job-name=rabbitmq-publish
                  controller-uid=da94fb64-b773-4210-86e5-049cf0c85bcc
                  job-name=rabbitmq-publish
Annotations:      <none>
Parallelism:      1
Completions:      1
Completion Mode:  NonIndexed
Suspend:          false
Backoff Limit:    4
Start Time:       Sat, 23 Nov 2024 12:58:28 +1100
Pods Statuses:    1 Active (0 Ready) / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  batch.kubernetes.io/controller-uid=da94fb64-b773-4210-86e5-049cf0c85bcc
           batch.kubernetes.io/job-name=rabbitmq-publish
           controller-uid=da94fb64-b773-4210-86e5-049cf0c85bcc
           job-name=rabbitmq-publish
  Containers:
   rabbitmq-client:
    Image:      ghcr.io/kedacore/rabbitmq-client:v1.0
    Port:       <none>
    Host Port:  <none>
    Command:
      send
      $(rabbitmq_host)
      300
    Environment:
      rabbitmq_host:  <set to the key 'host' in secret 'rabbitmq-consumer-secret'>  Optional: false
    Mounts:           <none>
  Volumes:            <none>
  Node-Selectors:     <none>
  Tolerations:        <none>
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  3s    job-controller  Created pod: rabbitmq-publish-vbv5z
--------------------
cd ../cronjob
--------------------
kubectl apply -f cronjob.yaml
cronjob.batch/hello created
--------------------
kubectl get pods
NAME                                READY   STATUS      RESTARTS   AGE
hello-28872119-tncl6                0/1     Completed   0          99s
hello-28872120-mv7zg                0/1     Completed   0          39s
probe-test-app-fc7776cb8-54jqv      1/1     Running     0          12m
probe-test-app-fc7776cb8-57xvm      1/1     Running     0          12m
probe-test-app-fc7776cb8-gp9bc      1/1     Running     0          117s
probe-test-app-fc7776cb8-lqkks      1/1     Running     0          12m
rabbitmq-0                          1/1     Running     0          8m44s
rabbitmq-consumer-dd9d7cfd4-7zgn2   1/1     Running     0          2m13s
rabbitmq-consumer-dd9d7cfd4-p8fgn   1/1     Running     0          47s
rabbitmq-consumer-dd9d7cfd4-rc756   1/1     Running     0          47s
rabbitmq-consumer-dd9d7cfd4-tk8fb   1/1     Running     0          47s
rabbitmq-publish-vbv5z              0/1     Completed   0          2m11s
--------------------
kubectl get cronjob
NAME    SCHEDULE    TIMEZONE   SUSPEND   ACTIVE   LAST SCHEDULE   AGE
hello   * * * * *   <none>     False     0        40s             2m7s
--------------------
kubectl delete cronjob hello
cronjob.batch "hello" deleted
--------------------
kubectl get pods -A
NAMESPACE     NAME                                                     READY   STATUS      RESTARTS        AGE
default       probe-test-app-fc7776cb8-54jqv                           1/1     Running     0               12m
default       probe-test-app-fc7776cb8-57xvm                           1/1     Running     0               12m
default       probe-test-app-fc7776cb8-gp9bc                           1/1     Running     0               2m1s
default       probe-test-app-fc7776cb8-lqkks                           1/1     Running     0               12m
default       rabbitmq-0                                               1/1     Running     0               8m48s
default       rabbitmq-consumer-dd9d7cfd4-7zgn2                        1/1     Running     0               2m17s
default       rabbitmq-consumer-dd9d7cfd4-p8fgn                        1/1     Running     0               51s
default       rabbitmq-consumer-dd9d7cfd4-rc756                        1/1     Running     0               51s
default       rabbitmq-consumer-dd9d7cfd4-tk8fb                        1/1     Running     0               51s
default       rabbitmq-publish-vbv5z                                   0/1     Completed   0               2m15s
keda          keda-admission-webhooks-6b7b75c487-lqdqr                 1/1     Running     1 (2m16s ago)   2m29s
keda          keda-operator-86846bb678-bxfh9                           1/1     Running     1 (2m21s ago)   2m29s
keda          keda-operator-metrics-apiserver-5b677c7769-6hccl         1/1     Running     1 (2m11s ago)   2m29s
kube-system   coredns-55cb58b774-d2jwk                                 1/1     Running     0               14m
kube-system   coredns-55cb58b774-xw9fb                                 1/1     Running     0               14m
kube-system   etcd-docker-desktop                                      1/1     Running     0               6m29s
kube-system   hostpath-provisioner-6bb9769b5f-5xqk8                    1/1     Running     1 (7m36s ago)   10m
kube-system   kube-apiserver-docker-desktop                            1/1     Running     0               14m
kube-system   kube-controller-manager-docker-desktop                   1/1     Running     0               8m22s
kube-system   kube-proxy-qfz8s                                         1/1     Running     0               8m39s
kube-system   kube-scheduler-docker-desktop                            1/1     Running     1 (7m31s ago)   7m59s
kube-system   storage-provisioner                                      1/1     Running     1 (7m36s ago)   14m
kube-system   vpnkit-controller                                        1/1     Running     0               14m
monitoring    adapter-prometheus-adapter-b84b78594-kbjwh               1/1     Running     0               5m57s
monitoring    alertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running     0               5m44s
monitoring    prometheus-grafana-6b758d7b46-vmjr5                      3/3     Running     0               6m3s
monitoring    prometheus-kube-prometheus-operator-c5f7c5b6-kgqs8       1/1     Running     0               6m3s
monitoring    prometheus-kube-state-metrics-677845d566-48wcs           1/1     Running     0               6m3s
monitoring    prometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running     0               5m44s
monitoring    prometheus-prometheus-node-exporter-x5gq7                1/1     Running     0               6m3s
--------------------
kubectl api-resources
NAME                                SHORTNAMES               APIVERSION                        NAMESPACED   KIND
bindings                                                     v1                                true         Binding
componentstatuses                   cs                       v1                                false        ComponentStatus
configmaps                          cm                       v1                                true         ConfigMap
endpoints                           ep                       v1                                true         Endpoints
events                              ev                       v1                                true         Event
limitranges                         limits                   v1                                true         LimitRange
namespaces                          ns                       v1                                false        Namespace
nodes                               no                       v1                                false        Node
persistentvolumeclaims              pvc                      v1                                true         PersistentVolumeClaim
persistentvolumes                   pv                       v1                                false        PersistentVolume
pods                                po                       v1                                true         Pod
podtemplates                                                 v1                                true         PodTemplate
replicationcontrollers              rc                       v1                                true         ReplicationController
resourcequotas                      quota                    v1                                true         ResourceQuota
secrets                                                      v1                                true         Secret
serviceaccounts                     sa                       v1                                true         ServiceAccount
services                            svc                      v1                                true         Service
mutatingwebhookconfigurations                                admissionregistration.k8s.io/v1   false        MutatingWebhookConfiguration
validatingadmissionpolicies                                  admissionregistration.k8s.io/v1   false        ValidatingAdmissionPolicy
validatingadmissionpolicybindings                            admissionregistration.k8s.io/v1   false        ValidatingAdmissionPolicyBinding
validatingwebhookconfigurations                              admissionregistration.k8s.io/v1   false        ValidatingWebhookConfiguration
customresourcedefinitions           crd,crds                 apiextensions.k8s.io/v1           false        CustomResourceDefinition
apiservices                                                  apiregistration.k8s.io/v1         false        APIService
controllerrevisions                                          apps/v1                           true         ControllerRevision
daemonsets                          ds                       apps/v1                           true         DaemonSet
deployments                         deploy                   apps/v1                           true         Deployment
replicasets                         rs                       apps/v1                           true         ReplicaSet
statefulsets                        sts                      apps/v1                           true         StatefulSet
selfsubjectreviews                                           authentication.k8s.io/v1          false        SelfSubjectReview
tokenreviews                                                 authentication.k8s.io/v1          false        TokenReview
localsubjectaccessreviews                                    authorization.k8s.io/v1           true         LocalSubjectAccessReview
selfsubjectaccessreviews                                     authorization.k8s.io/v1           false        SelfSubjectAccessReview
selfsubjectrulesreviews                                      authorization.k8s.io/v1           false        SelfSubjectRulesReview
subjectaccessreviews                                         authorization.k8s.io/v1           false        SubjectAccessReview
horizontalpodautoscalers            hpa                      autoscaling/v2                    true         HorizontalPodAutoscaler
cronjobs                            cj                       batch/v1                          true         CronJob
jobs                                                         batch/v1                          true         Job
certificatesigningrequests          csr                      certificates.k8s.io/v1            false        CertificateSigningRequest
leases                                                       coordination.k8s.io/v1            true         Lease
endpointslices                                               discovery.k8s.io/v1               true         EndpointSlice
cloudeventsources                                            eventing.keda.sh/v1alpha1         true         CloudEventSource
clustercloudeventsources                                     eventing.keda.sh/v1alpha1         false        ClusterCloudEventSource
events                              ev                       events.k8s.io/v1                  true         Event
flowschemas                                                  flowcontrol.apiserver.k8s.io/v1   false        FlowSchema
prioritylevelconfigurations                                  flowcontrol.apiserver.k8s.io/v1   false        PriorityLevelConfiguration
clustertriggerauthentications       cta,clustertriggerauth   keda.sh/v1alpha1                  false        ClusterTriggerAuthentication
scaledjobs                          sj                       keda.sh/v1alpha1                  true         ScaledJob
scaledobjects                       so                       keda.sh/v1alpha1                  true         ScaledObject
triggerauthentications              ta,triggerauth           keda.sh/v1alpha1                  true         TriggerAuthentication
nodes                                                        metrics.k8s.io/v1beta1            false        NodeMetrics
pods                                                         metrics.k8s.io/v1beta1            true         PodMetrics
alertmanagerconfigs                 amcfg                    monitoring.coreos.com/v1alpha1    true         AlertmanagerConfig
alertmanagers                       am                       monitoring.coreos.com/v1          true         Alertmanager
podmonitors                         pmon                     monitoring.coreos.com/v1          true         PodMonitor
probes                              prb                      monitoring.coreos.com/v1          true         Probe
prometheusagents                    promagent                monitoring.coreos.com/v1alpha1    true         PrometheusAgent
prometheuses                        prom                     monitoring.coreos.com/v1          true         Prometheus
prometheusrules                     promrule                 monitoring.coreos.com/v1          true         PrometheusRule
scrapeconfigs                       scfg                     monitoring.coreos.com/v1alpha1    true         ScrapeConfig
servicemonitors                     smon                     monitoring.coreos.com/v1          true         ServiceMonitor
thanosrulers                        ruler                    monitoring.coreos.com/v1          true         ThanosRuler
ingressclasses                                               networking.k8s.io/v1              false        IngressClass
ingresses                           ing                      networking.k8s.io/v1              true         Ingress
networkpolicies                     netpol                   networking.k8s.io/v1              true         NetworkPolicy
runtimeclasses                                               node.k8s.io/v1                    false        RuntimeClass
poddisruptionbudgets                pdb                      policy/v1                         true         PodDisruptionBudget
clusterrolebindings                                          rbac.authorization.k8s.io/v1      false        ClusterRoleBinding
clusterroles                                                 rbac.authorization.k8s.io/v1      false        ClusterRole
rolebindings                                                 rbac.authorization.k8s.io/v1      true         RoleBinding
roles                                                        rbac.authorization.k8s.io/v1      true         Role
priorityclasses                     pc                       scheduling.k8s.io/v1              false        PriorityClass
csidrivers                                                   storage.k8s.io/v1                 false        CSIDriver
csinodes                                                     storage.k8s.io/v1                 false        CSINode
csistoragecapacities                                         storage.k8s.io/v1                 true         CSIStorageCapacity
storageclasses                      sc                       storage.k8s.io/v1                 false        StorageClass
volumeattachments                                            storage.k8s.io/v1                 false        VolumeAttachment
--------------------
kubectl get clusterrole admin -o yaml
aggregationRule:
  clusterRoleSelectors:
  - matchLabels:
      rbac.authorization.k8s.io/aggregate-to-admin: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  creationTimestamp: "2024-11-23T01:45:45Z"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: admin
  resourceVersion: "323"
  uid: fc45765a-9381-4bc4-8e8f-367df613f942
rules:
- apiGroups:
  - ""
  resources:
  - pods/attach
  - pods/exec
  - pods/portforward
  - pods/proxy
  - secrets
  - services/proxy
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - serviceaccounts
  verbs:
  - impersonate
- apiGroups:
  - ""
  resources:
  - pods
  - pods/attach
  - pods/exec
  - pods/portforward
  - pods/proxy
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - ""
  resources:
  - pods/eviction
  verbs:
  - create
- apiGroups:
  - ""
  resources:
  - configmaps
  - events
  - persistentvolumeclaims
  - replicationcontrollers
  - replicationcontrollers/scale
  - secrets
  - serviceaccounts
  - services
  - services/proxy
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - ""
  resources:
  - serviceaccounts/token
  verbs:
  - create
- apiGroups:
  - apps
  resources:
  - daemonsets
  - deployments
  - deployments/rollback
  - deployments/scale
  - replicasets
  - replicasets/scale
  - statefulsets
  - statefulsets/scale
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - autoscaling
  resources:
  - horizontalpodautoscalers
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - batch
  resources:
  - cronjobs
  - jobs
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - extensions
  resources:
  - daemonsets
  - deployments
  - deployments/rollback
  - deployments/scale
  - ingresses
  - networkpolicies
  - replicasets
  - replicasets/scale
  - replicationcontrollers/scale
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  - networkpolicies
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - ""
  resources:
  - configmaps
  - endpoints
  - persistentvolumeclaims
  - persistentvolumeclaims/status
  - pods
  - replicationcontrollers
  - replicationcontrollers/scale
  - serviceaccounts
  - services
  - services/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - bindings
  - events
  - limitranges
  - namespaces/status
  - pods/log
  - pods/status
  - replicationcontrollers/status
  - resourcequotas
  - resourcequotas/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - discovery.k8s.io
  resources:
  - endpointslices
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - apps
  resources:
  - controllerrevisions
  - daemonsets
  - daemonsets/status
  - deployments
  - deployments/scale
  - deployments/status
  - replicasets
  - replicasets/scale
  - replicasets/status
  - statefulsets
  - statefulsets/scale
  - statefulsets/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - autoscaling
  resources:
  - horizontalpodautoscalers
  - horizontalpodautoscalers/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - batch
  resources:
  - cronjobs
  - cronjobs/status
  - jobs
  - jobs/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - extensions
  resources:
  - daemonsets
  - daemonsets/status
  - deployments
  - deployments/scale
  - deployments/status
  - ingresses
  - ingresses/status
  - networkpolicies
  - replicasets
  - replicasets/scale
  - replicasets/status
  - replicationcontrollers/scale
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets
  - poddisruptionbudgets/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  - ingresses/status
  - networkpolicies
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - authorization.k8s.io
  resources:
  - localsubjectaccessreviews
  verbs:
  - create
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  - rolebindings
  - roles
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch
--------------------
kubectl get clusterrole admin -o yaml | wc -l
315
--------------------
cd ../k8s-authz
--------------------
./setup-tokens-on-cluster.sh
--------------------
./add-users-kubeconfig.sh
Context "docker-desktop-jane" created.
Context "docker-desktop-john" created.
--------------------
cat team1.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: "team1"
  labels:
    name: "team1"
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: admin
  namespace: team1
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["*"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: admin
  namespace: team1
subjects:
- kind: User
  name: jane
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: admin
  apiGroup: rbac.authorization.k8s.io--------------------
kubectl apply -f team1.yaml && kubectl apply -f team2.yaml
namespace/team1 created
role.rbac.authorization.k8s.io/admin created
rolebinding.rbac.authorization.k8s.io/admin created
namespace/team2 created
role.rbac.authorization.k8s.io/admin created
rolebinding.rbac.authorization.k8s.io/admin created
--------------------
kubectl config get-contexts
CURRENT   NAME                  CLUSTER          AUTHINFO         NAMESPACE
*         docker-desktop        docker-desktop   docker-desktop   
          docker-desktop-jane   docker-desktop   jane             team1
          docker-desktop-john   docker-desktop   john             team2
--------------------
kubectl config use-context docker-desktop-jane
Switched to context "docker-desktop-jane".
--------------------
kubectl get pods -A
Error from server (Forbidden): pods is forbidden: User "jane" cannot list resource "pods" in API group "" at the cluster scope
--------------------
kubectl get pods
No resources found in team1 namespace.
--------------------
kubectl config use-context docker-desktop-john
Switched to context "docker-desktop-john".
--------------------
kubectl get pods
No resources found in team2 namespace.
--------------------
kubectl get pods --namespace=team1
Error from server (Forbidden): pods is forbidden: User "john" cannot list resource "pods" in API group "" in the namespace "team1"
--------------------
kubectl config use-context docker-desktop
Switched to context "docker-desktop".
--------------------
cd ../ingress
--------------------
./install-nginx.sh
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/jumiker/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/jumiker/.kube/config
"ingress-nginx" already exists with the same configuration, skipping
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/jumiker/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/jumiker/.kube/config
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "kedacore" chart repository
...Successfully got an update from the "ingress-nginx" chart repository
...Successfully got an update from the "kiali" chart repository
...Successfully got an update from the "prometheus-community" chart repository
...Successfully got an update from the "istio" chart repository
Update Complete. ⎈Happy Helming!⎈
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/jumiker/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/jumiker/.kube/config
NAME: ingress
LAST DEPLOYED: Sat Nov 23 13:01:42 2024
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
The ingress-nginx controller has been installed.
It may take a few minutes for the load balancer IP to be available.
You can watch the status by running 'kubectl get service --namespace default ingress-ingress-nginx-controller --output wide --watch'

An example Ingress that makes use of the controller:
  apiVersion: networking.k8s.io/v1
  kind: Ingress
  metadata:
    name: example
    namespace: foo
  spec:
    ingressClassName: nginx
    rules:
      - host: www.example.com
        http:
          paths:
            - pathType: Prefix
              backend:
                service:
                  name: exampleService
                  port:
                    number: 80
              path: /
    # This section is only required if TLS is to be enabled for the Ingress
    tls:
      - hosts:
        - www.example.com
        secretName: example-tls

If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided:

  apiVersion: v1
  kind: Secret
  metadata:
    name: example-tls
    namespace: foo
  data:
    tls.crt: <base64 encoded cert>
    tls.key: <base64 encoded key>
  type: kubernetes.io/tls
--------------------
kubectl apply -f probe-test-app-ingress.yaml
ingress.networking.k8s.io/probe-test-app created
--------------------
curl http://localhost
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   577  100   577    0     0   287k      0 --:--:-- --:--:-- --:--:--  563k
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>probetest</title>
</head>
<body>
    <p>This Flask app is served from probe-test-app-fc7776cb8-lqkks</p>
    <p>The readiness endpoint (/readyz) is Healthy</p>
    <p>The liveness endpoint is (/livez) Healthy</p>
    <form method="POST">
        <button type="submit" name="readyz">Toggle Readiness</button>
        <button type="submit" name="livez">Toggle Liveness</button>
    </form>
    <p>Version 1</p>
</body>
</html>--------------------
kubectl apply -f nyancat.yaml
deployment.apps/nyancat created
service/nyancat created
--------------------
kubectl rollout status deployment nyancat -n default
Waiting for deployment "nyancat" rollout to finish: 0 of 1 updated replicas are available...
deployment "nyancat" successfully rolled out
--------------------
kubectl apply -f nyancat-ingress.yaml
ingress.networking.k8s.io/probe-test-app configured
--------------------
curl http://localhost/nyancat/
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   207  100   207    0     0   106k      0 --:--:-- --:--:-- --:--:--  202k
<!doctype html>
<html lang=en>
<title>404 Not Found</title>
<h1>Not Found</h1>
<p>The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.</p>
--------------------
kubectl delete ingress probe-test-app
ingress.networking.k8s.io "probe-test-app" deleted
--------------------
helm uninstall ingress
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/jumiker/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/jumiker/.kube/config
release "ingress" uninstalled
--------------------
cd ../istio
--------------------
./install-istio.sh
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/jumiker/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/jumiker/.kube/config
"istio" already exists with the same configuration, skipping
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/jumiker/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/jumiker/.kube/config
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "kedacore" chart repository
...Successfully got an update from the "ingress-nginx" chart repository
...Successfully got an update from the "kiali" chart repository
...Successfully got an update from the "prometheus-community" chart repository
...Successfully got an update from the "istio" chart repository
Update Complete. ⎈Happy Helming!⎈
namespace/istio-system created
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/jumiker/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/jumiker/.kube/config
NAME: istio-base
LAST DEPLOYED: Sat Nov 23 13:03:10 2024
NAMESPACE: istio-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Istio base successfully installed!

To learn more about the release, try:
  $ helm status istio-base -n istio-system
  $ helm get all istio-base -n istio-system
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/jumiker/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/jumiker/.kube/config
NAME: istiod
LAST DEPLOYED: Sat Nov 23 13:03:11 2024
NAMESPACE: istio-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
"istiod" successfully installed!

To learn more about the release, try:
  $ helm status istiod -n istio-system
  $ helm get all istiod -n istio-system

Next steps:
  * Deploy a Gateway: https://istio.io/latest/docs/setup/additional-setup/gateway/
  * Try out our tasks to get started on common configurations:
    * https://istio.io/latest/docs/tasks/traffic-management
    * https://istio.io/latest/docs/tasks/security/
    * https://istio.io/latest/docs/tasks/policy-enforcement/
  * Review the list of actively supported releases, CVE publications and our hardening guide:
    * https://istio.io/latest/docs/releases/supported-releases/
    * https://istio.io/latest/news/security/
    * https://istio.io/latest/docs/ops/best-practices/security/

For further documentation see https://istio.io website
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/jumiker/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/jumiker/.kube/config
"kiali" already exists with the same configuration, skipping
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/jumiker/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/jumiker/.kube/config
NAME: kiali-server
LAST DEPLOYED: Sat Nov 23 13:03:20 2024
NAMESPACE: istio-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Welcome to Kiali! For more details on Kiali, see: https://kiali.io

The Kiali Server [v2.1.0] has been installed in namespace [istio-system]. It will be ready soon.

When installing with "deployment.cluster_wide_access=false" using this Kiali Server Helm Chart,
it is your responsibility to manually create the proper Roles and RoleBindings for the Kiali Server
to have the correct permissions to access the service mesh namespaces.

(Helm: Chart=[kiali-server], Release=[kiali-server], Version=[2.1.0])
servicemonitor.monitoring.coreos.com/prometheus-oper-istio-controlplane created
servicemonitor.monitoring.coreos.com/prometheus-oper-istio-dataplane created
customresourcedefinition.apiextensions.k8s.io/gatewayclasses.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/gateways.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/grpcroutes.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/httproutes.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/referencegrants.gateway.networking.k8s.io created
--------------------
kubectl label namespace default istio-injection=enabled
namespace/default labeled
--------------------
kubectl apply -f bookinfo/platform/kube/bookinfo.yaml
service/details created
serviceaccount/bookinfo-details created
deployment.apps/details-v1 created
service/ratings created
serviceaccount/bookinfo-ratings created
deployment.apps/ratings-v1 created
service/reviews created
serviceaccount/bookinfo-reviews created
deployment.apps/reviews-v1 created
deployment.apps/reviews-v2 created
deployment.apps/reviews-v3 created
service/productpage created
serviceaccount/bookinfo-productpage created
deployment.apps/productpage-v1 created
--------------------
kubectl apply -f bookinfo/gateway-api/bookinfo-gateway.yaml
gateway.gateway.networking.k8s.io/bookinfo-gateway created
httproute.gateway.networking.k8s.io/bookinfo created
--------------------
kubectl apply -f bookinfo/platform/kube/bookinfo-versions.yaml
service/reviews-v1 created
service/reviews-v2 created
service/reviews-v3 created
service/productpage-v1 created
service/ratings-v1 created
service/details-v1 created
--------------------
kubectl apply -f bookinfo/gateway-api/route-all-v1.yaml
httproute.gateway.networking.k8s.io/reviews created
httproute.gateway.networking.k8s.io/productpage created
httproute.gateway.networking.k8s.io/ratings created
httproute.gateway.networking.k8s.io/details created
--------------------
kubectl apply -f bookinfo/gateway-api/route-reviews-90-10.yaml
httproute.gateway.networking.k8s.io/reviews configured
--------------------
kubectl apply -f bookinfo/gateway-api/route-jason-v2.yaml
httproute.gateway.networking.k8s.io/reviews configured
