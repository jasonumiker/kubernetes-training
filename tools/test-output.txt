kubectl config get-contexts
CURRENT   NAME             CLUSTER          AUTHINFO         NAMESPACE
*         docker-desktop   docker-desktop   docker-desktop   
--------------------
kubectl get nodes
NAME             STATUS   ROLES           AGE     VERSION
docker-desktop   Ready    control-plane   4h14m   v1.32.2
--------------------
cd probe-test-app
--------------------
kubectl apply -f probe-test-app-pod.yaml
pod/probe-test-app created
pod/probe-test-app condition met
--------------------
kubectl get pods -o wide
NAME             READY   STATUS    RESTARTS   AGE   IP         NODE             NOMINATED NODE   READINESS GATES
probe-test-app   1/1     Running   0          32s   10.1.0.6   docker-desktop   <none>           <none>
--------------------
kubectl apply -f probe-test-app-service.yaml
service/probe-test-app created
--------------------
kubectl get services -o wide
NAME             TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE     SELECTOR
kubernetes       ClusterIP      10.96.0.1       <none>        443/TCP          4h15m   <none>
probe-test-app   LoadBalancer   10.107.172.39   localhost     8000:31515/TCP   1s      app.kubernetes.io/name=probe-test-app
--------------------
kubectl get endpoints
NAME             ENDPOINTS           AGE
kubernetes       192.168.65.3:6443   4h15m
probe-test-app   10.1.0.6:8080       2s
--------------------
kubectl apply -f probe-test-app-pod-2.yaml
pod/probe-test-app-2 created
pod/probe-test-app-2 condition met
--------------------
kubectl get endpoints
NAME             ENDPOINTS                     AGE
kubernetes       192.168.65.3:6443             4h15m
probe-test-app   10.1.0.6:8080,10.1.0.7:8080   6s
--------------------
kubectl delete pods --all
pod "probe-test-app" deleted
pod "probe-test-app-2" deleted
--------------------
kubectl apply -f probe-test-app-replicaset.yaml
replicaset.apps/probe-test-app created
pod/probe-test-app-4pldg condition met
pod/probe-test-app-f2tc6 condition met
pod/probe-test-app-r92q4 condition met
--------------------
kubectl scale replicaset probe-test-app --replicas=2
replicaset.apps/probe-test-app scaled
--------------------
kubectl get pods
NAME                   READY   STATUS        RESTARTS   AGE
probe-test-app-4pldg   1/1     Running       0          5s
probe-test-app-f2tc6   1/1     Running       0          5s
probe-test-app-r92q4   1/1     Terminating   0          5s
--------------------
kubectl delete replicaset probe-test-app
replicaset.apps "probe-test-app" deleted
--------------------
kubectl apply -f probe-test-app-deployment.yaml
deployment.apps/probe-test-app created
Waiting for deployment "probe-test-app" rollout to finish: 0 of 3 updated replicas are available...
Waiting for deployment "probe-test-app" rollout to finish: 1 of 3 updated replicas are available...
Waiting for deployment "probe-test-app" rollout to finish: 2 of 3 updated replicas are available...
deployment "probe-test-app" successfully rolled out
--------------------
kubectl get pods
NAME                              READY   STATUS    RESTARTS   AGE
probe-test-app-685956d4cf-nklgs   1/1     Running   0          3s
probe-test-app-685956d4cf-pnhb2   1/1     Running   0          3s
probe-test-app-685956d4cf-xvq4q   1/1     Running   0          3s
--------------------
kubectl get replicasets
NAME                        DESIRED   CURRENT   READY   AGE
probe-test-app-685956d4cf   3         3         3       4s
--------------------
kubectl set image deployment/probe-test-app probe-test-app=jasonumiker/probe-test-app:v2
deployment.apps/probe-test-app image updated
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "probe-test-app" rollout to finish: 1 old replicas are pending termination...
deployment "probe-test-app" successfully rolled out
--------------------
kubectl events
LAST SEEN   TYPE      REASON              OBJECT                                 MESSAGE
78s         Normal    Scheduled           Pod/probe-test-app                     Successfully assigned default/probe-test-app to docker-desktop
77s         Normal    Pulling             Pod/probe-test-app                     Pulling image "mirror.gcr.io/jasonumiker/probe-test-app:v1"
50s         Normal    Pulled              Pod/probe-test-app                     Successfully pulled image "mirror.gcr.io/jasonumiker/probe-test-app:v1" in 27.439s (27.439s including waiting). Image size: 1024950162 bytes.
49s         Normal    Created             Pod/probe-test-app                     Created container: probe-test-app
48s         Normal    Started             Pod/probe-test-app                     Started container probe-test-app
48s         Warning   Unhealthy           Pod/probe-test-app                     Readiness probe failed: Get "http://10.1.0.6:8080/readyz": dial tcp 10.1.0.6:8080: connect: connection refused
41s         Normal    Scheduled           Pod/probe-test-app-2                   Successfully assigned default/probe-test-app-2 to docker-desktop
41s         Normal    Pulled              Pod/probe-test-app-2                   Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
41s         Normal    Created             Pod/probe-test-app-2                   Created container: probe-test-app
41s         Normal    Started             Pod/probe-test-app-2                   Started container probe-test-app
38s         Normal    Killing             Pod/probe-test-app                     Stopping container probe-test-app
37s         Normal    Killing             Pod/probe-test-app-2                   Stopping container probe-test-app
35s         Normal    SuccessfulCreate    ReplicaSet/probe-test-app              Created pod: probe-test-app-f2tc6
35s         Normal    Scheduled           Pod/probe-test-app-r92q4               Successfully assigned default/probe-test-app-r92q4 to docker-desktop
35s         Normal    Scheduled           Pod/probe-test-app-4pldg               Successfully assigned default/probe-test-app-4pldg to docker-desktop
35s         Normal    SuccessfulCreate    ReplicaSet/probe-test-app              Created pod: probe-test-app-4pldg
35s         Normal    Scheduled           Pod/probe-test-app-f2tc6               Successfully assigned default/probe-test-app-f2tc6 to docker-desktop
35s         Normal    SuccessfulCreate    ReplicaSet/probe-test-app              Created pod: probe-test-app-r92q4
34s         Normal    Pulled              Pod/probe-test-app-4pldg               Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
34s         Normal    Pulled              Pod/probe-test-app-r92q4               Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
34s         Normal    Pulled              Pod/probe-test-app-f2tc6               Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
34s         Normal    Created             Pod/probe-test-app-f2tc6               Created container: probe-test-app
34s         Warning   Unhealthy           Pod/probe-test-app-4pldg               Readiness probe failed: Get "http://10.1.0.9:8080/readyz": dial tcp 10.1.0.9:8080: connect: connection refused
34s         Normal    Started             Pod/probe-test-app-4pldg               Started container probe-test-app
34s         Normal    Created             Pod/probe-test-app-4pldg               Created container: probe-test-app
34s         Normal    Started             Pod/probe-test-app-f2tc6               Started container probe-test-app
34s         Warning   Unhealthy           Pod/probe-test-app-f2tc6               Readiness probe failed: Get "http://10.1.0.8:8080/readyz": dial tcp 10.1.0.8:8080: connect: connection refused
34s         Normal    Created             Pod/probe-test-app-r92q4               Created container: probe-test-app
34s         Normal    Started             Pod/probe-test-app-r92q4               Started container probe-test-app
31s         Normal    SuccessfulDelete    ReplicaSet/probe-test-app              Deleted pod: probe-test-app-r92q4
30s         Normal    Killing             Pod/probe-test-app-r92q4               Stopping container probe-test-app
29s         Normal    Killing             Pod/probe-test-app-4pldg               Stopping container probe-test-app
29s         Normal    Killing             Pod/probe-test-app-f2tc6               Stopping container probe-test-app
28s         Normal    SuccessfulCreate    ReplicaSet/probe-test-app-685956d4cf   Created pod: probe-test-app-685956d4cf-pnhb2
28s         Normal    ScalingReplicaSet   Deployment/probe-test-app              Scaled up replica set probe-test-app-685956d4cf from 0 to 3
28s         Normal    SuccessfulCreate    ReplicaSet/probe-test-app-685956d4cf   Created pod: probe-test-app-685956d4cf-xvq4q
28s         Normal    SuccessfulCreate    ReplicaSet/probe-test-app-685956d4cf   Created pod: probe-test-app-685956d4cf-nklgs
28s         Normal    Pulled              Pod/probe-test-app-685956d4cf-nklgs    Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
28s         Normal    Scheduled           Pod/probe-test-app-685956d4cf-nklgs    Successfully assigned default/probe-test-app-685956d4cf-nklgs to docker-desktop
28s         Normal    Scheduled           Pod/probe-test-app-685956d4cf-xvq4q    Successfully assigned default/probe-test-app-685956d4cf-xvq4q to docker-desktop
28s         Normal    Scheduled           Pod/probe-test-app-685956d4cf-pnhb2    Successfully assigned default/probe-test-app-685956d4cf-pnhb2 to docker-desktop
27s         Normal    Started             Pod/probe-test-app-685956d4cf-pnhb2    Started container probe-test-app
27s         Normal    Created             Pod/probe-test-app-685956d4cf-nklgs    Created container: probe-test-app
27s         Normal    Started             Pod/probe-test-app-685956d4cf-nklgs    Started container probe-test-app
27s         Normal    Pulled              Pod/probe-test-app-685956d4cf-pnhb2    Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
27s         Normal    Created             Pod/probe-test-app-685956d4cf-pnhb2    Created container: probe-test-app
27s         Normal    Pulled              Pod/probe-test-app-685956d4cf-xvq4q    Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
27s         Normal    Created             Pod/probe-test-app-685956d4cf-xvq4q    Created container: probe-test-app
27s         Normal    Started             Pod/probe-test-app-685956d4cf-xvq4q    Started container probe-test-app
27s         Warning   Unhealthy           Pod/probe-test-app-685956d4cf-xvq4q    Readiness probe failed: Get "http://10.1.0.13:8080/readyz": dial tcp 10.1.0.13:8080: connect: connection refused
23s         Normal    ScalingReplicaSet   Deployment/probe-test-app              Scaled up replica set probe-test-app-68d99fdc94 from 0 to 1
23s         Normal    SuccessfulCreate    ReplicaSet/probe-test-app-68d99fdc94   Created pod: probe-test-app-68d99fdc94-98mg4
23s         Normal    Scheduled           Pod/probe-test-app-68d99fdc94-98mg4    Successfully assigned default/probe-test-app-68d99fdc94-98mg4 to docker-desktop
22s         Normal    Pulling             Pod/probe-test-app-68d99fdc94-98mg4    Pulling image "jasonumiker/probe-test-app:v2"
17s         Normal    Created             Pod/probe-test-app-68d99fdc94-98mg4    Created container: probe-test-app
17s         Normal    Pulled              Pod/probe-test-app-68d99fdc94-98mg4    Successfully pulled image "jasonumiker/probe-test-app:v2" in 5.605s (5.605s including waiting). Image size: 1024950162 bytes.
16s         Warning   Unhealthy           Pod/probe-test-app-68d99fdc94-98mg4    Readiness probe failed: Get "http://10.1.0.14:8080/readyz": dial tcp 10.1.0.14:8080: connect: connection refused
16s         Normal    Started             Pod/probe-test-app-68d99fdc94-98mg4    Started container probe-test-app
15s         Normal    ScalingReplicaSet   Deployment/probe-test-app              Scaled up replica set probe-test-app-68d99fdc94 from 1 to 2
15s         Normal    Scheduled           Pod/probe-test-app-68d99fdc94-qs6tm    Successfully assigned default/probe-test-app-68d99fdc94-qs6tm to docker-desktop
15s         Normal    ScalingReplicaSet   Deployment/probe-test-app              Scaled down replica set probe-test-app-685956d4cf from 3 to 2
15s         Normal    SuccessfulDelete    ReplicaSet/probe-test-app-685956d4cf   Deleted pod: probe-test-app-685956d4cf-nklgs
15s         Normal    Killing             Pod/probe-test-app-685956d4cf-nklgs    Stopping container probe-test-app
15s         Normal    SuccessfulCreate    ReplicaSet/probe-test-app-68d99fdc94   Created pod: probe-test-app-68d99fdc94-qs6tm
14s         Normal    Pulled              Pod/probe-test-app-68d99fdc94-qs6tm    Container image "jasonumiker/probe-test-app:v2" already present on machine
14s         Normal    Created             Pod/probe-test-app-68d99fdc94-qs6tm    Created container: probe-test-app
14s         Normal    Started             Pod/probe-test-app-68d99fdc94-qs6tm    Started container probe-test-app
14s         Warning   Unhealthy           Pod/probe-test-app-68d99fdc94-qs6tm    Readiness probe failed: Get "http://10.1.0.15:8080/readyz": dial tcp 10.1.0.15:8080: connect: connection refused
13s         Normal    ScalingReplicaSet   Deployment/probe-test-app              Scaled down replica set probe-test-app-685956d4cf from 2 to 1
13s         Normal    SuccessfulCreate    ReplicaSet/probe-test-app-68d99fdc94   Created pod: probe-test-app-68d99fdc94-f4t6d
13s         Normal    ScalingReplicaSet   Deployment/probe-test-app              Scaled up replica set probe-test-app-68d99fdc94 from 2 to 3
13s         Normal    Killing             Pod/probe-test-app-685956d4cf-xvq4q    Stopping container probe-test-app
13s         Normal    SuccessfulDelete    ReplicaSet/probe-test-app-685956d4cf   Deleted pod: probe-test-app-685956d4cf-xvq4q
13s         Normal    Scheduled           Pod/probe-test-app-68d99fdc94-f4t6d    Successfully assigned default/probe-test-app-68d99fdc94-f4t6d to docker-desktop
12s         Normal    Created             Pod/probe-test-app-68d99fdc94-f4t6d    Created container: probe-test-app
12s         Normal    Pulled              Pod/probe-test-app-68d99fdc94-f4t6d    Container image "jasonumiker/probe-test-app:v2" already present on machine
12s         Normal    Started             Pod/probe-test-app-68d99fdc94-f4t6d    Started container probe-test-app
2s          Normal    Killing             Pod/probe-test-app-685956d4cf-pnhb2    Stopping container probe-test-app
2s          Normal    SuccessfulDelete    ReplicaSet/probe-test-app-685956d4cf   Deleted pod: probe-test-app-685956d4cf-pnhb2
2s          Normal    ScalingReplicaSet   Deployment/probe-test-app              Scaled down replica set probe-test-app-685956d4cf from 1 to 0
--------------------
kubectl rollout undo deployment/probe-test-app
deployment.apps/probe-test-app rolled back
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "probe-test-app" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "probe-test-app" rollout to finish: 1 old replicas are pending termination...
deployment "probe-test-app" successfully rolled out
--------------------
kubectl get pods
NAME                              READY   STATUS    RESTARTS   AGE
probe-test-app-685956d4cf-fpmxh   1/1     Running   0          7s
probe-test-app-685956d4cf-jlf4s   1/1     Running   0          3s
probe-test-app-685956d4cf-v6fbz   1/1     Running   0          5s
--------------------
kubectl describe replicaset probe-test-app
Name:           probe-test-app-685956d4cf
Namespace:      default
Selector:       app.kubernetes.io/name=probe-test-app,pod-template-hash=685956d4cf
Labels:         app.kubernetes.io/name=probe-test-app
                pod-template-hash=685956d4cf
Annotations:    deployment.kubernetes.io/desired-replicas: 3
                deployment.kubernetes.io/max-replicas: 4
                deployment.kubernetes.io/revision: 3
                deployment.kubernetes.io/revision-history: 1
Controlled By:  Deployment/probe-test-app
Replicas:       3 current / 3 desired
Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app.kubernetes.io/name=probe-test-app
           pod-template-hash=685956d4cf
  Containers:
   probe-test-app:
    Image:      mirror.gcr.io/jasonumiker/probe-test-app:v1
    Port:       8080/TCP
    Host Port:  0/TCP
    Requests:
      cpu:         50m
      memory:      52Mi
    Liveness:      http-get http://:8080/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:     http-get http://:8080/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  39s   replicaset-controller  Created pod: probe-test-app-685956d4cf-nklgs
  Normal  SuccessfulCreate  39s   replicaset-controller  Created pod: probe-test-app-685956d4cf-xvq4q
  Normal  SuccessfulCreate  39s   replicaset-controller  Created pod: probe-test-app-685956d4cf-pnhb2
  Normal  SuccessfulDelete  26s   replicaset-controller  Deleted pod: probe-test-app-685956d4cf-nklgs
  Normal  SuccessfulDelete  24s   replicaset-controller  Deleted pod: probe-test-app-685956d4cf-xvq4q
  Normal  SuccessfulDelete  13s   replicaset-controller  Deleted pod: probe-test-app-685956d4cf-pnhb2
  Normal  SuccessfulCreate  10s   replicaset-controller  Created pod: probe-test-app-685956d4cf-fpmxh
  Normal  SuccessfulCreate  8s    replicaset-controller  Created pod: probe-test-app-685956d4cf-v6fbz
  Normal  SuccessfulCreate  6s    replicaset-controller  Created pod: probe-test-app-685956d4cf-jlf4s

Name:           probe-test-app-68d99fdc94
Namespace:      default
Selector:       app.kubernetes.io/name=probe-test-app,pod-template-hash=68d99fdc94
Labels:         app.kubernetes.io/name=probe-test-app
                pod-template-hash=68d99fdc94
Annotations:    deployment.kubernetes.io/desired-replicas: 3
                deployment.kubernetes.io/max-replicas: 4
                deployment.kubernetes.io/revision: 2
Controlled By:  Deployment/probe-test-app
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app.kubernetes.io/name=probe-test-app
           pod-template-hash=68d99fdc94
  Containers:
   probe-test-app:
    Image:      jasonumiker/probe-test-app:v2
    Port:       8080/TCP
    Host Port:  0/TCP
    Requests:
      cpu:         50m
      memory:      52Mi
    Liveness:      http-get http://:8080/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:     http-get http://:8080/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  35s   replicaset-controller  Created pod: probe-test-app-68d99fdc94-98mg4
  Normal  SuccessfulCreate  27s   replicaset-controller  Created pod: probe-test-app-68d99fdc94-qs6tm
  Normal  SuccessfulCreate  25s   replicaset-controller  Created pod: probe-test-app-68d99fdc94-f4t6d
  Normal  SuccessfulDelete  9s    replicaset-controller  Deleted pod: probe-test-app-68d99fdc94-f4t6d
  Normal  SuccessfulDelete  7s    replicaset-controller  Deleted pod: probe-test-app-68d99fdc94-qs6tm
  Normal  SuccessfulDelete  5s    replicaset-controller  Deleted pod: probe-test-app-68d99fdc94-98mg4

--------------------
cd ../sidecar-and-init-containers
--------------------
kubectl apply -f sidecar.yaml
pod/pod-with-sidecar created
pod/pod-with-sidecar condition met
--------------------
kubectl apply -f init.yaml
pod/myapp-pod created
--------------------
kubectl get pod myapp-pod
NAME        READY   STATUS     RESTARTS   AGE
myapp-pod   0/1     Init:0/2   0          2s
--------------------
kubectl apply -f services-init-requires.yaml
service/myservice created
service/mydb created
--------------------
NAME        READY   STATUS    RESTARTS   AGE
myapp-pod   1/1     Running   0          14s
--------------------
pod "myapp-pod" deleted
pod "pod-with-sidecar" deleted
service "myservice" deleted
service "mydb" deleted
--------------------
cd ../pvs-and-statefulsets
--------------------
kubectl apply -f hostpath-provisioner.yaml
deployment.apps/hostpath-provisioner created
storageclass.storage.k8s.io/hostpath-provisioner created
serviceaccount/hostpath-provisioner created
clusterrole.rbac.authorization.k8s.io/hostpath-provisioner created
clusterrolebinding.rbac.authorization.k8s.io/hostpath-provisioner created
--------------------
kubectl get storageclass
NAME                   PROVISIONER            RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
hostpath (default)     docker.io/hostpath     Delete          Immediate              false                  4h17m
hostpath-provisioner   microk8s.io/hostpath   Delete          WaitForFirstConsumer   false                  2s
--------------------
kubectl apply -f pvc.yaml
persistentvolumeclaim/test-pvc created
--------------------
kubectl get pvc
NAME       STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS           VOLUMEATTRIBUTESCLASS   AGE
test-pvc   Pending                                      hostpath-provisioner   <unset>                 1s
--------------------
kubectl apply -f pod.yaml
pod/nginx created
pod/nginx condition met
--------------------
kubectl get pvc
NAME       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS           VOLUMEATTRIBUTESCLASS   AGE
test-pvc   Bound    pvc-f1352803-d4e2-4870-a46f-016f5e2f760c   1Gi        RWO            hostpath-provisioner   <unset>                 14s
--------------------
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS           VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-f1352803-d4e2-4870-a46f-016f5e2f760c   1Gi        RWO            Delete           Bound    default/test-pvc   hostpath-provisioner   <unset>                          3s
--------------------
kubectl apply -f service.yaml
service/nginx created
--------------------
curl http://localhost:8001
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   153  100   153    0     0   116k      0 --:--:-- --:--:-- --:--:--  149k
<html>
<head><title>403 Forbidden</title></head>
<body>
<center><h1>403 Forbidden</h1></center>
<hr><center>nginx/1.27.4</center>
</body>
</html>
--------------------
kubectl exec -it nginx  -- bash -c "echo 'Data on PV' > /usr/share/nginx/html/index.html"
--------------------
curl http://localhost:8001
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100    11  100    11    0     0  10556      0 --:--:-- --:--:-- --:--:-- 11000
Data on PV
--------------------
kubectl delete pod nginx
pod "nginx" deleted
--------------------
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS           VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-f1352803-d4e2-4870-a46f-016f5e2f760c   1Gi        RWO            Delete           Bound    default/test-pvc   hostpath-provisioner   <unset>                          22s
--------------------
kubectl apply -f pod.yaml
pod/nginx created
pod/nginx condition met
--------------------
curl http://localhost:8001
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100    11  100    11    0     0  10396      0 --:--:-- --:--:-- --:--:-- 11000
Data on PV
--------------------
kubectl delete service nginx
service "nginx" deleted
--------------------
kubectl delete pod nginx
pod "nginx" deleted
--------------------
kubectl delete pvc test-pvc
persistentvolumeclaim "test-pvc" deleted
--------------------
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM              STORAGECLASS           VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-f1352803-d4e2-4870-a46f-016f5e2f760c   1Gi        RWO            Delete           Released   default/test-pvc   hostpath-provisioner   <unset>                          30s
--------------------
--------------------
kubectl apply -k .
serviceaccount/rabbitmq created
role.rbac.authorization.k8s.io/rabbitmq created
rolebinding.rbac.authorization.k8s.io/rabbitmq created
configmap/rabbitmq-config created
secret/erlang-cookie created
secret/rabbitmq-admin created
service/rabbitmq-client created
service/rabbitmq-headless created
statefulset.apps/rabbitmq created
Waiting for 1 pods to be ready...
partitioned roll out complete: 1 new pods have been updated...
--------------------
kubectl describe statefulset rabbitmq
Name:               rabbitmq
Namespace:          default
CreationTimestamp:  Fri, 18 Apr 2025 14:49:41 +1000
Selector:           app=rabbitmq
Labels:             <none>
Annotations:        <none>
Replicas:           1 desired | 1 total
Update Strategy:    RollingUpdate
  Partition:        0
Pods Status:        1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=rabbitmq
  Service Account:  rabbitmq
  Init Containers:
   rabbitmq-config:
    Image:      mirror.gcr.io/busybox:1.37.0
    Port:       <none>
    Host Port:  <none>
    Command:
      sh
      -c
      cp /tmp/rabbitmq/rabbitmq.conf /etc/rabbitmq/rabbitmq.conf && echo '' >> /etc/rabbitmq/rabbitmq.conf; cp /tmp/rabbitmq/enabled_plugins /etc/rabbitmq/enabled_plugins
    Environment:  <none>
    Mounts:
      /etc/rabbitmq from rabbitmq-config-rw (rw)
      /tmp/rabbitmq from rabbitmq-config (rw)
  Containers:
   rabbitmq:
    Image:       mirror.gcr.io/rabbitmq:3.8.34
    Ports:       5672/TCP, 15672/TCP, 15692/TCP, 4369/TCP
    Host Ports:  0/TCP, 0/TCP, 0/TCP, 0/TCP
    Liveness:    exec [rabbitmq-diagnostics status] delay=60s timeout=15s period=60s #success=1 #failure=3
    Readiness:   exec [rabbitmq-diagnostics ping] delay=20s timeout=10s period=60s #success=1 #failure=3
    Environment:
      RABBITMQ_DEFAULT_PASS:   <set to the key 'pass' in secret 'rabbitmq-admin'>   Optional: false
      RABBITMQ_DEFAULT_USER:   <set to the key 'user' in secret 'rabbitmq-admin'>   Optional: false
      RABBITMQ_ERLANG_COOKIE:  <set to the key 'cookie' in secret 'erlang-cookie'>  Optional: false
    Mounts:
      /etc/rabbitmq from rabbitmq-config-rw (rw)
      /var/lib/rabbitmq/mnesia from rabbitmq-data (rw)
  Volumes:
   rabbitmq-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      rabbitmq-config
    Optional:  false
   rabbitmq-config-rw:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
   rabbitmq-data:
    Type:          PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:     rabbitmq-data
    ReadOnly:      false
  Node-Selectors:  <none>
  Tolerations:     <none>
Volume Claims:
  Name:          rabbitmq-data
  StorageClass:  hostpath-provisioner
  Labels:        <none>
  Annotations:   <none>
  Capacity:      3Gi
  Access Modes:  [ReadWriteOnce]
Events:
  Type    Reason            Age   From                    Message
  ----    ------            ----  ----                    -------
  Normal  SuccessfulCreate  84s   statefulset-controller  create Claim rabbitmq-data-rabbitmq-0 Pod rabbitmq-0 in StatefulSet rabbitmq success
  Normal  SuccessfulCreate  84s   statefulset-controller  create Pod rabbitmq-0 in StatefulSet rabbitmq successful
--------------------
kubectl get pods
NAME                              READY   STATUS    RESTARTS   AGE
probe-test-app-685956d4cf-fpmxh   1/1     Running   0          4m3s
probe-test-app-685956d4cf-jlf4s   1/1     Running   0          3m59s
probe-test-app-685956d4cf-v6fbz   1/1     Running   0          4m1s
rabbitmq-0                        1/1     Running   0          86s
--------------------
kubectl get pvc
NAME                       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS           VOLUMEATTRIBUTESCLASS   AGE
rabbitmq-data-rabbitmq-0   Bound    pvc-0fc124b7-1ed6-413a-a5dc-08cf23048082   3Gi        RWO            hostpath-provisioner   <unset>                 87s
--------------------
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                              STORAGECLASS           VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-0fc124b7-1ed6-413a-a5dc-08cf23048082   3Gi        RWO            Delete           Bound    default/rabbitmq-data-rabbitmq-0   hostpath-provisioner   <unset>                          83s
--------------------
kubectl delete pod rabbitmq-0
pod "rabbitmq-0" deleted
--------------------
kubectl get pods
NAME                              READY   STATUS            RESTARTS   AGE
probe-test-app-685956d4cf-fpmxh   1/1     Running           0          4m10s
probe-test-app-685956d4cf-jlf4s   1/1     Running           0          4m6s
probe-test-app-685956d4cf-v6fbz   1/1     Running           0          4m8s
rabbitmq-0                        0/1     PodInitializing   0          2s
--------------------
cd ../../monitoring
--------------------
./install-prometheus.sh

Updating docker-desktop pods to expose metrics endpoints
This will involve several kube-system pod restarts

Fetching debian image to run nsenter on the docker-desktop host...
12.10: Pulling from debian
23b7d26ef1d2: Already exists
Digest: sha256:00cd074b40c4d99ff0c24540bdde0533ca3791edcdac0de36d6b9fb3260d89e2
Status: Downloaded newer image for mirror.gcr.io/debian:12.10
mirror.gcr.io/debian:12.10
Host Node IP: 192.168.65.3
Updating kube-proxy configmap...
configmap "kube-proxy" deleted
configmap/kube-proxy created
Restarting the kube-proxy pod
pod "kube-proxy-7f7b2" deleted
pod/kube-proxy-drxjv condition met
kube-proxy pod restarted.
Updating bind-address on kube-controller-manager...
Waiting for kube-controller-manager to restart, this can take some time...
pod/kube-controller-manager-docker-desktop condition met
pod/kube-controller-manager-docker-desktop condition met
kube-controller-manager pod restarted.
Updating bind-address on kube-scheduler
Waiting for kube-scheduler to restart, this can take some time...
pod/kube-scheduler-docker-desktop condition met
pod/kube-scheduler-docker-desktop condition met
kube-scheduler pod restarted.
Adding node ip to listen-metrics-urls on etcd
Waiting for etcd to restart, this can take some time...
Error from server (Timeout): the server was unable to return a response in the time allotted, but may still be processing the request (get pods)
pod/etcd-docker-desktop condition met

Done! You can now deploy the monitoring components.

"prometheus-community" already exists with the same configuration, skipping
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "kiali" chart repository
...Successfully got an update from the "ingress-nginx" chart repository
...Successfully got an update from the "gatekeeper" chart repository
...Successfully got an update from the "argo-helm" chart repository
...Successfully got an update from the "kedacore" chart repository
...Successfully got an update from the "prometheus-community" chart repository
...Successfully got an update from the "istio" chart repository
Update Complete. ⎈Happy Helming!⎈
namespace/monitoring created
NAME: prometheus
LAST DEPLOYED: Fri Apr 18 14:54:04 2025
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
NOTES:
kube-prometheus-stack has been installed. Check its status by running:
  kubectl --namespace monitoring get pods -l "release=prometheus"

Get Grafana 'admin' user password by running:

  kubectl --namespace monitoring get secrets prometheus-grafana -o jsonpath="{.data.admin-password}" | base64 -d ; echo

Access Grafana local instance:

  export POD_NAME=$(kubectl --namespace monitoring get pod -l "app.kubernetes.io/name=grafana,app.kubernetes.io/instance=prometheus" -oname)
  kubectl --namespace monitoring port-forward $POD_NAME 3000

Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create & configure Alertmanager and Prometheus instances using the Operator.
NAME: adapter
LAST DEPLOYED: Fri Apr 18 14:54:56 2025
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
adapter-prometheus-adapter has been deployed.
In a few minutes you should be able to list metrics using the following command(s):

  kubectl get --raw /apis/metrics.k8s.io/v1beta1
  kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1
Waiting for deployment "adapter-prometheus-adapter" rollout to finish: 0 of 1 updated replicas are available...
deployment "adapter-prometheus-adapter" successfully rolled out
Waiting for 1 pods to be ready...
statefulset rolling update complete 1 pods at revision prometheus-prometheus-kube-prometheus-prometheus-75b9fff9cd...
--------------------
kubectl top nodes
NAME             CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)   
docker-desktop   72m          0%       3800Mi          11%         
--------------------
kubectl top pods
NAME                              CPU(cores)   MEMORY(bytes)   
probe-test-app-685956d4cf-fpmxh   0m           41Mi            
probe-test-app-685956d4cf-jlf4s   0m           41Mi            
probe-test-app-685956d4cf-v6fbz   0m           41Mi            
rabbitmq-0                        20m          142Mi           
--------------------
kubectl top pods -n monitoring
NAME                                                     CPU(cores)   MEMORY(bytes)   
adapter-prometheus-adapter-57b7646776-f69vh              0m           29Mi            
alertmanager-prometheus-kube-prometheus-alertmanager-0   0m           33Mi            
prometheus-grafana-66cb8c946f-pmjdv                      1m           288Mi           
prometheus-kube-prometheus-operator-7f8d744cd7-d2h4z     0m           32Mi            
prometheus-kube-state-metrics-f699c577d-g2g9x            0m           21Mi            
prometheus-prometheus-kube-prometheus-prometheus-0       6m           105Mi           
prometheus-prometheus-node-exporter-qrt85                0m           9Mi             
--------------------
cd ../probe-test-app
--------------------
kubectl apply -f probe-test-app-hpa.yaml
horizontalpodautoscaler.autoscaling/probe-test-app created
--------------------
kubectl apply -f generate-load-app-replicaset.yaml
replicaset.apps/generate-load-app created
--------------------
kubectl get pods
NAME                              READY   STATUS    RESTARTS   AGE
generate-load-app-72wzx           1/1     Running   0          46s
generate-load-app-cfbns           1/1     Running   0          46s
generate-load-app-dtrn7           1/1     Running   0          46s
generate-load-app-t6n2j           1/1     Running   0          46s
generate-load-app-xgjwd           1/1     Running   0          46s
probe-test-app-685956d4cf-fpmxh   1/1     Running   0          11m
probe-test-app-685956d4cf-jlf4s   1/1     Running   0          11m
probe-test-app-685956d4cf-v6fbz   1/1     Running   0          11m
rabbitmq-0                        1/1     Running   0          7m43s
--------------------
kubectl delete replicaset generate-load-app
replicaset.apps "generate-load-app" deleted
--------------------
kubectl describe hpa probe-test-app
Name:                                                  probe-test-app
Namespace:                                             default
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Fri, 18 Apr 2025 14:58:08 +1000
Reference:                                             Deployment/probe-test-app
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  65% (32m) / 50%
Min replicas:                                          1
Max replicas:                                          5
Deployment pods:                                       3 current / 4 desired
Conditions:
  Type            Status  Reason              Message
  ----            ------  ------              -------
  AbleToScale     True    SucceededRescale    the HPA controller was able to update the target scale to 4
  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)
  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range
Events:
  Type    Reason             Age   From                       Message
  ----    ------             ----  ----                       -------
  Normal  SuccessfulRescale  35s   horizontal-pod-autoscaler  New size: 4; reason: cpu resource utilization (percentage of request) above target
--------------------
cd ../limit-examples
--------------------
kubectl apply -f cpu-stressor.yaml
deployment.apps/cpu-stressor created
Waiting for deployment "cpu-stressor" rollout to finish: 0 of 1 updated replicas are available...
deployment "cpu-stressor" successfully rolled out
--------------------
kubectl delete deployment cpu-stressor
deployment.apps "cpu-stressor" deleted
--------------------
kubectl apply -f memory-stressor.yaml
pod/memory-stressor created
--------------------
kubectl delete pod memory-stressor
pod "memory-stressor" deleted
--------------------
cd ../keda-example
--------------------
./install-keda.sh
"kedacore" already exists with the same configuration, skipping
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "kedacore" chart repository
...Successfully got an update from the "gatekeeper" chart repository
...Successfully got an update from the "kiali" chart repository
...Successfully got an update from the "prometheus-community" chart repository
...Successfully got an update from the "istio" chart repository
...Successfully got an update from the "ingress-nginx" chart repository
...Successfully got an update from the "argo-helm" chart repository
Update Complete. ⎈Happy Helming!⎈
NAME: keda
LAST DEPLOYED: Fri Apr 18 15:00:14 2025
NAMESPACE: keda
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
:::^.     .::::^:     :::::::::::::::    .:::::::::.                   .^.                  
7???~   .^7????~.     7??????????????.   :?????????77!^.              .7?7.                 
7???~  ^7???7~.       ~!!!!!!!!!!!!!!.   :????!!!!7????7~.           .7???7.                
7???~^7????~.                            :????:    :~7???7.         :7?????7.               
7???7????!.           ::::::::::::.      :????:      .7???!        :7??77???7.              
7????????7:           7???????????~      :????:       :????:      :???7?5????7.             
7????!~????^          !77777777777^      :????:       :????:     ^???7?#P7????7.            
7???~  ^????~                            :????:      :7???!     ^???7J#@J7?????7.           
7???~   :7???!.                          :????:   .:~7???!.    ~???7Y&@#7777????7.          
7???~    .7???7:      !!!!!!!!!!!!!!!    :????7!!77????7^     ~??775@@@GJJYJ?????7.         
7???~     .!????^     7?????????????7.   :?????????7!~:      !????G@@@@@@@@5??????7:        
::::.       :::::     :::::::::::::::    .::::::::..        .::::JGGGB@@@&7:::::::::        
                                                                      ?@@#~                  
                                                                      P@B^                   
                                                                    :&G:                    
                                                                    !5.                     
                                                                    .Kubernetes Event-driven Autoscaling (KEDA) - Application autoscaling made simple.

Get started by deploying Scaled Objects to your cluster:
    - Information about Scaled Objects : https://keda.sh/docs/latest/concepts/
    - Samples: https://github.com/kedacore/samples

Get information about the deployed ScaledObjects:
  kubectl get scaledobject [--namespace <namespace>]

Get details about a deployed ScaledObject:
  kubectl describe scaledobject <scaled-object-name> [--namespace <namespace>]

Get information about the deployed ScaledObjects:
  kubectl get triggerauthentication [--namespace <namespace>]

Get details about a deployed ScaledObject:
  kubectl describe triggerauthentication <trigger-authentication-name> [--namespace <namespace>]

Get an overview of the Horizontal Pod Autoscalers (HPA) that KEDA is using behind the scenes:
  kubectl get hpa [--all-namespaces] [--namespace <namespace>]

Learn more about KEDA:
- Documentation: https://keda.sh/
- Support: https://keda.sh/support/
- File an issue: https://github.com/kedacore/keda/issues/new/choose
--------------------
kubectl apply -f consumer.yaml
secret/rabbitmq-consumer-secret created
deployment.apps/rabbitmq-consumer created
--------------------
kubectl apply -f keda-scaled-object.yaml
scaledobject.keda.sh/rabbitmq-consumer created
triggerauthentication.keda.sh/rabbitmq-consumer-trigger created
--------------------
kubectl apply -f publisher.yaml
job.batch/rabbitmq-publish created
--------------------
kubectl get pods
NAME                                 READY   STATUS              RESTARTS   AGE
probe-test-app-685956d4cf-d4ndz      1/1     Running             0          81s
probe-test-app-685956d4cf-fpmxh      1/1     Running             0          13m
probe-test-app-685956d4cf-jlf4s      1/1     Running             0          13m
probe-test-app-685956d4cf-lb676      1/1     Running             0          21s
probe-test-app-685956d4cf-v6fbz      1/1     Running             0          13m
rabbitmq-0                           1/1     Running             0          9m17s
rabbitmq-consumer-54454cf965-b84v5   0/1     ContainerCreating   0          3s
rabbitmq-publish-m8fl6               0/1     ContainerCreating   0          1s
--------------------
kubectl events
LAST SEEN             TYPE      REASON                  OBJECT                                           MESSAGE
14m                   Normal    Scheduled               Pod/probe-test-app                               Successfully assigned default/probe-test-app to docker-desktop
14m                   Normal    Pulling                 Pod/probe-test-app                               Pulling image "mirror.gcr.io/jasonumiker/probe-test-app:v1"
14m                   Normal    Pulled                  Pod/probe-test-app                               Successfully pulled image "mirror.gcr.io/jasonumiker/probe-test-app:v1" in 27.439s (27.439s including waiting). Image size: 1024950162 bytes.
14m                   Normal    Created                 Pod/probe-test-app                               Created container: probe-test-app
14m                   Normal    Started                 Pod/probe-test-app                               Started container probe-test-app
14m                   Warning   Unhealthy               Pod/probe-test-app                               Readiness probe failed: Get "http://10.1.0.6:8080/readyz": dial tcp 10.1.0.6:8080: connect: connection refused
14m                   Normal    Pulled                  Pod/probe-test-app-2                             Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
14m                   Normal    Created                 Pod/probe-test-app-2                             Created container: probe-test-app
14m                   Normal    Started                 Pod/probe-test-app-2                             Started container probe-test-app
14m                   Normal    Scheduled               Pod/probe-test-app-2                             Successfully assigned default/probe-test-app-2 to docker-desktop
14m                   Normal    Killing                 Pod/probe-test-app                               Stopping container probe-test-app
14m                   Normal    Killing                 Pod/probe-test-app-2                             Stopping container probe-test-app
14m                   Normal    SuccessfulCreate        ReplicaSet/probe-test-app                        Created pod: probe-test-app-f2tc6
14m                   Normal    SuccessfulCreate        ReplicaSet/probe-test-app                        Created pod: probe-test-app-r92q4
14m                   Normal    SuccessfulCreate        ReplicaSet/probe-test-app                        Created pod: probe-test-app-4pldg
14m                   Normal    Scheduled               Pod/probe-test-app-f2tc6                         Successfully assigned default/probe-test-app-f2tc6 to docker-desktop
14m                   Normal    Scheduled               Pod/probe-test-app-4pldg                         Successfully assigned default/probe-test-app-4pldg to docker-desktop
14m                   Normal    Scheduled               Pod/probe-test-app-r92q4                         Successfully assigned default/probe-test-app-r92q4 to docker-desktop
14m                   Normal    Created                 Pod/probe-test-app-r92q4                         Created container: probe-test-app
14m                   Normal    Pulled                  Pod/probe-test-app-4pldg                         Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
14m                   Warning   Unhealthy               Pod/probe-test-app-4pldg                         Readiness probe failed: Get "http://10.1.0.9:8080/readyz": dial tcp 10.1.0.9:8080: connect: connection refused
14m                   Normal    Pulled                  Pod/probe-test-app-r92q4                         Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
14m                   Normal    Started                 Pod/probe-test-app-4pldg                         Started container probe-test-app
14m                   Normal    Created                 Pod/probe-test-app-4pldg                         Created container: probe-test-app
14m                   Warning   Unhealthy               Pod/probe-test-app-f2tc6                         Readiness probe failed: Get "http://10.1.0.8:8080/readyz": dial tcp 10.1.0.8:8080: connect: connection refused
14m                   Normal    Started                 Pod/probe-test-app-f2tc6                         Started container probe-test-app
14m                   Normal    Created                 Pod/probe-test-app-f2tc6                         Created container: probe-test-app
14m                   Normal    Pulled                  Pod/probe-test-app-f2tc6                         Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
14m                   Normal    Started                 Pod/probe-test-app-r92q4                         Started container probe-test-app
13m                   Normal    SuccessfulDelete        ReplicaSet/probe-test-app                        Deleted pod: probe-test-app-r92q4
13m                   Normal    Killing                 Pod/probe-test-app-r92q4                         Stopping container probe-test-app
13m                   Normal    Killing                 Pod/probe-test-app-f2tc6                         Stopping container probe-test-app
13m                   Normal    Killing                 Pod/probe-test-app-4pldg                         Stopping container probe-test-app
13m                   Normal    SuccessfulCreate        ReplicaSet/probe-test-app-685956d4cf             Created pod: probe-test-app-685956d4cf-pnhb2
13m                   Normal    Scheduled               Pod/probe-test-app-685956d4cf-nklgs              Successfully assigned default/probe-test-app-685956d4cf-nklgs to docker-desktop
13m                   Normal    Pulled                  Pod/probe-test-app-685956d4cf-nklgs              Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
13m                   Normal    Scheduled               Pod/probe-test-app-685956d4cf-pnhb2              Successfully assigned default/probe-test-app-685956d4cf-pnhb2 to docker-desktop
13m                   Normal    Scheduled               Pod/probe-test-app-685956d4cf-xvq4q              Successfully assigned default/probe-test-app-685956d4cf-xvq4q to docker-desktop
13m                   Normal    ScalingReplicaSet       Deployment/probe-test-app                        Scaled up replica set probe-test-app-685956d4cf from 0 to 3
13m                   Normal    SuccessfulCreate        ReplicaSet/probe-test-app-685956d4cf             Created pod: probe-test-app-685956d4cf-nklgs
13m                   Normal    SuccessfulCreate        ReplicaSet/probe-test-app-685956d4cf             Created pod: probe-test-app-685956d4cf-xvq4q
13m                   Normal    Pulled                  Pod/probe-test-app-685956d4cf-xvq4q              Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
13m                   Normal    Started                 Pod/probe-test-app-685956d4cf-xvq4q              Started container probe-test-app
13m                   Normal    Created                 Pod/probe-test-app-685956d4cf-xvq4q              Created container: probe-test-app
13m                   Normal    Started                 Pod/probe-test-app-685956d4cf-pnhb2              Started container probe-test-app
13m                   Warning   Unhealthy               Pod/probe-test-app-685956d4cf-xvq4q              Readiness probe failed: Get "http://10.1.0.13:8080/readyz": dial tcp 10.1.0.13:8080: connect: connection refused
13m                   Normal    Created                 Pod/probe-test-app-685956d4cf-pnhb2              Created container: probe-test-app
13m                   Normal    Created                 Pod/probe-test-app-685956d4cf-nklgs              Created container: probe-test-app
13m                   Normal    Started                 Pod/probe-test-app-685956d4cf-nklgs              Started container probe-test-app
13m                   Normal    Pulled                  Pod/probe-test-app-685956d4cf-pnhb2              Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
13m                   Normal    ScalingReplicaSet       Deployment/probe-test-app                        Scaled up replica set probe-test-app-68d99fdc94 from 0 to 1
13m                   Normal    Scheduled               Pod/probe-test-app-68d99fdc94-98mg4              Successfully assigned default/probe-test-app-68d99fdc94-98mg4 to docker-desktop
13m                   Normal    SuccessfulCreate        ReplicaSet/probe-test-app-68d99fdc94             Created pod: probe-test-app-68d99fdc94-98mg4
13m                   Normal    Pulling                 Pod/probe-test-app-68d99fdc94-98mg4              Pulling image "jasonumiker/probe-test-app:v2"
13m                   Normal    Pulled                  Pod/probe-test-app-68d99fdc94-98mg4              Successfully pulled image "jasonumiker/probe-test-app:v2" in 5.605s (5.605s including waiting). Image size: 1024950162 bytes.
13m                   Normal    Created                 Pod/probe-test-app-68d99fdc94-98mg4              Created container: probe-test-app
13m                   Warning   Unhealthy               Pod/probe-test-app-68d99fdc94-98mg4              Readiness probe failed: Get "http://10.1.0.14:8080/readyz": dial tcp 10.1.0.14:8080: connect: connection refused
13m                   Normal    Started                 Pod/probe-test-app-68d99fdc94-98mg4              Started container probe-test-app
13m                   Normal    ScalingReplicaSet       Deployment/probe-test-app                        Scaled up replica set probe-test-app-68d99fdc94 from 1 to 2
13m                   Normal    SuccessfulCreate        ReplicaSet/probe-test-app-68d99fdc94             Created pod: probe-test-app-68d99fdc94-qs6tm
13m                   Normal    ScalingReplicaSet       Deployment/probe-test-app                        Scaled down replica set probe-test-app-685956d4cf from 3 to 2
13m                   Normal    Scheduled               Pod/probe-test-app-68d99fdc94-qs6tm              Successfully assigned default/probe-test-app-68d99fdc94-qs6tm to docker-desktop
13m                   Normal    Killing                 Pod/probe-test-app-685956d4cf-nklgs              Stopping container probe-test-app
13m                   Normal    SuccessfulDelete        ReplicaSet/probe-test-app-685956d4cf             Deleted pod: probe-test-app-685956d4cf-nklgs
13m                   Normal    Created                 Pod/probe-test-app-68d99fdc94-qs6tm              Created container: probe-test-app
13m                   Normal    Pulled                  Pod/probe-test-app-68d99fdc94-qs6tm              Container image "jasonumiker/probe-test-app:v2" already present on machine
13m                   Normal    Started                 Pod/probe-test-app-68d99fdc94-qs6tm              Started container probe-test-app
13m                   Warning   Unhealthy               Pod/probe-test-app-68d99fdc94-qs6tm              Readiness probe failed: Get "http://10.1.0.15:8080/readyz": dial tcp 10.1.0.15:8080: connect: connection refused
13m                   Normal    Killing                 Pod/probe-test-app-685956d4cf-xvq4q              Stopping container probe-test-app
13m                   Normal    SuccessfulCreate        ReplicaSet/probe-test-app-68d99fdc94             Created pod: probe-test-app-68d99fdc94-f4t6d
13m                   Normal    Scheduled               Pod/probe-test-app-68d99fdc94-f4t6d              Successfully assigned default/probe-test-app-68d99fdc94-f4t6d to docker-desktop
13m                   Normal    SuccessfulDelete        ReplicaSet/probe-test-app-685956d4cf             Deleted pod: probe-test-app-685956d4cf-xvq4q
13m                   Normal    ScalingReplicaSet       Deployment/probe-test-app                        Scaled down replica set probe-test-app-685956d4cf from 2 to 1
13m                   Normal    ScalingReplicaSet       Deployment/probe-test-app                        Scaled up replica set probe-test-app-68d99fdc94 from 2 to 3
13m                   Normal    Started                 Pod/probe-test-app-68d99fdc94-f4t6d              Started container probe-test-app
13m                   Normal    Created                 Pod/probe-test-app-68d99fdc94-f4t6d              Created container: probe-test-app
13m                   Normal    Pulled                  Pod/probe-test-app-68d99fdc94-f4t6d              Container image "jasonumiker/probe-test-app:v2" already present on machine
13m                   Normal    Killing                 Pod/probe-test-app-685956d4cf-pnhb2              Stopping container probe-test-app
13m                   Normal    ScalingReplicaSet       Deployment/probe-test-app                        Scaled down replica set probe-test-app-685956d4cf from 1 to 0
13m                   Normal    SuccessfulDelete        ReplicaSet/probe-test-app-685956d4cf             Deleted pod: probe-test-app-685956d4cf-pnhb2
13m                   Normal    Scheduled               Pod/probe-test-app-685956d4cf-fpmxh              Successfully assigned default/probe-test-app-685956d4cf-fpmxh to docker-desktop
13m                   Normal    SuccessfulCreate        ReplicaSet/probe-test-app-685956d4cf             Created pod: probe-test-app-685956d4cf-fpmxh
13m                   Normal    ScalingReplicaSet       Deployment/probe-test-app                        Scaled up replica set probe-test-app-685956d4cf from 0 to 1
13m                   Warning   Unhealthy               Pod/probe-test-app-685956d4cf-fpmxh              Readiness probe failed: Get "http://10.1.0.17:8080/readyz": dial tcp 10.1.0.17:8080: connect: connection refused
13m                   Normal    Started                 Pod/probe-test-app-685956d4cf-fpmxh              Started container probe-test-app
13m                   Normal    Created                 Pod/probe-test-app-685956d4cf-fpmxh              Created container: probe-test-app
13m                   Normal    Pulled                  Pod/probe-test-app-685956d4cf-fpmxh              Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
13m                   Normal    Killing                 Pod/probe-test-app-68d99fdc94-f4t6d              Stopping container probe-test-app
13m                   Normal    SuccessfulDelete        ReplicaSet/probe-test-app-68d99fdc94             Deleted pod: probe-test-app-68d99fdc94-f4t6d
13m                   Normal    ScalingReplicaSet       Deployment/probe-test-app                        Scaled down replica set probe-test-app-68d99fdc94 from 3 to 2
13m                   Normal    SuccessfulCreate        ReplicaSet/probe-test-app-685956d4cf             Created pod: probe-test-app-685956d4cf-v6fbz
13m                   Normal    Scheduled               Pod/probe-test-app-685956d4cf-v6fbz              Successfully assigned default/probe-test-app-685956d4cf-v6fbz to docker-desktop
13m                   Normal    Pulled                  Pod/probe-test-app-685956d4cf-v6fbz              Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
13m                   Normal    Created                 Pod/probe-test-app-685956d4cf-v6fbz              Created container: probe-test-app
13m                   Normal    Started                 Pod/probe-test-app-685956d4cf-v6fbz              Started container probe-test-app
13m                   Warning   Unhealthy               Pod/probe-test-app-685956d4cf-v6fbz              Readiness probe failed: Get "http://10.1.0.18:8080/readyz": dial tcp 10.1.0.18:8080: connect: connection refused
13m                   Normal    Scheduled               Pod/probe-test-app-685956d4cf-jlf4s              Successfully assigned default/probe-test-app-685956d4cf-jlf4s to docker-desktop
13m                   Normal    Killing                 Pod/probe-test-app-68d99fdc94-qs6tm              Stopping container probe-test-app
13m                   Normal    SuccessfulDelete        ReplicaSet/probe-test-app-68d99fdc94             Deleted pod: probe-test-app-68d99fdc94-qs6tm
13m                   Normal    SuccessfulCreate        ReplicaSet/probe-test-app-685956d4cf             Created pod: probe-test-app-685956d4cf-jlf4s
13m                   Normal    Created                 Pod/probe-test-app-685956d4cf-jlf4s              Created container: probe-test-app
13m                   Normal    Pulled                  Pod/probe-test-app-685956d4cf-jlf4s              Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
13m                   Normal    Started                 Pod/probe-test-app-685956d4cf-jlf4s              Started container probe-test-app
13m                   Warning   Unhealthy               Pod/probe-test-app-685956d4cf-jlf4s              Readiness probe failed: Get "http://10.1.0.19:8080/readyz": dial tcp 10.1.0.19:8080: connect: connection refused
13m                   Normal    Killing                 Pod/probe-test-app-68d99fdc94-98mg4              Stopping container probe-test-app
13m (x4 over 13m)     Normal    ScalingReplicaSet       Deployment/probe-test-app                        (combined from similar events): Scaled down replica set probe-test-app-68d99fdc94 from 1 to 0
13m                   Normal    SuccessfulDelete        ReplicaSet/probe-test-app-68d99fdc94             Deleted pod: probe-test-app-68d99fdc94-98mg4
13m                   Normal    Pulling                 Pod/pod-with-sidecar                             Pulling image "mirror.gcr.io/alpine:3.21.3"
13m                   Normal    Scheduled               Pod/pod-with-sidecar                             Successfully assigned default/pod-with-sidecar to docker-desktop
13m                   Normal    Created                 Pod/pod-with-sidecar                             Created container: app-container
13m                   Normal    Pulled                  Pod/pod-with-sidecar                             Successfully pulled image "mirror.gcr.io/alpine:3.21.3" in 4.234s (4.234s including waiting). Image size: 7834312 bytes.
13m                   Normal    Pulling                 Pod/pod-with-sidecar                             Pulling image "mirror.gcr.io/nginx:1.27.4-bookworm"
13m                   Normal    Started                 Pod/pod-with-sidecar                             Started container app-container
12m                   Normal    Created                 Pod/pod-with-sidecar                             Created container: sidecar-container
12m                   Normal    Pulled                  Pod/pod-with-sidecar                             Successfully pulled image "mirror.gcr.io/nginx:1.27.4-bookworm" in 10.64s (10.64s including waiting). Image size: 192056179 bytes.
12m                   Normal    Started                 Pod/pod-with-sidecar                             Started container sidecar-container
12m                   Normal    Pulling                 Pod/myapp-pod                                    Pulling image "mirror.gcr.io/busybox:1.37.0"
12m                   Normal    Scheduled               Pod/myapp-pod                                    Successfully assigned default/myapp-pod to docker-desktop
12m                   Normal    Pulled                  Pod/myapp-pod                                    Container image "mirror.gcr.io/busybox:1.37.0" already present on machine
12m                   Normal    Created                 Pod/myapp-pod                                    Created container: init-mydb
12m                   Normal    Created                 Pod/myapp-pod                                    Created container: init-myservice
12m                   Normal    Started                 Pod/myapp-pod                                    Started container init-myservice
12m                   Normal    Pulled                  Pod/myapp-pod                                    Successfully pulled image "mirror.gcr.io/busybox:1.37.0" in 5.133s (5.133s including waiting). Image size: 4277910 bytes.
12m                   Normal    Started                 Pod/myapp-pod                                    Started container init-mydb
12m                   Normal    Pulled                  Pod/myapp-pod                                    Container image "mirror.gcr.io/busybox:1.37.0" already present on machine
12m                   Normal    Started                 Pod/myapp-pod                                    Started container myapp-container
12m                   Normal    Created                 Pod/myapp-pod                                    Created container: myapp-container
12m                   Normal    Killing                 Pod/myapp-pod                                    Stopping container myapp-container
12m                   Normal    Killing                 Pod/pod-with-sidecar                             Stopping container app-container
12m                   Normal    Killing                 Pod/pod-with-sidecar                             Stopping container sidecar-container
11m                   Normal    WaitForFirstConsumer    PersistentVolumeClaim/test-pvc                   waiting for first consumer to be created before binding
11m (x2 over 11m)     Normal    ExternalProvisioning    PersistentVolumeClaim/test-pvc                   Waiting for a volume to be created either by the external provisioner 'microk8s.io/hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
11m                   Normal    Provisioning            PersistentVolumeClaim/test-pvc                   External provisioner is provisioning volume for claim "default/test-pvc"
11m                   Normal    Scheduled               Pod/nginx                                        Successfully assigned default/nginx to docker-desktop
11m                   Normal    ProvisioningSucceeded   PersistentVolumeClaim/test-pvc                   Successfully provisioned volume pvc-f1352803-d4e2-4870-a46f-016f5e2f760c
11m                   Normal    Pulled                  Pod/nginx                                        Container image "mirror.gcr.io/nginx:1.27.4-bookworm" already present on machine
11m                   Normal    Created                 Pod/nginx                                        Created container: nginx
11m                   Normal    Started                 Pod/nginx                                        Started container nginx
11m                   Normal    Killing                 Pod/nginx                                        Stopping container nginx
10m                   Normal    Scheduled               Pod/nginx                                        Successfully assigned default/nginx to docker-desktop
10m                   Normal    Created                 Pod/nginx                                        Created container: nginx
10m                   Normal    Started                 Pod/nginx                                        Started container nginx
10m                   Normal    Pulled                  Pod/nginx                                        Container image "mirror.gcr.io/nginx:1.27.4-bookworm" already present on machine
10m                   Normal    Killing                 Pod/nginx                                        Stopping container nginx
10m                   Normal    Provisioning            PersistentVolumeClaim/rabbitmq-data-rabbitmq-0   External provisioner is provisioning volume for claim "default/rabbitmq-data-rabbitmq-0"
10m (x2 over 10m)     Normal    ExternalProvisioning    PersistentVolumeClaim/rabbitmq-data-rabbitmq-0   Waiting for a volume to be created either by the external provisioner 'microk8s.io/hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
10m                   Normal    SuccessfulCreate        StatefulSet/rabbitmq                             create Claim rabbitmq-data-rabbitmq-0 Pod rabbitmq-0 in StatefulSet rabbitmq success
10m                   Normal    WaitForFirstConsumer    PersistentVolumeClaim/rabbitmq-data-rabbitmq-0   waiting for first consumer to be created before binding
10m                   Normal    ProvisioningSucceeded   PersistentVolumeClaim/rabbitmq-data-rabbitmq-0   Successfully provisioned volume pvc-0fc124b7-1ed6-413a-a5dc-08cf23048082
10m                   Normal    Scheduled               Pod/rabbitmq-0                                   Successfully assigned default/rabbitmq-0 to docker-desktop
10m                   Normal    Pulled                  Pod/rabbitmq-0                                   Container image "mirror.gcr.io/busybox:1.37.0" already present on machine
10m                   Normal    Created                 Pod/rabbitmq-0                                   Created container: rabbitmq-config
10m                   Normal    Started                 Pod/rabbitmq-0                                   Started container rabbitmq-config
10m                   Normal    Pulling                 Pod/rabbitmq-0                                   Pulling image "mirror.gcr.io/rabbitmq:3.8.34"
10m                   Normal    Pulled                  Pod/rabbitmq-0                                   Successfully pulled image "mirror.gcr.io/rabbitmq:3.8.34" in 12.647s (12.647s including waiting). Image size: 220131704 bytes.
10m                   Normal    Created                 Pod/rabbitmq-0                                   Created container: rabbitmq
10m                   Normal    Started                 Pod/rabbitmq-0                                   Started container rabbitmq
<unknown>             Normal    Created                 RabbitMQ/pod/rabbitmq-0                          Node rabbit@rabbitmq-0 is registered
9m21s                 Normal    Killing                 Pod/rabbitmq-0                                   Stopping container rabbitmq
9m19s (x2 over 10m)   Normal    SuccessfulCreate        StatefulSet/rabbitmq                             create Pod rabbitmq-0 in StatefulSet rabbitmq successful
9m19s                 Normal    Scheduled               Pod/rabbitmq-0                                   Successfully assigned default/rabbitmq-0 to docker-desktop
9m18s                 Normal    Pulled                  Pod/rabbitmq-0                                   Container image "mirror.gcr.io/busybox:1.37.0" already present on machine
9m18s                 Normal    Created                 Pod/rabbitmq-0                                   Created container: rabbitmq-config
9m18s                 Normal    Started                 Pod/rabbitmq-0                                   Started container rabbitmq-config
9m18s                 Normal    Pulled                  Pod/rabbitmq-0                                   Container image "mirror.gcr.io/rabbitmq:3.8.34" already present on machine
9m18s                 Normal    Created                 Pod/rabbitmq-0                                   Created container: rabbitmq
9m17s                 Normal    Started                 Pod/rabbitmq-0                                   Started container rabbitmq
<unknown>             Normal    Created                 RabbitMQ/pod/rabbitmq-0                          Node rabbit@rabbitmq-0 is registered
9m7s                  Normal    Starting                Node/docker-desktop                              
8m55s                 Normal    RegisteredNode          Node/docker-desktop                              Node docker-desktop event: Registered Node docker-desktop in Controller
2m22s                 Normal    SuccessfulCreate        ReplicaSet/generate-load-app                     Created pod: generate-load-app-dtrn7
2m22s                 Normal    SuccessfulCreate        ReplicaSet/generate-load-app                     Created pod: generate-load-app-xgjwd
2m22s                 Normal    SuccessfulCreate        ReplicaSet/generate-load-app                     Created pod: generate-load-app-72wzx
2m22s                 Normal    SuccessfulCreate        ReplicaSet/generate-load-app                     Created pod: generate-load-app-cfbns
2m22s                 Normal    SuccessfulCreate        ReplicaSet/generate-load-app                     Created pod: generate-load-app-t6n2j
2m21s                 Normal    Scheduled               Pod/generate-load-app-t6n2j                      Successfully assigned default/generate-load-app-t6n2j to docker-desktop
2m21s                 Normal    Scheduled               Pod/generate-load-app-cfbns                      Successfully assigned default/generate-load-app-cfbns to docker-desktop
2m21s                 Normal    Scheduled               Pod/generate-load-app-72wzx                      Successfully assigned default/generate-load-app-72wzx to docker-desktop
2m21s                 Normal    Scheduled               Pod/generate-load-app-xgjwd                      Successfully assigned default/generate-load-app-xgjwd to docker-desktop
2m21s                 Normal    Scheduled               Pod/generate-load-app-dtrn7                      Successfully assigned default/generate-load-app-dtrn7 to docker-desktop
2m21s                 Normal    Pulled                  Pod/generate-load-app-t6n2j                      Container image "mirror.gcr.io/busybox:1.37.0" already present on machine
2m21s                 Normal    Created                 Pod/generate-load-app-cfbns                      Created container: generate-load-app
2m21s                 Normal    Pulled                  Pod/generate-load-app-72wzx                      Container image "mirror.gcr.io/busybox:1.37.0" already present on machine
2m21s                 Normal    Started                 Pod/generate-load-app-72wzx                      Started container generate-load-app
2m21s                 Normal    Created                 Pod/generate-load-app-dtrn7                      Created container: generate-load-app
2m21s                 Normal    Pulled                  Pod/generate-load-app-dtrn7                      Container image "mirror.gcr.io/busybox:1.37.0" already present on machine
2m21s                 Normal    Started                 Pod/generate-load-app-cfbns                      Started container generate-load-app
2m21s                 Normal    Started                 Pod/generate-load-app-xgjwd                      Started container generate-load-app
2m21s                 Normal    Created                 Pod/generate-load-app-xgjwd                      Created container: generate-load-app
2m21s                 Normal    Pulled                  Pod/generate-load-app-xgjwd                      Container image "mirror.gcr.io/busybox:1.37.0" already present on machine
2m21s                 Normal    Pulled                  Pod/generate-load-app-cfbns                      Container image "mirror.gcr.io/busybox:1.37.0" already present on machine
2m21s                 Normal    Created                 Pod/generate-load-app-72wzx                      Created container: generate-load-app
2m21s                 Normal    Started                 Pod/generate-load-app-t6n2j                      Started container generate-load-app
2m21s                 Normal    Created                 Pod/generate-load-app-t6n2j                      Created container: generate-load-app
2m21s                 Normal    Started                 Pod/generate-load-app-dtrn7                      Started container generate-load-app
94s                   Normal    Killing                 Pod/generate-load-app-dtrn7                      Stopping container generate-load-app
94s                   Normal    Killing                 Pod/generate-load-app-cfbns                      Stopping container generate-load-app
94s                   Normal    Killing                 Pod/generate-load-app-72wzx                      Stopping container generate-load-app
94s                   Normal    Killing                 Pod/generate-load-app-xgjwd                      Stopping container generate-load-app
94s                   Normal    Killing                 Pod/generate-load-app-t6n2j                      Stopping container generate-load-app
83s                   Normal    SuccessfulCreate        ReplicaSet/probe-test-app-685956d4cf             Created pod: probe-test-app-685956d4cf-d4ndz
83s                   Normal    SuccessfulRescale       HorizontalPodAutoscaler/probe-test-app           New size: 4; reason: cpu resource utilization (percentage of request) above target
83s                   Normal    ScalingReplicaSet       Deployment/probe-test-app                        Scaled up replica set probe-test-app-685956d4cf from 3 to 4
82s                   Normal    Scheduled               Pod/probe-test-app-685956d4cf-d4ndz              Successfully assigned default/probe-test-app-685956d4cf-d4ndz to docker-desktop
82s                   Normal    Started                 Pod/probe-test-app-685956d4cf-d4ndz              Started container probe-test-app
82s                   Normal    Created                 Pod/probe-test-app-685956d4cf-d4ndz              Created container: probe-test-app
82s                   Normal    Pulled                  Pod/probe-test-app-685956d4cf-d4ndz              Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
46s                   Normal    ScalingReplicaSet       Deployment/cpu-stressor                          Scaled up replica set cpu-stressor-5d8fff5687 from 0 to 1
46s                   Normal    SuccessfulCreate        ReplicaSet/cpu-stressor-5d8fff5687               Created pod: cpu-stressor-5d8fff5687-lgjcc
46s                   Normal    Pulling                 Pod/cpu-stressor-5d8fff5687-lgjcc                Pulling image "mirror.gcr.io/narmidm/k8s-pod-cpu-stressor:v1.2.0"
45s                   Normal    Scheduled               Pod/cpu-stressor-5d8fff5687-lgjcc                Successfully assigned default/cpu-stressor-5d8fff5687-lgjcc to docker-desktop
39s                   Normal    Pulled                  Pod/cpu-stressor-5d8fff5687-lgjcc                Successfully pulled image "mirror.gcr.io/narmidm/k8s-pod-cpu-stressor:v1.2.0" in 6.316s (6.316s including waiting). Image size: 9842826 bytes.
39s                   Normal    Created                 Pod/cpu-stressor-5d8fff5687-lgjcc                Created container: cpu-stressor
39s                   Normal    Started                 Pod/cpu-stressor-5d8fff5687-lgjcc                Started container cpu-stressor
37s                   Normal    Killing                 Pod/cpu-stressor-5d8fff5687-lgjcc                Stopping container cpu-stressor
35s                   Normal    Pulling                 Pod/memory-stressor                              Pulling image "mirror.gcr.io/polinux/stress:1.0.4"
35s                   Normal    Scheduled               Pod/memory-stressor                              Successfully assigned default/memory-stressor to docker-desktop
30s                   Normal    Pulled                  Pod/memory-stressor                              Successfully pulled image "mirror.gcr.io/polinux/stress:1.0.4" in 5.413s (5.413s including waiting). Image size: 9744175 bytes.
29s (x2 over 29s)     Normal    Created                 Pod/memory-stressor                              Created container: memory-stressor
29s                   Normal    Pulled                  Pod/memory-stressor                              Container image "mirror.gcr.io/polinux/stress:1.0.4" already present on machine
28s (x2 over 29s)     Normal    Started                 Pod/memory-stressor                              Started container memory-stressor
27s (x2 over 28s)     Warning   BackOff                 Pod/memory-stressor                              Back-off restarting failed container memory-stressor in pod memory-stressor_default(1dc52c30-9e5c-49fa-b86d-92b91b909e6b)
23s                   Normal    SuccessfulCreate        ReplicaSet/probe-test-app-685956d4cf             Created pod: probe-test-app-685956d4cf-lb676
23s                   Normal    ScalingReplicaSet       Deployment/probe-test-app                        Scaled up replica set probe-test-app-685956d4cf from 4 to 5
23s                   Normal    SuccessfulRescale       HorizontalPodAutoscaler/probe-test-app           New size: 5; reason: cpu resource utilization (percentage of request) above target
22s                   Normal    Scheduled               Pod/probe-test-app-685956d4cf-lb676              Successfully assigned default/probe-test-app-685956d4cf-lb676 to docker-desktop
22s                   Normal    Created                 Pod/probe-test-app-685956d4cf-lb676              Created container: probe-test-app
22s                   Normal    Started                 Pod/probe-test-app-685956d4cf-lb676              Started container probe-test-app
22s                   Normal    Pulled                  Pod/probe-test-app-685956d4cf-lb676              Container image "mirror.gcr.io/jasonumiker/probe-test-app:v1" already present on machine
5s                    Normal    ScalingReplicaSet       Deployment/rabbitmq-consumer                     Scaled up replica set rabbitmq-consumer-54454cf965 from 0 to 1
5s                    Normal    SuccessfulCreate        ReplicaSet/rabbitmq-consumer-54454cf965          Created pod: rabbitmq-consumer-54454cf965-b84v5
4s                    Normal    Scheduled               Pod/rabbitmq-consumer-54454cf965-b84v5           Successfully assigned default/rabbitmq-consumer-54454cf965-b84v5 to docker-desktop
4s                    Normal    Pulling                 Pod/rabbitmq-consumer-54454cf965-b84v5           Pulling image "ghcr.io/kedacore/rabbitmq-client:v1.0"
3s                    Normal    SuccessfulCreate        Job/rabbitmq-publish                             Created pod: rabbitmq-publish-m8fl6
2s                    Normal    Scheduled               Pod/rabbitmq-publish-m8fl6                       Successfully assigned default/rabbitmq-publish-m8fl6 to docker-desktop
2s                    Normal    Pulling                 Pod/rabbitmq-publish-m8fl6                       Pulling image "ghcr.io/kedacore/rabbitmq-client:v1.0"
--------------------
kubectl describe job rabbitmq-publish
Name:             rabbitmq-publish
Namespace:        default
Selector:         batch.kubernetes.io/controller-uid=deff3a28-f136-4c00-a72a-685dd7cd4fac
Labels:           batch.kubernetes.io/controller-uid=deff3a28-f136-4c00-a72a-685dd7cd4fac
                  batch.kubernetes.io/job-name=rabbitmq-publish
                  controller-uid=deff3a28-f136-4c00-a72a-685dd7cd4fac
                  job-name=rabbitmq-publish
Annotations:      <none>
Parallelism:      1
Completions:      1
Completion Mode:  NonIndexed
Suspend:          false
Backoff Limit:    4
Start Time:       Fri, 18 Apr 2025 15:00:28 +1000
Pods Statuses:    1 Active (0 Ready) / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  batch.kubernetes.io/controller-uid=deff3a28-f136-4c00-a72a-685dd7cd4fac
           batch.kubernetes.io/job-name=rabbitmq-publish
           controller-uid=deff3a28-f136-4c00-a72a-685dd7cd4fac
           job-name=rabbitmq-publish
  Containers:
   rabbitmq-client:
    Image:      ghcr.io/kedacore/rabbitmq-client:v1.0
    Port:       <none>
    Host Port:  <none>
    Command:
      send
      $(rabbitmq_host)
      300
    Environment:
      rabbitmq_host:  <set to the key 'host' in secret 'rabbitmq-consumer-secret'>  Optional: false
    Mounts:           <none>
  Volumes:            <none>
  Node-Selectors:     <none>
  Tolerations:        <none>
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  4s    job-controller  Created pod: rabbitmq-publish-m8fl6
--------------------
cd ../cronjob
--------------------
kubectl apply -f cronjob.yaml
cronjob.batch/hello created
--------------------
kubectl get pods
NAME                                 READY   STATUS      RESTARTS   AGE
hello-29082541-8vl8s                 0/1     Completed   0          100s
hello-29082542-7jknx                 0/1     Completed   0          40s
probe-test-app-685956d4cf-d4ndz      1/1     Running     0          3m32s
probe-test-app-685956d4cf-fpmxh      1/1     Running     0          15m
probe-test-app-685956d4cf-jlf4s      1/1     Running     0          15m
probe-test-app-685956d4cf-lb676      1/1     Running     0          2m32s
probe-test-app-685956d4cf-v6fbz      1/1     Running     0          15m
rabbitmq-0                           1/1     Running     0          11m
rabbitmq-consumer-54454cf965-b84v5   1/1     Running     0          2m14s
rabbitmq-publish-m8fl6               0/1     Completed   0          2m12s
--------------------
kubectl get cronjob
NAME    SCHEDULE    TIMEZONE   SUSPEND   ACTIVE   LAST SCHEDULE   AGE
hello   * * * * *   <none>     False     0        41s             2m7s
--------------------
kubectl delete cronjob hello
cronjob.batch "hello" deleted
--------------------
kubectl get pods -A
NAMESPACE     NAME                                                     READY   STATUS              RESTARTS      AGE
default       probe-test-app-685956d4cf-d4ndz                          1/1     Running             0             3m35s
default       probe-test-app-685956d4cf-fpmxh                          1/1     Running             0             15m
default       probe-test-app-685956d4cf-jlf4s                          1/1     Running             0             15m
default       probe-test-app-685956d4cf-lb676                          1/1     Running             0             2m35s
default       probe-test-app-685956d4cf-v6fbz                          1/1     Running             0             15m
default       rabbitmq-0                                               1/1     Running             0             11m
default       rabbitmq-consumer-54454cf965-b84v5                       1/1     Running             0             2m17s
default       rabbitmq-publish-m8fl6                                   0/1     Completed           0             2m15s
keda          keda-admission-webhooks-657667f97d-tgh55                 0/1     ContainerCreating   0             2m28s
keda          keda-operator-887598c9b-qwm4g                            1/1     Running             1 (66s ago)   2m28s
keda          keda-operator-metrics-apiserver-7f5d6df58b-wj62v         0/1     ContainerCreating   0             2m28s
kube-system   coredns-668d6bf9bc-2jlrj                                 1/1     Running             0             4h31m
kube-system   coredns-668d6bf9bc-vg85r                                 1/1     Running             0             4h31m
kube-system   etcd-docker-desktop                                      1/1     Running             0             9m2s
kube-system   hostpath-provisioner-85c5f47d7b-n5zcm                    1/1     Running             1 (10m ago)   13m
kube-system   kube-apiserver-docker-desktop                            1/1     Running             0             4h31m
kube-system   kube-controller-manager-docker-desktop                   1/1     Running             0             11m
kube-system   kube-proxy-drxjv                                         1/1     Running             0             11m
kube-system   kube-scheduler-docker-desktop                            1/1     Running             1 (10m ago)   10m
kube-system   storage-provisioner                                      1/1     Running             1 (10m ago)   4h31m
kube-system   vpnkit-controller                                        1/1     Running             0             4h31m
monitoring    adapter-prometheus-adapter-57b7646776-f69vh              1/1     Running             0             7m47s
monitoring    alertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running             0             7m26s
monitoring    prometheus-grafana-66cb8c946f-pmjdv                      3/3     Running             0             7m54s
monitoring    prometheus-kube-prometheus-operator-7f8d744cd7-d2h4z     1/1     Running             0             7m54s
monitoring    prometheus-kube-state-metrics-f699c577d-g2g9x            1/1     Running             0             7m54s
monitoring    prometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running             0             7m26s
monitoring    prometheus-prometheus-node-exporter-qrt85                1/1     Running             0             7m54s
--------------------
kubectl api-resources
NAME                                SHORTNAMES               APIVERSION                        NAMESPACED   KIND
bindings                                                     v1                                true         Binding
componentstatuses                   cs                       v1                                false        ComponentStatus
configmaps                          cm                       v1                                true         ConfigMap
endpoints                           ep                       v1                                true         Endpoints
events                              ev                       v1                                true         Event
limitranges                         limits                   v1                                true         LimitRange
namespaces                          ns                       v1                                false        Namespace
nodes                               no                       v1                                false        Node
persistentvolumeclaims              pvc                      v1                                true         PersistentVolumeClaim
persistentvolumes                   pv                       v1                                false        PersistentVolume
pods                                po                       v1                                true         Pod
podtemplates                                                 v1                                true         PodTemplate
replicationcontrollers              rc                       v1                                true         ReplicationController
resourcequotas                      quota                    v1                                true         ResourceQuota
secrets                                                      v1                                true         Secret
serviceaccounts                     sa                       v1                                true         ServiceAccount
services                            svc                      v1                                true         Service
mutatingwebhookconfigurations                                admissionregistration.k8s.io/v1   false        MutatingWebhookConfiguration
validatingadmissionpolicies                                  admissionregistration.k8s.io/v1   false        ValidatingAdmissionPolicy
validatingadmissionpolicybindings                            admissionregistration.k8s.io/v1   false        ValidatingAdmissionPolicyBinding
validatingwebhookconfigurations                              admissionregistration.k8s.io/v1   false        ValidatingWebhookConfiguration
customresourcedefinitions           crd,crds                 apiextensions.k8s.io/v1           false        CustomResourceDefinition
apiservices                                                  apiregistration.k8s.io/v1         false        APIService
controllerrevisions                                          apps/v1                           true         ControllerRevision
daemonsets                          ds                       apps/v1                           true         DaemonSet
deployments                         deploy                   apps/v1                           true         Deployment
replicasets                         rs                       apps/v1                           true         ReplicaSet
statefulsets                        sts                      apps/v1                           true         StatefulSet
selfsubjectreviews                                           authentication.k8s.io/v1          false        SelfSubjectReview
tokenreviews                                                 authentication.k8s.io/v1          false        TokenReview
localsubjectaccessreviews                                    authorization.k8s.io/v1           true         LocalSubjectAccessReview
selfsubjectaccessreviews                                     authorization.k8s.io/v1           false        SelfSubjectAccessReview
selfsubjectrulesreviews                                      authorization.k8s.io/v1           false        SelfSubjectRulesReview
subjectaccessreviews                                         authorization.k8s.io/v1           false        SubjectAccessReview
horizontalpodautoscalers            hpa                      autoscaling/v2                    true         HorizontalPodAutoscaler
cronjobs                            cj                       batch/v1                          true         CronJob
jobs                                                         batch/v1                          true         Job
certificatesigningrequests          csr                      certificates.k8s.io/v1            false        CertificateSigningRequest
leases                                                       coordination.k8s.io/v1            true         Lease
endpointslices                                               discovery.k8s.io/v1               true         EndpointSlice
cloudeventsources                                            eventing.keda.sh/v1alpha1         true         CloudEventSource
clustercloudeventsources                                     eventing.keda.sh/v1alpha1         false        ClusterCloudEventSource
events                              ev                       events.k8s.io/v1                  true         Event
flowschemas                                                  flowcontrol.apiserver.k8s.io/v1   false        FlowSchema
prioritylevelconfigurations                                  flowcontrol.apiserver.k8s.io/v1   false        PriorityLevelConfiguration
clustertriggerauthentications       cta,clustertriggerauth   keda.sh/v1alpha1                  false        ClusterTriggerAuthentication
scaledjobs                          sj                       keda.sh/v1alpha1                  true         ScaledJob
scaledobjects                       so                       keda.sh/v1alpha1                  true         ScaledObject
triggerauthentications              ta,triggerauth           keda.sh/v1alpha1                  true         TriggerAuthentication
nodes                                                        metrics.k8s.io/v1beta1            false        NodeMetrics
pods                                                         metrics.k8s.io/v1beta1            true         PodMetrics
alertmanagerconfigs                 amcfg                    monitoring.coreos.com/v1alpha1    true         AlertmanagerConfig
alertmanagers                       am                       monitoring.coreos.com/v1          true         Alertmanager
podmonitors                         pmon                     monitoring.coreos.com/v1          true         PodMonitor
probes                              prb                      monitoring.coreos.com/v1          true         Probe
prometheusagents                    promagent                monitoring.coreos.com/v1alpha1    true         PrometheusAgent
prometheuses                        prom                     monitoring.coreos.com/v1          true         Prometheus
prometheusrules                     promrule                 monitoring.coreos.com/v1          true         PrometheusRule
scrapeconfigs                       scfg                     monitoring.coreos.com/v1alpha1    true         ScrapeConfig
servicemonitors                     smon                     monitoring.coreos.com/v1          true         ServiceMonitor
thanosrulers                        ruler                    monitoring.coreos.com/v1          true         ThanosRuler
ingressclasses                                               networking.k8s.io/v1              false        IngressClass
ingresses                           ing                      networking.k8s.io/v1              true         Ingress
networkpolicies                     netpol                   networking.k8s.io/v1              true         NetworkPolicy
runtimeclasses                                               node.k8s.io/v1                    false        RuntimeClass
poddisruptionbudgets                pdb                      policy/v1                         true         PodDisruptionBudget
clusterrolebindings                                          rbac.authorization.k8s.io/v1      false        ClusterRoleBinding
clusterroles                                                 rbac.authorization.k8s.io/v1      false        ClusterRole
rolebindings                                                 rbac.authorization.k8s.io/v1      true         RoleBinding
roles                                                        rbac.authorization.k8s.io/v1      true         Role
priorityclasses                     pc                       scheduling.k8s.io/v1              false        PriorityClass
csidrivers                                                   storage.k8s.io/v1                 false        CSIDriver
csinodes                                                     storage.k8s.io/v1                 false        CSINode
csistoragecapacities                                         storage.k8s.io/v1                 true         CSIStorageCapacity
storageclasses                      sc                       storage.k8s.io/v1                 false        StorageClass
volumeattachments                                            storage.k8s.io/v1                 false        VolumeAttachment
--------------------
kubectl get clusterrole admin -o yaml
aggregationRule:
  clusterRoleSelectors:
  - matchLabels:
      rbac.authorization.k8s.io/aggregate-to-admin: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  creationTimestamp: "2025-04-18T00:30:51Z"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: admin
  resourceVersion: "316"
  uid: 7fa32c33-4573-47e5-abcd-499e2fb84ef7
rules:
- apiGroups:
  - ""
  resources:
  - pods/attach
  - pods/exec
  - pods/portforward
  - pods/proxy
  - secrets
  - services/proxy
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - serviceaccounts
  verbs:
  - impersonate
- apiGroups:
  - ""
  resources:
  - pods
  - pods/attach
  - pods/exec
  - pods/portforward
  - pods/proxy
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - ""
  resources:
  - pods/eviction
  verbs:
  - create
- apiGroups:
  - ""
  resources:
  - configmaps
  - events
  - persistentvolumeclaims
  - replicationcontrollers
  - replicationcontrollers/scale
  - secrets
  - serviceaccounts
  - services
  - services/proxy
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - ""
  resources:
  - serviceaccounts/token
  verbs:
  - create
- apiGroups:
  - apps
  resources:
  - daemonsets
  - deployments
  - deployments/rollback
  - deployments/scale
  - replicasets
  - replicasets/scale
  - statefulsets
  - statefulsets/scale
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - autoscaling
  resources:
  - horizontalpodautoscalers
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - batch
  resources:
  - cronjobs
  - jobs
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - extensions
  resources:
  - daemonsets
  - deployments
  - deployments/rollback
  - deployments/scale
  - ingresses
  - networkpolicies
  - replicasets
  - replicasets/scale
  - replicationcontrollers/scale
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  - networkpolicies
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - ""
  resources:
  - configmaps
  - endpoints
  - persistentvolumeclaims
  - persistentvolumeclaims/status
  - pods
  - replicationcontrollers
  - replicationcontrollers/scale
  - serviceaccounts
  - services
  - services/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - bindings
  - events
  - limitranges
  - namespaces/status
  - pods/log
  - pods/status
  - replicationcontrollers/status
  - resourcequotas
  - resourcequotas/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - discovery.k8s.io
  resources:
  - endpointslices
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - apps
  resources:
  - controllerrevisions
  - daemonsets
  - daemonsets/status
  - deployments
  - deployments/scale
  - deployments/status
  - replicasets
  - replicasets/scale
  - replicasets/status
  - statefulsets
  - statefulsets/scale
  - statefulsets/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - autoscaling
  resources:
  - horizontalpodautoscalers
  - horizontalpodautoscalers/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - batch
  resources:
  - cronjobs
  - cronjobs/status
  - jobs
  - jobs/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - extensions
  resources:
  - daemonsets
  - daemonsets/status
  - deployments
  - deployments/scale
  - deployments/status
  - ingresses
  - ingresses/status
  - networkpolicies
  - replicasets
  - replicasets/scale
  - replicasets/status
  - replicationcontrollers/scale
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets
  - poddisruptionbudgets/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  - ingresses/status
  - networkpolicies
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - authorization.k8s.io
  resources:
  - localsubjectaccessreviews
  verbs:
  - create
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  - rolebindings
  - roles
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch
--------------------
kubectl get clusterrole admin -o yaml | wc -l
315
--------------------
cd ../k8s-authz
--------------------
./setup-tokens-on-cluster.sh
--------------------
./add-users-kubeconfig.sh
Context "docker-desktop-jane" created.
Context "docker-desktop-john" created.
--------------------
cat team1.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: "team1"
  labels:
    name: "team1"
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: admin
  namespace: team1
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["*"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: admin
  namespace: team1
subjects:
- kind: User
  name: jane
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: admin
  apiGroup: rbac.authorization.k8s.io--------------------
kubectl apply -f team1.yaml && kubectl apply -f team2.yaml
namespace/team1 created
role.rbac.authorization.k8s.io/admin created
rolebinding.rbac.authorization.k8s.io/admin created
namespace/team2 created
role.rbac.authorization.k8s.io/admin created
rolebinding.rbac.authorization.k8s.io/admin created
--------------------
kubectl config get-contexts
CURRENT   NAME                  CLUSTER          AUTHINFO         NAMESPACE
*         docker-desktop        docker-desktop   docker-desktop   
          docker-desktop-jane   docker-desktop   jane             team1
          docker-desktop-john   docker-desktop   john             team2
--------------------
kubectl config use-context docker-desktop-jane
Switched to context "docker-desktop-jane".
--------------------
kubectl get pods -A
Error from server (Forbidden): pods is forbidden: User "jane" cannot list resource "pods" in API group "" at the cluster scope
--------------------
kubectl get pods
No resources found in team1 namespace.
--------------------
kubectl config use-context docker-desktop-john
Switched to context "docker-desktop-john".
--------------------
kubectl get pods
No resources found in team2 namespace.
--------------------
kubectl get pods --namespace=team1
Error from server (Forbidden): pods is forbidden: User "john" cannot list resource "pods" in API group "" in the namespace "team1"
--------------------
kubectl config use-context docker-desktop
Switched to context "docker-desktop".
--------------------
Cleaning up Jane and John...
deleted user jane from /home/jumiker/.kube/config
deleted user john from /home/jumiker/.kube/config
deleted context docker-desktop-jane from /home/jumiker/.kube/config
deleted context docker-desktop-john from /home/jumiker/.kube/config
--------------------
cd ../ingress
--------------------
./install-nginx.sh
"ingress-nginx" already exists with the same configuration, skipping
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "gatekeeper" chart repository
...Successfully got an update from the "kedacore" chart repository
...Successfully got an update from the "ingress-nginx" chart repository
...Successfully got an update from the "kiali" chart repository
...Successfully got an update from the "argo-helm" chart repository
...Successfully got an update from the "prometheus-community" chart repository
...Successfully got an update from the "istio" chart repository
Update Complete. ⎈Happy Helming!⎈
NAME: ingress
LAST DEPLOYED: Fri Apr 18 15:03:43 2025
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
The ingress-nginx controller has been installed.
It may take a few minutes for the load balancer IP to be available.
You can watch the status by running 'kubectl get service --namespace default ingress-ingress-nginx-controller --output wide --watch'

An example Ingress that makes use of the controller:
  apiVersion: networking.k8s.io/v1
  kind: Ingress
  metadata:
    name: example
    namespace: foo
  spec:
    ingressClassName: nginx
    rules:
      - host: www.example.com
        http:
          paths:
            - pathType: Prefix
              backend:
                service:
                  name: exampleService
                  port:
                    number: 80
              path: /
    # This section is only required if TLS is to be enabled for the Ingress
    tls:
      - hosts:
        - www.example.com
        secretName: example-tls

If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided:

  apiVersion: v1
  kind: Secret
  metadata:
    name: example-tls
    namespace: foo
  data:
    tls.crt: <base64 encoded cert>
    tls.key: <base64 encoded key>
  type: kubernetes.io/tls
Waiting for deployment "ingress-ingress-nginx-controller" rollout to finish: 0 of 1 updated replicas are available...
deployment "ingress-ingress-nginx-controller" successfully rolled out
--------------------
kubectl apply -f probe-test-app-ingress.yaml
ingress.networking.k8s.io/probe-test-app created
--------------------
curl http://localhost
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   578  100   578    0     0   217k      0 --:--:-- --:--:-- --:--:--  282k
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>probetest</title>
</head>
<body>
    <p>This Flask app is served from probe-test-app-685956d4cf-fpmxh</p>
    <p>The readiness endpoint (/readyz) is Healthy</p>
    <p>The liveness endpoint is (/livez) Healthy</p>
    <form method="POST">
        <button type="submit" name="readyz">Toggle Readiness</button>
        <button type="submit" name="livez">Toggle Liveness</button>
    </form>
    <p>Version 1</p>
</body>
</html>--------------------
kubectl apply -f nyancat.yaml
deployment.apps/nyancat created
service/nyancat created
--------------------
kubectl rollout status deployment nyancat -n default
Waiting for deployment "nyancat" rollout to finish: 0 of 1 updated replicas are available...
deployment "nyancat" successfully rolled out
--------------------
kubectl apply -f nyancat-ingress.yaml
ingress.networking.k8s.io/probe-test-app configured
--------------------
curl http://localhost/nyancat/
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  1279  100  1279    0     0   427k      0 --:--:-- --:--:-- --:--:--  416k100  1279  100  1279    0     0   408k      0 --:--:-- --:--:-- --:--:--  312k
<!DOCTYPE HTML>
<html lang="en-US">
<head>
	<meta charset="UTF-8">
	<title>Nyan Cat - HTML5+CSS3+JS</title>

	<link rel="stylesheet" href="css/nyan.css"/>
</head>
<body>
	<div class="sparks-combo">
		<div class="spark"></div>
		<div class="spark"></div>
		<div class="spark"></div>
		<div class="spark"></div>
	</div>

	<div id="wave-a" class="hot rainbow"></div>
	<div id="wave-a" class="cold rainbow"></div>

	<div id="wave-b" class="hot rainbow"></div>
	<div id="wave-b" class="cold rainbow"></div>

	<div id="nyan-cat" class="frame1">
		<div id="tail"></div>

		<div id="paws"></div>

		<div id="pop-tarts-body">
			<div id="pop-tarts-body-cream"></div>
		</div>

		<div id="head">
			<div id="face"></div>
		</div>
	</div>

	<a href="https://github.com/cristurm/nyan-cat" title="My Github Repo" class="repo">
		<i class="octicon octicon-mark-github"></i>
		<span>My Github Repo</span>
	</a>

	<audio autoplay="true" loop="true">
		<source src="audio/nyan-cat.ogg" type="audio/ogg" />
		<source src="audio/nyan-cat.mp3" type="audio/mpeg" />
	</audio>

	<!-- Libs -->
	<script src="//code.jquery.com/jquery-1.10.1.min.js"></script>
	<script src="//code.jquery.com/jquery-migrate-1.2.1.min.js"></script>
	<!-- Nyan Stuff -->
	<script src="js/nyan.js"></script>
</body>
</html>
--------------------
kubectl delete ingress probe-test-app
ingress.networking.k8s.io "probe-test-app" deleted
--------------------
helm uninstall ingress
release "ingress" uninstalled
--------------------
Cleaning up probe-test-app and nyancat...
horizontalpodautoscaler.autoscaling "probe-test-app" deleted
deployment.apps "probe-test-app" deleted
deployment.apps "nyancat" deleted
service "probe-test-app" deleted
service "nyancat" deleted
--------------------
cd ../istio
--------------------
./install-istio.sh
"istio" already exists with the same configuration, skipping
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "kiali" chart repository
...Successfully got an update from the "ingress-nginx" chart repository
...Successfully got an update from the "gatekeeper" chart repository
...Successfully got an update from the "kedacore" chart repository
...Successfully got an update from the "argo-helm" chart repository
...Successfully got an update from the "prometheus-community" chart repository
...Successfully got an update from the "istio" chart repository
Update Complete. ⎈Happy Helming!⎈
namespace/istio-system created
NAME: istio-base
LAST DEPLOYED: Fri Apr 18 15:05:40 2025
NAMESPACE: istio-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Istio base successfully installed!

To learn more about the release, try:
  $ helm status istio-base -n istio-system
  $ helm get all istio-base -n istio-system
NAME: istiod
LAST DEPLOYED: Fri Apr 18 15:05:42 2025
NAMESPACE: istio-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
"istiod" successfully installed!

To learn more about the release, try:
  $ helm status istiod -n istio-system
  $ helm get all istiod -n istio-system

Next steps:
  * Deploy a Gateway: https://istio.io/latest/docs/setup/additional-setup/gateway/
  * Try out our tasks to get started on common configurations:
    * https://istio.io/latest/docs/tasks/traffic-management
    * https://istio.io/latest/docs/tasks/security/
    * https://istio.io/latest/docs/tasks/policy-enforcement/
  * Review the list of actively supported releases, CVE publications and our hardening guide:
    * https://istio.io/latest/docs/releases/supported-releases/
    * https://istio.io/latest/news/security/
    * https://istio.io/latest/docs/ops/best-practices/security/

For further documentation see https://istio.io website
"kiali" already exists with the same configuration, skipping
NAME: kiali-server
LAST DEPLOYED: Fri Apr 18 15:05:58 2025
NAMESPACE: istio-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Welcome to Kiali! For more details on Kiali, see: https://kiali.io

The Kiali Server [v2.8.0] has been installed in namespace [istio-system]. It will be ready soon.

When installing with "deployment.cluster_wide_access=false" using this Kiali Server Helm Chart,
it is your responsibility to manually create the proper Roles and RoleBindings for the Kiali Server
to have the correct permissions to access the service mesh namespaces.

(Helm: Chart=[kiali-server], Release=[kiali-server], Version=[2.8.0])
servicemonitor.monitoring.coreos.com/prometheus-oper-istio-controlplane created
servicemonitor.monitoring.coreos.com/prometheus-oper-istio-dataplane created
customresourcedefinition.apiextensions.k8s.io/gatewayclasses.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/gateways.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/grpcroutes.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/httproutes.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/referencegrants.gateway.networking.k8s.io created
--------------------
kubectl label namespace default istio-injection=enabled
namespace/default labeled
--------------------
kubectl apply -f bookinfo/platform/kube/bookinfo.yaml
service/details created
serviceaccount/bookinfo-details created
deployment.apps/details-v1 created
service/ratings created
serviceaccount/bookinfo-ratings created
deployment.apps/ratings-v1 created
service/reviews created
serviceaccount/bookinfo-reviews created
deployment.apps/reviews-v1 created
deployment.apps/reviews-v2 created
deployment.apps/reviews-v3 created
service/productpage created
serviceaccount/bookinfo-productpage created
deployment.apps/productpage-v1 created
--------------------
kubectl apply -f bookinfo/gateway-api/bookinfo-gateway.yaml
gateway.gateway.networking.k8s.io/bookinfo-gateway created
httproute.gateway.networking.k8s.io/bookinfo created
--------------------
kubectl apply -f bookinfo/platform/kube/bookinfo-versions.yaml
service/reviews-v1 created
service/reviews-v2 created
service/reviews-v3 created
service/productpage-v1 created
service/ratings-v1 created
service/details-v1 created
--------------------
kubectl apply -f bookinfo/gateway-api/route-all-v1.yaml
httproute.gateway.networking.k8s.io/reviews created
httproute.gateway.networking.k8s.io/productpage created
httproute.gateway.networking.k8s.io/ratings created
httproute.gateway.networking.k8s.io/details created
--------------------
kubectl apply -f bookinfo/gateway-api/route-reviews-90-10.yaml
httproute.gateway.networking.k8s.io/reviews configured
--------------------
kubectl apply -f bookinfo/gateway-api/route-jason-v2.yaml
httproute.gateway.networking.k8s.io/reviews configured
--------------------
bookinfo/platform/kube/cleanup.sh
using NAMESPACE=default
gateway.gateway.networking.k8s.io "bookinfo-gateway" deleted
httproute.gateway.networking.k8s.io "bookinfo" deleted
httproute.gateway.networking.k8s.io "details" deleted
httproute.gateway.networking.k8s.io "productpage" deleted
httproute.gateway.networking.k8s.io "ratings" deleted
httproute.gateway.networking.k8s.io "reviews" deleted
Application cleanup may take up to one minute
service "details" deleted
serviceaccount "bookinfo-details" deleted
deployment.apps "details-v1" deleted
service "ratings" deleted
serviceaccount "bookinfo-ratings" deleted
deployment.apps "ratings-v1" deleted
service "reviews" deleted
serviceaccount "bookinfo-reviews" deleted
deployment.apps "reviews-v1" deleted
deployment.apps "reviews-v2" deleted
deployment.apps "reviews-v3" deleted
service "productpage" deleted
serviceaccount "bookinfo-productpage" deleted
deployment.apps "productpage-v1" deleted
Application cleanup successful
--------------------
helm install istio-ingress istio/gateway --create-namespace -n istio-ingress --wait --version=1.25.1
NAME: istio-ingress
LAST DEPLOYED: Fri Apr 18 15:07:17 2025
NAMESPACE: istio-ingress
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
"istio-ingress" successfully installed!

To learn more about the release, try:
  $ helm status istio-ingress -n istio-ingress
  $ helm get all istio-ingress -n istio-ingress

Next steps:
  * Deploy an HTTP Gateway: https://istio.io/latest/docs/tasks/traffic-management/ingress/ingress-control/
  * Deploy an HTTPS Gateway: https://istio.io/latest/docs/tasks/traffic-management/ingress/secure-ingress/
--------------------
kubectl apply -k legacy-ingress-gateway
service/probe-test-app created
deployment.apps/probe-test-app created
gateway.networking.istio.io/istio-ingress created
virtualservice.networking.istio.io/probe-test-app created
--------------------
helm uninstall istio-ingress -n istio-ingress
release "istio-ingress" uninstalled
--------------------
kubectl delete -k legacy-ingress-gateway
service "probe-test-app" deleted
deployment.apps "probe-test-app" deleted
gateway.networking.istio.io "istio-ingress" deleted
virtualservice.networking.istio.io "probe-test-app" deleted
--------------------
cd ../kustomize
--------------------
kustomize build prod
apiVersion: v1
kind: Service
metadata:
  labels:
    run: my-nginx
  name: prod-my-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
  selector:
    run: my-nginx
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prod-my-nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      run: my-nginx
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - image: mirror.gcr.io/nginx:1.27.4-bookworm
        name: my-nginx
--------------------
kubectl apply -k prod
service/prod-my-nginx created
deployment.apps/prod-my-nginx created
--------------------
kubectl get pods
NAME                              READY   STATUS        RESTARTS   AGE
probe-test-app-685956d4cf-2zmtk   2/2     Terminating   0          7s
probe-test-app-685956d4cf-5chvs   2/2     Terminating   0          7s
probe-test-app-685956d4cf-rwsww   2/2     Terminating   0          7s
prod-my-nginx-5c8c9cc974-2z5qj    0/2     Init:0/1      0          2s
prod-my-nginx-5c8c9cc974-5gkt9    0/2     Init:0/1      0          2s
rabbitmq-0                        1/1     Running       0          16m
rabbitmq-publish-m8fl6            0/1     Completed     0          7m2s
--------------------
kubectl apply -k dev
service/dev-my-nginx created
deployment.apps/dev-my-nginx created
--------------------
kubectl get pods
NAME                              READY   STATUS            RESTARTS   AGE
dev-my-nginx-5c8c9cc974-k5vc5     0/2     PodInitializing   0          1s
probe-test-app-685956d4cf-2zmtk   2/2     Terminating       0          9s
probe-test-app-685956d4cf-5chvs   2/2     Terminating       0          9s
probe-test-app-685956d4cf-rwsww   2/2     Terminating       0          9s
prod-my-nginx-5c8c9cc974-2z5qj    1/2     Running           0          4s
prod-my-nginx-5c8c9cc974-5gkt9    1/2     Running           0          4s
rabbitmq-0                        1/1     Running           0          16m
rabbitmq-publish-m8fl6            0/1     Completed         0          7m4s
--------------------
Cleaning up Kustomization example...
service "prod-my-nginx" deleted
deployment.apps "prod-my-nginx" deleted
service "dev-my-nginx" deleted
deployment.apps "dev-my-nginx" deleted
--------------------
helm ls -A
NAME        	NAMESPACE   	REVISION	UPDATED                                 	STATUS  	CHART                       	APP VERSION
adapter     	monitoring  	1       	2025-04-18 14:54:56.606710736 +1000 AEST	deployed	prometheus-adapter-4.14.1   	v0.12.0    
istio-base  	istio-system	1       	2025-04-18 15:05:40.626952134 +1000 AEST	deployed	base-1.25.1                 	1.25.1     
istiod      	istio-system	1       	2025-04-18 15:05:42.74216651 +1000 AEST 	deployed	istiod-1.25.1               	1.25.1     
keda        	keda        	1       	2025-04-18 15:00:14.261295245 +1000 AEST	deployed	keda-2.17.0                 	2.17.0     
kiali-server	istio-system	1       	2025-04-18 15:05:58.090622092 +1000 AEST	deployed	kiali-server-2.8.0          	v2.8.0     
prometheus  	monitoring  	1       	2025-04-18 14:54:04.748514078 +1000 AEST	deployed	kube-prometheus-stack-70.4.1	v0.81.0    
--------------------
helm upgrade prometheus prometheus-community/kube-prometheus-stack --version 70.4.2 -n monitoring
Release "prometheus" has been upgraded. Happy Helming!
NAME: prometheus
LAST DEPLOYED: Fri Apr 18 15:07:39 2025
NAMESPACE: monitoring
STATUS: deployed
REVISION: 2
NOTES:
kube-prometheus-stack has been installed. Check its status by running:
  kubectl --namespace monitoring get pods -l "release=prometheus"

Get Grafana 'admin' user password by running:

  kubectl --namespace monitoring get secrets prometheus-grafana -o jsonpath="{.data.admin-password}" | base64 -d ; echo

Access Grafana local instance:

  export POD_NAME=$(kubectl --namespace monitoring get pod -l "app.kubernetes.io/name=grafana,app.kubernetes.io/instance=prometheus" -oname)
  kubectl --namespace monitoring port-forward $POD_NAME 3000

Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create & configure Alertmanager and Prometheus instances using the Operator.
--------------------
helm get values prometheus -n monitoring
USER-SUPPLIED VALUES:
crds:
  upgradeJob:
    enabled: true
    image:
      busybox:
        registry: mirror.gcr.io
grafana:
  image:
    registry: mirror.gcr.io
  service:
    port: 3000
    type: LoadBalancer
kubelet:
  serviceMonitor:
    cAdvisorMetricRelabelings: null
prometheus:
  service:
    type: LoadBalancer
prometheus-node-exporter:
  hostRootFsMount:
    enabled: false
  prometheus:
    monitor:
      attachMetadata:
        node: true
      relabelings:
      - action: replace
        regex: (.+)
        replacement: ${1}
        sourceLabels:
        - __meta_kubernetes_endpoint_node_name
        targetLabel: node
--------------------
helm repo add gatekeeper https://open-policy-agent.github.io/gatekeeper/charts
"gatekeeper" already exists with the same configuration, skipping
--------------------
helm install gatekeeper/gatekeeper --name-template=gatekeeper --namespace gatekeeper-system --create-namespace --version 3.19.0
NAME: gatekeeper
LAST DEPLOYED: Fri Apr 18 15:08:19 2025
NAMESPACE: gatekeeper-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
--------------------
cd ../opa-gatekeeper
--------------------
kubectl apply -f k8srequiredlabels-constraint-template.yaml
constrainttemplate.templates.gatekeeper.sh/k8srequiredlabels created
--------------------
kubectl apply -f pods-in-default-must-have-owner.yaml
k8srequiredlabels.constraints.gatekeeper.sh/pods-in-default-must-have-owner created
--------------------
kubectl apply -f ../probe-test-app/probe-test-app-pod.yaml
Error from server (Forbidden): error when creating "../probe-test-app/probe-test-app-pod.yaml": admission webhook "validation.gatekeeper.sh" denied the request: [pods-in-default-must-have-owner] missing required label, requires all of: owner
[pods-in-default-must-have-owner] regex mismatch
--------------------
kubectl apply -f probe-test-app-pod.yaml
pod/probe-test-app created
--------------------
kubectl delete constraint pods-in-default-must-have-owner
k8srequiredlabels.constraints.gatekeeper.sh "pods-in-default-must-have-owner" deleted
--------------------
kubectl delete pod probe-test-app
pod "probe-test-app" deleted
--------------------
helm repo add argo-helm https://argoproj.github.io/argo-helm
"argo-helm" already exists with the same configuration, skipping
--------------------
helm install argo-cd argo-helm/argo-cd --namespace argocd --create-namespace --version 7.8.24
NAME: argo-cd
LAST DEPLOYED: Fri Apr 18 15:09:38 2025
NAMESPACE: argocd
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
In order to access the server UI you have the following options:

1. kubectl port-forward service/argo-cd-argocd-server -n argocd 8080:443

    and then open the browser on http://localhost:8080 and accept the certificate

2. enable ingress in the values file `server.ingress.enabled` and either
      - Add the annotation for ssl passthrough: https://argo-cd.readthedocs.io/en/stable/operator-manual/ingress/#option-1-ssl-passthrough
      - Set the `configs.params."server.insecure"` in the values file and terminate SSL at your ingress: https://argo-cd.readthedocs.io/en/stable/operator-manual/ingress/#option-2-multiple-ingress-objects-and-hosts


After reaching the UI the first time you can login with username: admin and the random password generated during the installation. You can find the password by running:

kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d

(You should delete the initial secret afterwards as suggested by the Getting Started Guide: https://argo-cd.readthedocs.io/en/stable/getting_started/#4-login-using-the-cli)
--------------------
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath={.data.password} | base64 -d
AM07kOjP91u66rUA--------------------
cd ../argocd
--------------------
kubectl apply -f probe-test-app.yaml -n argocd
application.argoproj.io/probe-test-app created
--------------------
kubectl apply -f argo-rollouts-app.yaml -n argocd
application.argoproj.io/argo-rollouts created
--------------------
kubectl delete deployment probe-test-app
deployment.apps "probe-test-app" deleted
--------------------
kubectl get pods
NAME                              READY   STATUS      RESTARTS   AGE
probe-test-app-685956d4cf-5shl4   2/2     Running     0          9s
probe-test-app-685956d4cf-h9knh   2/2     Running     0          9s
probe-test-app-685956d4cf-tr9j4   2/2     Running     0          9s
rabbitmq-0                        1/1     Running     0          19m
rabbitmq-publish-m8fl6            0/1     Completed   0          10m
--------------------
kubectl delete application probe-test-app -n argocd
application.argoproj.io "probe-test-app" deleted
--------------------
cd ../argo-rollouts
--------------------
kubectl apply -f bluegreen-service.yaml
service/bluegreen-demo created
--------------------
kubectl apply -f bluegreen-gateway.yaml
gateway.gateway.networking.k8s.io/bluegreen-demo created
httproute.gateway.networking.k8s.io/bluegreen-demo created
--------------------
kubectl apply -f bluegreen-preview-service.yaml
service/bluegreen-demo-preview created
--------------------
kubectl apply -f bluegreen-preview-gateway.yaml
gateway.gateway.networking.k8s.io/bluegreen-demo-preview created
httproute.gateway.networking.k8s.io/bluegreen-demo-preview created
--------------------
kubectl apply -f bluegreen-rollout-manual.yaml
rollout.argoproj.io/bluegreen-demo created
--------------------
kubectl apply -f bluegreen-rollout-manual-green.yaml
rollout.argoproj.io/bluegreen-demo configured
--------------------
kubectl argo rollouts get rollout bluegreen-demo
Name:            bluegreen-demo
Namespace:       default
Status:          [94m◌[0m Progressing
Message:         more replicas need to be updated
Strategy:        BlueGreen
Replicas:
  Desired:       1
  Current:       0
  Updated:       0
  Ready:         0
  Available:     0

NAME              KIND     STATUS         AGE  INFO
⟳ bluegreen-demo  Rollout  [94m◌[0m Progressing  12s  
--------------------
kubectl argo rollouts promote bluegreen-demo
rollout 'bluegreen-demo' promoted
--------------------
kubectl argo rollouts get rollout bluegreen-demo
Name:            bluegreen-demo
Namespace:       default
Status:          [94m◌[0m Progressing
Message:         updated replicas are still becoming available
Strategy:        BlueGreen
Images:          mirror.gcr.io/argoproj/rollouts-demo:green ([32mstable[0m, [94mpreview[0m)
Replicas:
  Desired:       1
  Current:       1
  Updated:       1
  Ready:         0
  Available:     0

NAME                                        KIND        STATUS             AGE  INFO
⟳ bluegreen-demo                            Rollout     [94m◌[0m Progressing      33s  
└──# revision:1                                                                 
   └──⧉ [94m[32mbluegreen-demo-754b69fb9c[0m[0m           ReplicaSet  [94m◌[0m Progressing      9s   [32mstable[0m,[94mpreview[0m
      └──□ bluegreen-demo-754b69fb9c-xdvrj  Pod         [0m [0m PodInitializing  9s   ready:0/2
--------------------
kubectl argo rollouts undo bluegreen-demo
Error: no revision found for rollout "bluegreen-demo"
--------------------
